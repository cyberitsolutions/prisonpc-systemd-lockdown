-GOAL: Alpha and Omega are running Debian 10 Buster.


Checklist
=========

- **TODO** choose hardware

  Omega & Omega-Understudy will remain on the good supermicro servers they're already on.
  They will get MORE RAM.
  They will get small SSDs to make ZFS work better.
  They might have their HDDs replaced.

  For initial experiments (the "learning server"),
  a temporary system has been built from old parts.

  Alpha & Alpha-Understudy hardware remains undecided at this time.
  (Currently on sucky 3U whitebox servers.)

- **TODO** make sure omega/omega-understudy/alpha/alpha-understudy are running EFI (not BIOS)

- **TODO** buy more **ECC** RAM for omega & omega-undertudy
- **TODO** buy small SSDs for omega & omega-understudy (for SLOG ZIL, and *maybe* for L2ARC)

- **DONE** get Debian 10 to boot with the rootfs on ZFS
- **DONE** get Debian 10 to boot with /boot also on ZFS (i.e. make a ZFS EFI driver).

  This will let us give the *entire* disk to ZFS, and have just a simple refind-based ESP on a USB key.
  (We won't be able to use stock refind, we'd have to have a ZFS.EFI from somewhere else.)

  This is not comforting:
  https://github.com/zfsonlinux/zfs/commit/7f35d649253c29da8852ae105980a86d8ee7ee2a

- **TODO** how does the AD domain connect to our existing domain (esp. DNS)?
- **TODO** should samba's DNS server have its caching resolver turned off?
- **TODO** how do we manage users and groups in AD, from the CLI?
- **TODO** how do we connect PAM/NSS to AD?
- **TODO** how do we connect nginx (or apache) to AD?
- **TODO** how do we nerf password guessing *in samba*, a la ppolicy, as defense-in-depth in case one of the downstream applications fails to nerf password guessing?
- **TODO** add resource constraints and systemd lockdown and apparmor lockdown for EVERYTHING.
- **TODO** do all the Cyber BCP things


- **TODO** install apt-cacher-ng (instead of manually debmirroring),
  and then use squid-deb-proxy-client on "all" the clients, so they
  auto-discover _apt._tcp stuff from apt-cacher-ng on the LAN?

- **TODO** firewalld is the default on RHEL7.  Should we adopt it?



Plan / Overview
===============
* New-Alpha does networking stuff.

  * firewall, DNS, ntpsec, &c
  * nsd3 (authoritative DNS?)
  * fail2ban again?

* New-Omega does everything else.

  * MORE RAM!  (ideally 64GB to 128GB)
  * VT-d & VT-x
  * 1 minimum-size SSD for ZIL SLOG (NVMe would be nice, but AHCI is OK)
  * 1 minimum-size SSD for L2ARC (NVMe would be nice, but AHCI is OK)

  * UEFI and refind, *NOT* BIOS/extlinux/grub

  * Convert omega-understudy and alpha-understudy and cyber-offsite to
    be based on ZFS send/recv.

  * Buster, not Stretch

  * Use dracut instead of initramfs-tools ???

    The benefit is a "more systemd-y" ramdisk.
    The default Debian ramdisk is very old and based on buggy sh scripts.
    The systemd model is that systemd should be IN THE RAMDISK, too.
    The systemd model ALSO goes BACK into the ramdisk during the shutdown process.
    This is particularly useful for systems with ZFS or btrfs rootfs.

  * ZFS on Linux (ZOL) for archive/RAID
  * Samba AD (accounts/LDAP), SMB3.2 file sharing
  * postfix/dovecot/(mailman3?)

    * use dovecot LDA (and sieve); BAN PROCMAIL.

  * strong apparmor/systemd lockdown, NOT containers/VMs
  * icinga3 (replaces nagios), collectd/graphite

  * alloc (mariadb+php7+apache) in a VM, because we don't trust alloc.
    KVM, libvirtd, libvirt-daemon-driver-storage-zfs?

  * rsyslog (logserv) + journald
  * ssh/sftp gateway

  * IRC (eventualy replace with matrix/riot.im)

  * nginx, gitit

  * fail2ban for ALL services (SSH might continue to use the in-kernel IPS as well/instead)
  * letsencrypt for all services
  * gitolite for /srv/vcs

  * configuration management? (ansible / salt)
  * no squid
  * no cups (everyone has to install cups on their laptop)
  * motion (camera)
  * apt mirror (or NBN?)

  * start actually doing IPv6 and nftables, not just IPv4 and iptables

  * discourage/ban people writing /var/spool/cron/crontabs;
    make them write ~/.config/systemd/ timers, so that
    they're version-controlled better.

    Question: do they run when the user isn't logged in?
    e.g. overnight alloc submission job?

  * nut (upsd)


[SOLVED] How do we boot ZFS?
----------------------------

* SHORT VERSION:  git clone login.cyber.com.au:/srv/vcs/flash-kernel-efi.git

* ZFS wants to be given whole disks, not partitions.
  On Linux, benefits are

  1) ZFS will auto-set the disk's I/O scheduler; and
  2) ZFS will auto-partition replacement disks.

  UPDATE: <rlaager> twb: #1 is not applicable. The initramfs will set noop on the disks in the root pool.

* UEFI (or BIOS) support for /boot on ZFS exist, but do not support all ZFS features.
  Therefore, *either*

  1) the main ZFS pool is limited to a crap feature set; or
  2) /boot lives somewhere else (not the main ZFS pool)

  UPDATE::

    15:34 <rlaager> I think "crap feature set" is a bit harsh. That was _the_ feature set not that long ago.
    15:35 <twb> I haven't looked at exactly what the featureset is

  UPDATE: here is the featureset that zfs_x64.efi from efifs 1.3 (= grub-2.02-143-g51be3372e) supports::

      /*
       * List of pool features that the grub implementation of ZFS supports for
       * read. Note that features that are only required for write do not need
       * to be listed here since grub opens pools in read-only mode.
       */
      #define MAX_SUPPORTED_FEATURE_STRLEN 50
      static const char *spa_feature_names[] = {
        "org.illumos:lz4_compress",
        "com.delphix:hole_birth",
        "com.delphix:embedded_data",
        "com.delphix:extensible_dataset",
        "org.open-zfs:large_blocks",
        NULL
      };

  Comparing that to what rlaager is doing in the HOWTO, the zfs create
  args aren't directly diffable, because in the bpool they're using
  "-d" to turn off most things, then "-o" to turn some things back on.

  rlaager worked it out for me::

    15:53 <rlaager> twb: multi_vdev_crash_dump (which is never used anyway), large_dnode, sha512, skein, and edonr.
    15:53 <rlaager> twb: Oops, sorry, that was bionic, not buster.
    15:56 <rlaager> twb: Buster is the same as Bionic.
                    0.8.0 adds encryption, device_removal, obsolete_counts (part of device_removal, essentially), and bookmark_v2 (part of encryption, essentially)

    15:52 <rlaager> There's also a reasonable debate to be had here about whether we should enable features "just because we can" or whether we should only able features that "matter" on the bpool.
    15:53 <twb> I actually use ext2 for /boot quite often because YAGNI even for the journal and extents :-)


* UEFI ESP is a small (~4MB) FAT32 partition, that must exist *somewhere*.
  It can't reliably be mirrored, so Cyber Best Current Practice is to make it a completely generic refind.img USB key, mounted inside the case.
  Yes, it's a SPOF, but it's a *read-only* device, and if it breaks, it's a simple matter to "cp refind.img /dev/sdz" to make a new USB key.

  Refind doesn't have a ZFS.EFI driver, but it can use grub2's zfs.mod, recompiled for EFI by the efifs project.
  https://efi.akeo.ie/downloads/efifs-1.3/x64/zfs_x64.efi

  Is efifs's zfs_x64.efi the same as grub2's zfs.mod?

      I have confirmed the EFIFS 1.3 uses grub-core's zfs.c almost at HEAD.
      This is the commit of grub that efifs is using::

          51be337 2018-04-16 22:36 -0700 NVi ∙ [HEAD] templates: Update grub script template files

      There are negligible differences between that and grub master::

          twb@goll[grub]$ tig HEAD..master -- grub-core/fs/zfs
          ad4bfee 2019-04-08 07:24 +0200 VSe ∙ Change fs functions to add fs_ prefix
          ca0a4f6 2013-11-20 02:28 +0100 VSe ∙ verifiers: File type for fine-grained signature-verification controlling

      There are negligible differences between that and the (earlier) 2.02 stable grub release::

          twb@goll[grub]$ tig 2.02..HEAD -- grub-core/fs/zfs
          fa42786 2017-08-03 15:46 +0100 PBa ∙ zfs: remove size_t typedef and use grub_size_t instead

      2.02 is the version of grub in Debian buster.
      There is a 2.04 release candidate at this time (Apr 2019). ::

          $ tig 2.02..grub-2.04-rc1 -- grub-core/fs/zfs
          ad4bfee 2019-04-08 07:24 +0200 VSe ∙ Change fs functions to add fs_ prefix
          ca0a4f6 2013-11-20 02:28 +0100 VSe ∙ verifiers: File type for fine-grained signature-verification controlling
          fa42786 2017-08-03 15:46 +0100 PBa ∙ zfs: remove size_t typedef and use grub_size_t instead


  rlaager's design would then work like this:

  * ESP (w/ ZFS driver) →
  * bpool (small, feature-limited ZFS pool just for /boot) kernel + ramdisk →
  * rpool (large, full-feature ZFS pool for everything else) rootfs et al.

* Instead of rlaager's design, we propose to store a copy of /boot on the ESP itself.

  Whenever flashrom would run (on initramfs compile / kernel install?)::

      mount -o remount,rw /boot/efi
      rsync --exclude=efi /boot/ /boot/efi/debian/
      mount -o remount,ro /boot/efi

  Also, to preserve the old model of "just dd this image to a new USB
  key", take a backup of the entire USB key *back* to a zvol inside
  the main pool, e.g. ::

      # The --inplace is to avoid COWs on NOP blocks;
      # I think there is a new ZFS feature that can automatically skip NOP writes?
      rsync --inplace /dev/disk/by-id/TOSHIBA-XXXX  /dev/disk/by-uuid/XXXX

  This ensures both /boot and the full ESP disk get ZFS snapshotted and ZFS send-ed to the usual backup places.

  * UPDATE::

        15:41 <rlaager> twb: You might be able to save some steps with this approach:
                1) Always keep your FAT32-formatted zvol mounted at /boot. Mount it with "sync" for extra safety, especially in light of...
                2) rsync the zvol to /dev/disk/by-id/TOSHIBA-xxxx "Whenever flashrom would run"
        15:42 <twb> ah so copy it in the other direction, basically
        15:42 <rlaager> Exactly. Then it should be "safe" to keep it mounted all the time, which
                        is what I assume you were trying to avoid with the flash drive, which causes the dual re-mount steps.
        15:43 <twb> That would do (slightly) more writes to the USB key, but probably fine.
        15:43 <rlaager> Would it write more?
        15:44 <twb> depends if the rsync blocks line up with the FAT and/or erase blocks, I guess

        15:43 <twb> It would also hose any writes by the bootloader or mainboard (or other OSs) to the USB key...
        15:44 <rlaager> But yes, it would overwrite any changes from elsewhere. That is a downside.


  * Why not just put /boot on the normal pool and use grub zfs.mod driver?
    Because then we wouldn't be able to use sexy new ZFS features until GRUB gets support for them, and
    grub isn't updated very often.
    (Currently I'm just *assuming* those features are important enough to be worthwhile.)

  * Why not just use rlaager's bpool and rpool?
    Because then we'd have to handle disk partitioning ourselves and I/O scheduling, instead of letting ZFS manage it.
    Because we hate grub (and update-grub and os-prober) and like refind, and getting the ZFS driver from grub into refind (via efifs project) is an extra hassle.

  * Why not just netboot?
    Because then we can't boot until the PXE server is running.
    Because we also want to use this design *on* the PXE server itself.

  * Why not just mount the ESP as /boot?
    Because then the ONLY copy of /boot is on the USB key, which is a SPOF.

  * Why not use BIOS instead of UEFI?
    Because the grub ZFS driver is just as limited on BIOS as on UEFI, i.e. it wouldn't help.
    Because we would also still need somewhere to store the stage0 and stage1, i.e. a disk whose partitioning is managed outside of ZFS.

  * Why not mirror the ESP so it's redundant?
    Because it's more failure-prone than just reflashing the SPOF USB key.

    At a minimum, we'd need to use md metadata=0.9 (so the md metadata lives at the END of the partition), and
    we'd need to make the FAT filesystem slightly smaller than the partition its on
    (so that if/when e.g. refind or the mainboard UEFI firmware writes to the FAT, it will never overwrite the md metadata).
    And even once that's done, the GPT partition tables would not be md mirrored, so if they ever changed, they'd need to be manually kept in sync.

    Possibly we could work around that by md mirroring the entire USB
    keys, with metadata=1.2, so that the disk layout would be
    "[GPT1][md metadata][FAT][blank][GPT2]".  But UGH.


[SOLVED] ZOL 0.8~rc4?
---------------------
* UPDATE 2019-05-20: I've been running 0.8~rc4 instead of 0.7.
  I see no reason to go back to 0.7 and fight the systemd start-order issues.

* ZOL 0.8 adds a systemd generator (zfs-mount-generator).
  This makes /etc/fstab obsolete and allows mounts to be done ENTIRELY inside ZFS.
  Without this, ZFS and systemd fight, and you have to jump through hoops to work around it.

* ZOL 0.8 adds native ZFS full-disk encryption.
  We want this for the offsite host.

  If we do it for new omega itself, then

  1) the offsite host *NEVER* needs to know the decrypt key; but
  2) every time omega has a power outage, someone has to provide the decrypt key.

  If we set up alpha the same way, the same applies to alpha.
  If we don't, then I *think* snapshots of alpha sent to the offsite DR will be unencrypted?
  I'm not too sure.

  This is still super experimental, so I'm not really comfortable with it.
  We will probably end up with the shitty situation of missing out on this by <1y, and
  having unencrypted new-alpha and new-omega, and LUKS encrypted offsite DR.


[SOLVED] How do I grow the refind ESP enough to put kernels in there?
--------------------------------------------------------------------------------
* gparted can grow a FAT32 filesystem
* parted 3.0+ has no "resize", only "resizepart", which explicitly does not touch the filesystem, only the partition.
* gparted is doing the resize by calling libparted's fat_resize(), AFAICT.
* libparted's provides no way to access fat_resize() from a script or CLI?
* no other tools, e.g. dosfsutils, seem to provide a filesystem-only growing tool a la resize2fs for ext4.
* UPDATE: apt install fatresize ???
* UPDATE: refind uses FAT12 now, which neither gparted nor fatresize actually support.
  We *have to* make a new (bigger) ESP partition, so we might as well make the whole ESP USB key ourselves.
  This also means we are running the refind from inside Debian (rather than "download from some guy's website").

  We use refind-install (install to ESP:\BOOT\BOOTX64.EFI) *not* the
  default behaviour (install to ESP:\REFIND\REFIND_X64.EFI & then use
  efibootmgr to "add" that path to the mainboard's list of boot
  options) because

  1) the latter doesn't work for USB keys on the "learning" server (old gigabyte mainboard); and
  2) the same problem might bite us latter, and we don't need >1 bootloader, so "stealing" the fallback bootloader's slot doesn't hurt us.


[SOLVED] How do we lay out the datasets in the ZFS pool?
----------------------------------------------------------------------
* Why do we have -o canmount=no "useless" datasets?
  It is just to avoid specifying -o mountpoint=X?

  Mostly, but russm says this also is definitely the recommended way Solaris ZFS worked when ZFS was new.

  Therefore twb convinced to just carry around these "extra" "useless" intermediary datasets.

* Is it sensible to group datasets "logically" instead of "as
  mounted"?  e.g. ::

    OS/root,
    DATA/USER/twb/home (/home/twb),
    DATA/USER/twb/mail (/var/mail/twb),
    DATA/USER/ALL/mail (/var/mail/Lists),
    DATA/USER/ALL/biz (/srv/biz)

  russm asks: what does that GAIN us?

  twb says: we can say things like "What's the growth on twb's data?" and "Do another backup, but ONLY of twb's data".

  russm says: but OTOH it's more confusing (compared to pool structure matching mount structure),

  Therefore twb convinced to make the pool/dataset structure match the mountpoint structure.

  * PS: in addition to pool-structure-matches-mount and
    pool-structure-is-logically-grouped, we also considered
    pool-structure-is-flat, like we do for LVM (because we have to
    for LVM) e.g. ::

        root
        home-twb  (/home/twb)
        var-spool-cron (/var/spool/cron)

  * PS: since this is a singleton server, we CAN just move things from
    their traditional locations in many cases, e.g. /var/mail can move
    to /srv/mail.  Is this sensible, or a waste of time?

    russm says overall waste of time;
    twb doesn't really care.

* Also, should the ZVOLs (VM disks, plus the ESP backup) go at the root of the pool, or what?

  russm says: put them under pool/ZVOLs, because by default zpool/zfs
  commands don't show the zvols, but the "ZVOLs" dataset will still
  show up, basically as a reminder to look at zvols as well.

  We might have called it "VMs" instead of "ZVOLs", but there's on
  (ESP backup) that isn't a VM, so... meh.


[SOLVED] initramfs-tools or dracut?
-----------------------------------
Both can be MADE to work.
As at 2019-05-06, I (twb) am inclined to stick with initramfs-tools.

Arguments against dracut:

* initramfs-tools is the default on Debian and the best understood & supported by Debian people.
* dracut is missing some hooks, so it might not reliably update the ESP version when it is rebuilt
* dracut is missing some hooks, so it might not reliably rebuild itself when it should
* dracut uses bash and coreutils (not busybox).  This makes the ramdisk bigger, slower, and builds less reliably.

  Large parts of dracut are written as bash scripts, instead of by
  "systemd-y".  The whole dracut stack internally looks at least as
  unreliable and error-ignore-y as initramfs-tools.

* dracut is supposed to include /etc/hostid, but our system seems to be buggy.

  This means hostid must be manually passed by us using
  dracut-zfs-specific "spl_hostid=0xFFFFFFFF" boot option.
  I haven't been able to isolate the problem here.

Arguments against initramfs-tools:

* ZFS support lands in dracut first (for RH/Fedora people), and is then ported to initramfs-tools.
* initramfs-tools doesn't use systemd, so anything "brought up" in the ramdisk might not hand over cleanly to systemd.

  The obvious example would be zed.  I haven't checked yet, but it is
  very likely that zed simply isn't started in the ramdisk by *EITHER*
  ramdisk, meaning that any ZFS events during the zpool import and
  rootfs mount don't trigger e.g. warning emails.
  This is PROBABLY not a big deal, but I haven't explicitly asked #zfsonlinux about it yet.

* initramfs-tools is missing /run/initramfs/shutdown support, so the ZFS pool is never fully exported during shutdown.

  * Debian has a suggestion to fix this, but it's not well tested.  When I tried it, it didn't Just Work.
  * The ZFS people say that "zfs export" really just updates hostid
  * The inability to umount /var/log/journal during shutdown is an UNRELATED bug, which happens to EVERYONE that has separate / and /var/log filesystems.
    This is caused by systemd, and systemd doesn't care, and #systemd believes it's not a big deal.


Samba
-----

* How do we do ppolicy things (brute-force lockout, lock account, expire passwords) under Samba?
* How do we make sure Samba is only using strong, salted hashed (e.g. SSHA2) for passwords?
* What *is* the "sysvol"?
* How do I put sudoers.ldap into the Samba LDAP schema?
* UPDATE 2019-05-20: nobody is helping ANYONE in #samba and AD is full of MS weirdness;
  I think there is not enough benefit to be worth the cost!
  For now I am using flat files (/etc/shadow) and doing other stuff; might come back to this later.


Ansible / Configuration Management
-------------------------------------

2019-05-15 #emacs::

    13:46 [twb gives up on ansible again after a record minimum time!]
    13:51 <dale> twb: Ha
    13:51 <dale> twb: Replacement?
    13:51 <twb> ugggggggh
    13:52 <parsnip> ansible is too fast?
    13:54 <twb> ansible is too overwhelming
    13:55 <dale> I've spent so much damn time with it that I no longer have a good perspective on that.
    13:55 <twb> I pretty much just want to start with "apt install <list>", then rsync across <these files>
    13:56 <twb> Even the basic ansible examples have like 50 files across 10 dirs
    13:56 <twb> Reminds me of ROR
    13:57 <dale> Yeah, figuring out how to organize everything according to their "best practices" has always seemed murky to me.
    14:00 <twb> I want an example that isn't "best practice" but will at least *do something* straight away
    14:00 <twb> Also the fact that all their playbooks do "yum:" is pretty un-generic
    14:18 <dale> twb: I think this would work but I totally didn't test it: http://ix.io/1J5m
    14:26 <twb> dale: thanks
    14:28 <twb> dale: that looks pretty close to where I was when I gave up
    14:28 <twb> dale: the other thing is I was trying to run it on localhost without sshing from localhost to localhost
    14:28 <twb> dale: which Just Works when using the secret hidden built-in "localhost", but not if you define a host called "localhost"
    14:29 <dale> twb: There's a thing for that, hang on
    14:29 <twb> That's how I used to use puppet, long long ago
    14:30 <dale> twb: Try putting "localhost ansible_connection=local" in that hosts.ini.

Put these (ansible.cfg, hosts.ini, site.yaml) in a directory and run::

    ansible-playbook --check site.yml      # test
    ansible-playbook         site.yml      # do it for real

site.yaml::

    - hosts: all
      tasks:
        - name: Install some stuff
          apt:
            name: emacs24, emacs25, emacs26
            state: present

        - name: rsync some stuff
          synchronize:
            src: some/local/dir
            dest: path/on/the/remote

ansible.cfg::

    [defaults]
    inventory = hosts.ini

hosts.ini::

    your.host.com



Containers (or not)
-------------------
* We want to constrain how much any given daemon can stomp on other daemons.
* HOWEVER, we don't want to set up a full rootfs (inc. syslog, cron, apt, &c) for each daemon,
  because we used to do that, and it still annoying overkill that required additional babysitting.

* In particular, I want to have e.g. samba see only lo and eth0
  interfaces, and nsd3 see only lo and eth1 interfaces.  Possibly
  (probably!) instead of actual eth0 and eth1, they're virtual
  interfaces that are all bridged to a single physical interface, and
  each one only has a single IP address assigned to it.

  (But... why?  The theory is that nsd3 spins up without any
  privileges, so even if nsd3 is pwned, it can't simply steal
  resources (inc. addresses) from other parts of the host.
  Is this sensible?)

* https://unix.stackexchange.com/questions/506441/why-does-systemd-nspawn-n-network-namespace-not-show-in-ip-netns-list
  seems to say that regular netns aren't the same as whatever the fuck systemd-nspawn (systemd-containers) is doing.
  My brain hurts!

* MAYBE what they expect me to do is have like nsd3.service PrivateNetwork=yes, AND THEN create-iface-for-nsd3.service with JoinsNamespaceOf=nsd3.service ExecStart=/sbin/ip set dev eth0 netns SOMETHING ???

  https://github.com/systemd/systemd/issues/2741  seems to be describing a similar setup (eventually).

* UPDATE: I tried following the instructions in that github issue, but all I got was ip(8) segfaulting.
  It's not clear to me if this is achievable AT ALL.

  systemd-nspawn has exactly this kind of magic (veth / netns), but it's not clear how to access it without a separate, full rootfs.
  Can I maybe do something like this, so that inside the container it "only" runs the one service we want (and anything it needs)? ::

    systemctl-nspawn --root=/ --args='quiet nosplash -- --unit=nsd.service'

  For now (2019-05-20), I am leaving this issue and looking at other stuff.


Which IRCd?
------------------------------
* We used to just install ircd-irc2, which is the OLDEST and DUMBEST irc server, based on "YAGNI" and laziness.

* We don't actually need *ANYTHING* clever,
  except for channel logging and announcing alloc status changes (poll an RSS feed and broadcast it into IRC).

  We don't even need TLS + SASL (encryption + authentication) because
  currently we simply require IRC to happen over SSH.  (Except maybe Ron would value that.)

  UPDATE: ron is VERY interested.

* https://upload.wikimedia.org/wikipedia/commons/d/d5/IRCd_software_implementations3.svg
* https://en.wikipedia.org/wiki/Comparison_of_Internet_Relay_Chat_daemons
* None of the ircds in Debian have existing systemd lockdown.
* None of the ircds in Debian have existing apparmor lockdown.
* Of the ircds in Debian, several have TLS; only charybdis has SASL.
* Let's use charybdis for now and see how we go?
* UPDATE: to provide SASL you need *both* an ircd *and* a set of "privileged" bots, which is either "anope" or "atheme-services" package.
  The "atheme-services" package is the one written by the charybdis maintainers.


Which IRC bot?
------------------------------
* supybot is dead
* we need it to 1) join #cyber, 2) log conversation therein, and 3) announce "news" from alloc's RSS feed
* https://github.com/Supybot/Supybot directs us to https://github.com/ProgVal/Limnoria, which exists in Debian 10.
* Let's start with limnoria for now.



References
==========
* login.cyber.com.au:/srv/vcs/flash-kernel-efi.git
* `lshw-omega-understudy.html`_
* https://www.supermicro.com/products/motherboard/Xeon/C600/X9SRi-F.cfm
* https://github.com/zfsonlinux/zfs/wiki/Debian-Buster-Root-on-ZFS  (Using ZFS 0.7)
* https://github.com/zfsonlinux/zfs/wiki/Debian-Buster-Encrypted-Root-on-ZFS  (Using ZFS 0.8~rc4 from experimental)
* https://github.com/pbatard/efifs
* https://efi.akeo.ie/downloads/efifs-1.3/x64/zfs_x64.efi
* https://github.com/hn/debian-buster-zfs-root
* https://raw.githubusercontent.com/hn/debian-buster-zfs-root/master/debian-buster-zfs-root.sh
* http://open-zfs.org/wiki/Feature_Flags
* https://manpages.debian.org/buster/manpages/hier.7.en.html
* https://manpages.debian.org/buster/dracut-core/dracut.8.en.html#SEE_ALSO
* https://manpages.debian.org/buster/zfsutils-linux/zfs.8.en.html
* https://manpages.debian.org/buster/zfsutils-linux/zpool.8.en.html
* https://systemd.io/BOOT_LOADER_SPECIFICATION   # UPDATE: not really helpful.
* https://www.freedesktop.org/wiki/Specifications/DiscoverablePartitionsSpec/  # This is what I thought the previous link was!!!

Account management (LDAP, krb, DNS)

* https://wiki.samba.org/index.php/Active_Directory_Naming_FAQ
* https://wiki.samba.org/index.php/Setting_up_Samba_as_an_Active_Directory_Domain_Controller




IRC Logs
========

2019-04-17  #cyber::

    <twb> ron: old Z68 mobo is now running UEFI firmware and can boot refind
    <twb> FTR, the process was:
    <twb> 1. download .exe from gigabyte's website (requires js)
    <twb> 2. from windows, run the .exe, which happens to be a .7z self-extractor (so PROBABLY could use 7zip on linux)
    <twb> 3. from that, get the FLASHEFI.EXE and FOO.U1F files
    <twb> 4. download FREEDOS 1.2 LITE USB .zip, unzip it, get FD12LITE.IMG.
    <twb> 5. sudo cp FD12LITE.IMG /dev/sdz
    <twb> 6. gparted /dev/sdz, grow the FAT16 partition from ~30 MB to ~62 MB
    <twb> 7. sudo mount /dev/sdz1 /mnt; sudo cp FLASHEFI.EXE FOO.U1F /mnt
    <twb> 8. plug the USB key into a **USB2** port, because FreeDOS doesn't have a USB3 (XHCI) driver?
    <twb> 9. boot freedos, choose "english" choose "fuck off"
    <twb> 10. type "FLASHEFI FOO.U1F" to run the flashing program, wait a few minutes while it runs.
    <twb> 11. can't exit the program, so power off, remove power cable, replace power cable, turn on, hooray, you have an UEFI firmware

2019-04-18  #debian-au::

    12:07 <k-man> twb: regarding zfs for rootfs, no, I didn't do that on my laptopt
    12:07 <k-man> as it seemed too difficult at the time
    12:08 <k-man> in fact, I don't store much, if any data on my laptop that isn't backed up elsewhere so at the moment, my laptop isn't backed up - which I guess isn't ideal
    12:08 <twb> heh
    12:08 <k-man> I've been thinking of re-installing buster, and trying to use zfs for rootfs
    12:09 <k-man> there is this blackbox stript to do it. I've not tried it though: https://github.com/hn/debian-buster-zfs-root
    12:11 <twb> k
    12:11 <twb> I'll read that, but I'm planning to do the install via debootstrap, I think
    12:11 <k-man> righto
    12:11 <k-man> is this for yourself or someone else?
    12:11 <twb> this is for my new main server at work
    12:12 <k-man> nice
    12:12 <k-man> is that your choice to use zfs?
    12:13 <twb> yes
    12:13 <twb> AFAICT btrfs has lost
    12:13 <k-man> wow, interesting
    12:14 <k-man> is it a race where there can only be one winner?
    12:14 <twb> Well it seems crazy to have two good filesystems
    12:15 <k-man> yes
    12:15 <k-man> i think zfs had a richer featureset than btrfs - which meant that btrfs was always going to be playing catchup
    12:17 <twb> AFAIK the main missing features in ZFS are:  1. linux-compatible license; 2. per-file nodatacow; 3. EFI driver (i.e. /boot can be inside the ZFS array)
    12:18 <twb> All of which are annoying, but offset by a. built-in bcache equivalent (ZIL, L2ARC); b. fully automatic resilver to spare drive
    12:19 <twb> Oh also ZFS dedup is post-facto and expensive; btrfs dedup is pre-facto and cheap, but requires you to opt into it
    12:19 <k-man> right
    12:19 <twb> But in both cases the end result is mostly "nobody has dedup"
    12:19 <k-man> just about everything I read about ZFS dedup says "don't use it"
    12:19 <k-man> yeah
    12:19 <k-man> i really like ZVols too
    12:20 <k-man> which BTRFS doesn't do afaict
    12:20 <twb> btrfs doesn't need them
    12:20 <twb> regular files work just as well
    12:20 <k-man> isee
    12:20 <twb> both of them suck at quotas
    12:20 <twb> per-user quotas, I mean
    12:21 <twb> the recommended method is "make a separate zfs for each /home/foo and set a quota on that zfs"
    12:22 <twb> -o com.sun:auto-snapshot=false  I'd have never found that one
    12:24 <twb> That script weirdly makes / and /var separate filesystems
    12:24 <twb> And it's written in bash, but only sh is available in the place he claims to be running it
    12:59 <k-man> twb: he mentioned in the readme that /var had to be seperate for some reason
    12:59 <k-man> something to do with the timing of mounting the zfs fs
    12:59 <k-man>  /var has to be mounted from /etc/fstab rather than the ZFS way
    13:00 <twb> k-man: his readme seems to be explainin why he had a workaround to support a separate /var, but not why he wants /var separate in the first place
    13:00 <k-man> oh ok
    13:00 <k-man> doesn't debian do that if you choose the "everything in seperate partitions" option?
    13:01 <twb> dunno, maybe
    13:01 <twb> having a separate /var is stupid IMO
    13:01 <twb> The reason that's A Thing is because originally / and /usr were read-only and shared between 100 computers
    13:01 <twb> and /var was the per-computer part
    13:01 <k-man> twb: ok. I'm not arguing- just speculating that that was his reasoning
    13:01 <k-man> ah ok
    13:02 <twb> But having /usr separate isn't even supported anymore; it was about 60% broken and then systemd forced that to be 100% broken and officially abandoned
    13:02 <k-man> ah

2019-04-18 #zfsonlinux::

    12:56 <twb> I've babysat a couple of Debian + ZOL hosts before, but I'm setting up my own for the first time.  I also want the rootfs to be on ZOL.  As well as the notes in /topic, my main reference is https://raw.githubusercontent.com/hn/debian-buster-zfs-root/master/debian-buster-zfs-root.sh  (from https://github.com/hn/debian-buster-zfs-root).
    12:57 <twb> I notice that's adding *partitions* to the pool, instead of whole disks.  Isn't that a Bad Thing?
    12:57 <rlaager> No, it's fine. I fought the good fight for wholedisk, but it's just not the right fit here.
    12:57 <twb> Because you need an ESP still?
    12:58 <twb> And a /boot, I guess
    13:00 <rlaager> twb: Right, and now we're doing two pools: bpool for /boot and rpool for /
    13:00 <twb> rlaager: who is "we" ?
    13:03 <twb> Is there a ZFS EFI driver yet?  When using BTRFS, my preferred way to handle this is to have whole-disk elements in the pool, and then have a completely static refind ESP on an internal USB key.  Since refind has a btrfs EFI driver, /boot can just be part of the normal btrfs pool.
    13:04 <twb> There's still a SPOF for that USB key, but it's read-only and not device-specific, so it's trivial to replace
    13:06 <twb> I think refind's btrfs EFI driver was stolen from grub2, and I see there's a /usr/lib/grub/x86_64-efi/zfs.mod... not sure how to turn that into an .efi...

    FIXME I had a link from somebody to a ZFS EFI driver from the "efifs" project, but firefox crashed by laptop and I lost it.


    13:01 <gurg> btw rlaager, is this your tutorial since it mentions you?
    13:01 <gurg> https://github.com/zfsonlinux/zfs/wiki/Ubuntu-18.04-Root-on-ZFS
    13:02 <gurg> Because I was wondering if I should open an issue for the hints for step 2.3 "creating the boot pool" because the hint command there for if doing a raidz says rpool instead of bpool
    13:10 <rlaager> gurg: Typo fixed. Thanks!


    14:50 <twb> Hey, did you know your https://github.com/zfsonlinux/zfs.wiki.git has fsck errors
    14:50 <twb> You can't clone it unless you turn off transfer.fsckobjects=true in git
    14:51 <twb> It's almost entirely linear so you might get away with actually fixing it, instead of just having to live with it


    15:26 <twb> The ZOL wiki links to some introductory blog posts https://pthree.org/2012/12/07/zfs-administration-part-iv-the-adjustable-replacement-cache/
    15:26 <zfs-bot> [ Aaron Toponce : ZFS Administration, Part IV- The Adjustable Replacement Cache ] - pthree.org
    15:26 <twb> That says ZIL >1GB isn't really useful.  Is that still true?
    15:27 <twb> And if so, is it silly to get two separate ~100GB SSDs, one for ZIL and one for L2ARC?
    15:28 <twb> I did that last time, but maybe that was just because partitioning disks is lame
    15:29 <jtara> twb it all comes down to your workload
    15:29 <jtara> a SLOG (separate ZIL) is almost always a good idea for a pool that will see significant sync writes, even if it's no faster than the other disks
    15:29 <jtara> l2arc can help, sometimes, sort of...but imo it really is more for salvaging an awful situation than anything else
    15:29 <jtara> memory is better when you don't have to fake it ;)
    15:30 <PMT> yeah but at the time it was built you weren't usually cost-limited for arbitrary amounts of memory, but HW limited because you couldn't stick more in =P
    15:32 <twb> aren't SSDs cheaper than "buy more RAM" still?
    15:32 <jtara> the times i've seen gains with it align are when metadata is badly fragmented from data and you are suffering from it
    15:32 <jtara> yeah but l2arc helps quite a bit less than just more arc
    15:32 <twb> k
    15:33 <jtara> if you have a trashed pool (logbias=throughput, primarycache=metadata to try to cut down on random read iops) then yeah l2arc can help make an awful situation better
    15:33 <jtara> but so much better to just not get into that in the first place
    15:34 <twb> Looks like for comparable capacity, RAM costs is about four times SSDs cost
    15:35 <twb> oops, I can't math
    15:36 <twb> it's more like 32 times
    15:37 <jtara> imo get a pool going well, look at your arc hit rates, then decide if its worth it
    15:37 <twb> That's fair
    15:38 <jtara> i did zfs consulting for database setups for many years, l2arc really was only beneficial a small amount of the time
    15:38 <twb> because (assuming you kept an AHCI slot free) you can always add the l2arc in <1h downtime
    15:38 <PMT> twb: cheaper isn't what I meant, I meant "no money short of a custom Cray thing would buy you more"
    15:39 <jtara> if l2arc makes or breaks your setup, ime something is very weird.  it can help, but it won't rock your world.
    15:39 <twb> PMT: IIUC you're saying that when l2arc was invented, you simply couldn't put more ram in the server.  Now, you can (although it's still a bit more expensive than SSDs)
    15:40 <jtara> if you can't fit enough ram, try to drive up your average recordsize if you can
    15:40 <jtara> that will decrease metadata overhead and help
    15:41 <jtara> old trick from the sun days anyway
    15:41 <twb> I did have one server that was struggling for some reason, and one of the things I did was add ZIL SLOG and L2ARC on NVME SSDs, and the problem went away.  So I guess I (falsely) internalized that adding SSDs for SLOG and L2ARC is a cheap and easy thing you should ALWAYS do.
    15:42 <PMT> twb: you still have limits, but yes, you could put in much less RAM at almost any cost.
    15:42 <twb> PMT: understood
    15:42 <jtara> the big thing a slog will do that surprises most people, is to decouple rmw reads and compression from the point of time of write
    15:42 <jtara> without one, big sync writes get rmw and compression inline
    15:42 <jtara> which can really be a dramatic dofference
    15:42 <twb> rmw?
    15:43 <jtara> read modify write.  like if you write 32k of a 128k record and the rest needs to get read to merge it
    15:43 <twb> I think I understand that
    15:44 <jtara> basically deferring rmw is good because eventually you may get all the pieces and can avoid the read
    15:44 <PMT> twb: ZFS ~never overwrites things in place. So if you modify some or all of a block, the entire new block gets written separately somewhere else, then it may mark the old one as not needed any more. But since that means reading the old one, modifying the contents, and writing it out separately, you call it [...]
    15:44 <twb> I did something like that once to linearize / cohere write spam to RRD files
    15:45 <jtara> anyway, learn and love zpool iostat, especially -r
    15:45 <jtara> it tells you a lot about what's really happening
    15:46 <PMT> sometimes it tells lies.
    15:46 <PMT> Not often, but sometimes.
    15:47 <jtara> it helps to turn off aggregation and to cross reference with blktrace, i do admit
    15:48 <jtara> but for a "wtf is going on with this pool" peek it's hard to beat
    15:49 <twb> I've used it before.  And sar / iostat on pre-ZFS systems
    15:51 <twb> jtara: does having compression "in the loop" like you were talking about, does that matter if it's a realtime compression algorithm like LZ4 / LZO?
    15:52 <jtara> it does, because usually sync write concerns are driven by latency
    15:53 <jtara> so even a pretty fast compression alg can have significant latency impact
    15:53 <twb> Hrm, OK.
    15:53 <jtara> but usually a bigger concern are rmw reads
    15:53 <jtara> when you make a write wait for a read, it's bad
    15:54 <jtara> i almost always recommend deferring them to txg commit time when you can
    15:54 <jtara> which means either a slog or a very high zfs_immediate_write_sz
    15:55 <twb> So is this a reasonable rule-of-thumb?  1) a SLOG ZIL is always useful; 2) an L2ARC is only really useful when you can't get more ARC and/or your system is badly configured
    15:55 <jtara> thats my opinion anyway, there are others out there
    15:56 <jtara> but that's the classic solaris zfs approach and it works well
    15:58 <twb> Is it true that a SLOG ZIL >1GB is a waste of time?
    15:59 <PMT> twb: the rule of thumb is generally X seconds of sync IO for the main pool
    15:59 <jtara> a slog stores sync writes (and all writes in their sync domains) between txg commits
    15:59 <PMT> usually X is 5
    15:59 <jtara> usually you don't need a lot
    15:59 <twb> PMT: so like if I do zpool iostat and I see 100MB of writes per second, my SLOG should be around 5*100MB = 500MB?
    16:00 <PMT> twb: if that's how long you make txg sync length, yeah. You might adjust that for other reasons. But generally it's X seconds of maximum for the pool, not maximum you've seen.
    16:00 <jtara> there's overhead for every write and for every 128k or so block
    16:01 <PMT> Padding it is fairly minimally risky. Undersizing it makes it more useless than you might want.
    16:01 <jtara> just oversize it 10x what you think you'll ever need and don't worry
    16:01 <PMT> If you can do that, sure.
    16:01 <twb> PMT: OK so it would be more like 5 seconds times the sustained write speed of all the HDDs in the pool?
    16:02 <twb> In my immediate case it's easy for me to give it 10GB or 100GB instead of 1GB, I'm just trying to understand where the numbers come from
    16:02 <PMT> twb: generally? if it were raidz vdevs I'd probably suggest something like X seconds times the total throughput of data disks per pool, but the overprovisioning suggestion means you probably shouldn't bother caring about the difference.
    16:02 <twb> OK cool
    16:02 <PMT> twb: your goal is for it to be able to take all the sync IO from a txg without needing to wait on the disks to catch up.
    16:02 <jtara> if you really want to precision size it, take zfs_dirty_data_max and double it
    16:03 <jtara> that should be worst case
    16:03 <jtara> seriously if you run out then you will just slam into a complete wall until the txg gets committed in full
    16:04 <twb> jtara: OK, although it helps to be able to estimate the number BEFORE I put in a purchasing request, let alone set up the pool :-)
    16:04 <jtara> can you even get disks that small though
    16:05 <PMT> Sometimes.
    16:05 <PMT> The other thing is that as disks get smaller they often have lower performance because they need less flash. ;)
    16:05 <twb> So let me use some real numbers here... sustained write for a WD Red 4TB is 150MB/s.  150MB/s * 5s * 4 disks in a RAIDZ1, yields about 3000MB, or 3GB
    16:05 <jtara> ok, say you're writing compressible data, like text
    16:06 <PMT> Are you sure the drives only do 150 MB/s, max?
    16:06 <twb> So if the smallest SSD I can buy anyway is 100GB, I can give 10GB to the SLOG and 90GB to L2ARC just because it's lying around anyway
    16:06 <twb> PMT: https://www.wd.com/content/dam/wdc/website/downloadable_assets/eng/spec_data_sheet/2879-800002.pdf
    16:06 <jtara> so for your raidz to handle 3000mb, your slog might have to handle 9000mb
    16:06 <PMT> twb: caveat, data in the L2ARC takes up space in the ARC. So it's not just a free set of more RAM.
    16:06 <jtara> or even
    16:06 <jtara> over 9000.
    16:06 <PMT> twb: heh, 4TB.
    16:07 <jtara> try to resist the temptation to do something else with a slog device
    16:07 <jtara> but, people do
    16:07 <twb> This is for what you might call a "pet server"; the entire dataset fits into 1TB currently
    16:08 <twb> jtara: OK.  So buy the smallest SSD and let the SLOG have all of it, and it might be massively overprovisioned, but who cares
    16:15 <jtara> my customers always care about predictable latency and smooth degradation and stuff like that...so yeah


    18:42 <twb> rlaager: in https://github.com/zfsonlinux/zfs/wiki/Debian-Buster-Root-on-ZFS §2  you use mdadm and sgdisk to wipe existing config on a disk.  You might want to also use wipefs(1), which removes everything that blkid(1) can see
    18:49 <twb> Ugh, I was wondering why gdisk instead of libparted --- turns out the ZFS support in libparted is Ubuntu-specific


2019-04-23 #zfsonlinux::

    12:31 /join #zfsonlinux
    12:34 <twb> rlaager: re zfs.wiki/Debian-Buster-Root-on-ZFS.md, you said last week that you wanted to give ZFS the whole disk(s), but had to give up in that HOWTO (because grub zfs is feature-limited, and because ESP).
    12:35 <twb> $coworker has a different idea and I want to know how much you (all) hate it
    12:35 <twb> 1. ESP is stock refind on a USB key inside the chassis, because refind Just Works, and then it's a completely static ESP that's easy to replace
    12:35 <twb> 2. except that refind has the same problem as grub, so instead of making /boot separate, leave it on the ZFS, but have an apt post-install hook that copies /boot onto the ESP
    12:36 <twb> If the ESP drive goes tits-up, you need to boot a live system to recover, instead of just making another stock refind USB key, but that's not TOO hard, and it means you can continue to give full disks (instead of partitions) to ZFS.  Which is/was a Super Nice Thing
    12:39 <PMT> twb: i mean, that assumes the existence of an internal USB key, which is a bit of a specialized way to solve this
    12:39 <twb> Granted
    12:39 <twb> That's our (work's) standard way to work around EFI having no concept of mirrored ESP
    12:40 <twb> You make the ESP the same across all systems, so it's more like a network card than a software configuration, and if it breaks you just install a new one
    12:41 <twb> grub zfs.mod limitations won't let me make it EXACTLY the same (at least, if the pool is going to use newer features), so #2 is my workaround for that
    12:42 <PMT> I mean, at that point, why not just netboot refind?
    12:43 <twb> I guess mainly because that makes it dependent on the PXE server already being up.  Also one of the targets for this design *is* the netboot server
    12:44 <PMT> So you have multiple netboot servers and they boot from each other. :V
    12:44 <twb> heh, fair
    12:46 <PMT> pts0: i mean, everything you've never done before is complex, so
    12:46 <twb> Looking at the issue from the other end: Solaris ZFS used to *really* want whole disks, not partitions (AFAIK).  Is that (disks not partitions) something ZOL still cares about?
    12:49 <PMT> twb: Solaris wanted whole disks to play with the write cache settings. ZoL wants whole disks to set the whole disk's IO scheduler to noop or equivalent. If you can't use the whole disk, you could still script doing that.
    12:49 <twb> Ah OK
    12:49 <PMT> (if it's not a whole disk you also need to go fiddle with the partition tables to expand a device, I believe, but vOv)
    12:49 <jasonwc> DeHackEd, Cisco already listed the Optane DIMMS for sale.  $2000 for 128GB and $21k for 512GB.  --> https://www.storagereview.com/cisco_details_intel_optane_dc_persistent_memory_pricing
    12:49 <zfs-bot> [ Cisco Details Intel Optane DC Persistent Memory Pricing | StorageReview.com - Storage Reviews ] - www.storagereview.com
    12:50 <PMT> lmfao
    12:50 <PMT> quite the goddamn dimm
    12:50 <twb> Does ZFS care if the partitions were created with parted --align=optimal (or whatever the gdisk equivalent is)?
    12:51 <PMT> ZFS doesn't, but the drives do. :D
    12:51 <twb> k
    12:51 <jasonwc> they have a review of a Supermicro server with those persistent DIMMs.  "Our first test is the 4K random read test here the persistent memory started at 1,371,386 IOPS at 4.6μs and went on to peak at 13,169,761 IOPS at a latency of only 12.1μs. "
    12:51 <pts0> PMT so it sounds like mdadm with mbr partitions booting to bios boot mode is the way to given all that twb is going through
    12:51 <PMT> That's mostly a joke - using non-physical block size aligned partitions is an invitation to pain and suffering.
    12:51 <PMT> pts0: what?
    12:52 <twb> PMT: yeah understood, I just half-expected you to say "ZFS is so smart it'll automatically fix that for you if you screw up"
    12:52 <pts0> so just ext4 a root and then a boot partition and do a bios as opposed to a uefi boot
    12:52 <PMT> pts0: ...how did you get there from any of this discussion.
    12:52 <pts0> zfs on boot with uefi is crazy complex
    12:52 <twb> PMT: I think they're thinking of a md raid1 ext /boot, and the UEFI->BIOS thing is just a conflation
    12:52 <pts0> what i said is much simpler
    12:53 <pts0> no forget uefi
    12:53 <PMT> twb: i know, i'm just unsure why they think this is helpful.
    12:53 <pts0> it won't even see the stuff right without a bunch of fanagling
    12:53 <jasonwc> pts0, What's so complex about it? It's documenteed by rlaager
    12:53 <twb> pts0: are you suggesting that the non-UEFI (BIOS) ZFS drivers in grub are *better* than the UEFI ZFS drivers in grub?
    12:53 <pts0> no i'm saying no zfs for the boot and root
    12:53 <pts0> and just do a bios boot
    12:54 <PMT> pts0: you appear to be advocating a solution to a problem nobody else thinks is a problem.
    12:54 <jasonwc> You can do BIOS boot with separate boot and root pools
    12:54 <jasonwc> You can also do a BIOS boot with a single root pool if you don't need features Grub doesn't support
    12:54 <pts0> seems to complex to me from everything i've looked at
    12:54 <pts0> 100 damn steps
    12:54 <jasonwc> It's just thoroughly documented.  It's not a hard process.
    12:54 <twb> I don't want pre-UEFI BIOS, and I want / on ZFS.  Having /boot on ZFS is "nice to have" but not critical..
    12:55 <rlaager> pts0: You're free to do whatever you want on your systems, of course, but this works fine and is well documented. It's also the way forward if this ever gets distro integration, which is being considered.
    12:55 <pts0> Can someone give me a link to the documentation
    12:55 <pts0> maybe i saw the wrong thing
    12:55 <twb> pts0: zfs.wiki/Debian-Buster-Root-on-ZFS.md
    12:55 <jasonwc> pts0, What distribution are you planning to use?
    12:55 <twb> pts0: so uh... https://github.com/zfsonlinux/zfs/wiki/Debian-Buster-Root-on-ZFS
    12:55 <pts0> ubuntu
    12:56 <twb> pts0: there is a similar page for Ubuntu LTS on that wiki
    12:56 <jasonwc> pts0, https://github.com/zfsonlinux/zfs/wiki/Ubuntu-18.04-Root-on-ZFS
    12:56 <pts0> yeah...that's the one i saw
    12:56 <pts0> it's above my brain capacity
    12:56 <pts0> ugh
    12:56 <jasonwc> pts0, For one thing, it documents both an encrypted and non-encrypted setup
    12:56 <jasonwc> For another, it includes every step, so you just need to follow the instructions once.
    12:56 <twb> pts0: most of it is standard, e.g. the ESP stuff is the same as if Ubuntu's debian-installer script was doing it.
    12:57 <rlaager> twb: If you put /boot into the ESP, then yes, your alternative proposal works fine, and ZFS can have whole disks for the rpool.
    12:57 <pts0> I just can't understand it
    12:57 <jasonwc> pts0, What don't you understand?
    12:57 <pts0> I wonder if they were to put it in sections--maybe taking out the encrypted part,etc.
    12:57 <twb> rlaager: OK, thanks.  I was mainly hoping for a response of "oh, interesting idea" or "don't do that, because <reason>!"  :-)
    12:57 <pts0> put the encrypted part in another link
    12:57 <pts0> i dunno
    12:58 <pts0> Then it's just not explained well
    12:58 <pts0> run sgdisk with crazy switches
    12:58 <pts0> well i've never run sgdisk before
    12:58 <pts0> i've used fdisk
    12:58 <rlaager> twb: This also works if you have separate disks generally. My tentative plan for my personal server is to replace everything with the following: 1) A mirrored pair of SSDs. They will be partitioned for BBP (because this is BIOS only hardware but could be ESP), bpool, and a "special" (allocation classes) partition for the rpool. 2) The spinning disks will be a raidz3 with wholedisk.
    12:58 <pts0> and another one so I dont' even know what it's doing
    12:58 <twb> pts0: fdisk -> gdisk is just MBR -> GPT
    12:58 <jasonwc> pts0, I mean, you can manually partition with fdisk
    12:59 <rlaager> twb: The main reason to NOT do it in a server environment is the lack of mirroring, which you've acknowledged.
    12:59 <pts0> all day long
    12:59 <pts0> but what's with all the switches
    12:59 <jasonwc> pts0, In fact, I"ve done it.  You can also use the commands provided and it will work.
    12:59 <pts0> then they don't talk about how to get the disk uuids or any explanation for the sizing and zfs switches
    12:59 <pts0> just do all this craziness
    12:59 <twb> rlaager: OK, I understood all that except for the special allocation classes thing
    12:59 <pts0> ok, my main disk dies
    12:59 <pts0> then what
    13:00 <pts0> i have to go into the bios and repoint it to another disk or something
    13:00 <rlaager> pts0: If you have concrete suggestions, I'm open to adding more explanations. It does make it longer, but people seem to like the explanations generally, as they can skip them if they don't care.
    13:00 <pts0> the format arch uses always helps me
    13:00 <pts0> i don't know why
    13:00 <jasonwc> I suppose it would be nice if there were expandable sections so you could just follow the one you wnated
    13:00 <pts0> yeah
    13:00 <pts0> yes
    13:00 <rlaager> twb: It's a new in 0.8.0 feature that puts the metadata on separate devices, in this case, SSDs. So the hope is I'd get a useful mix of capacity from raidz3 and metadata performance from SSDs.
    13:00 <twb> rlaager: ah, OK.  I have seen a similar thing in btrfs, I think
    13:01 <jasonwc> but not sure if the Wiki even supports that
    13:01 <rlaager> I'm not sure either. Maybe I could look at readthedocs.io or whatever the kids are using these days.
    13:01 <pts0> rlaager for starters make two separate links for encrypted vs non-encryped
    13:01 <pts0> to clear up the cluttered appearance...but other than that...truth is i dont' know what the hell im doing so
    13:01 <pts0> this is going to be a tough one
    13:02 <jasonwc> pts0, Try following the instructions in a VM.  I bet you'll find it is easier than you think.
    13:02 <jasonwc> pts0, and if you break the VM, no big deal
    13:02 <rlaager> Another option would be to have the .md output from some script. Then it could write out BIOS vs UEFI; encrypted vs. not, etc. And we wouldn't care how many different outputs there were, because it's automated. Then just have the users click some links to get the right version.
    13:02 <rlaager> But another way to do this is to write this into the OS installer, which is hopefully happening.
    13:02 <pts0> so what happens if my main disk dies
    13:02 <pts0> do i have to go into the bios
    13:03 <rlaager> pts0: "Hints: ... ls -la /dev/disk/by-id will list the aliases."
    13:03 <jasonwc> pts0, You can install Grub to all disks. Presumably, your BIOS allows you to sort boot order, so it should just boot from the second listed disk
    13:03 <twb> One of my hats is documentation manager / tech writer, and I can definite vouch for reST/sphinx/readthedocs stack over markdown.  The expanding/collapsing block thing would either be part of the CSS/js theme, or an extension.
    13:04 <pts0> ok, updates are ALWAYS changing grub crap
    13:04 <pts0> is an update going to break it
    13:04 <twb> (sphinx is what linux kernel and python communities use these days)
    13:04 <rlaager> pts0: As a general tip... I suggest following the instructions once, exactly as written. Once you see how things work and have a working system to poke at, it will make a lot more sense.
    13:04 <pts0> ****do you want to overwrite custom grub with package maintaners...that one
    13:04 <rlaager> It's fine. Obviously say "no" to that prompt.
    13:05 <pts0> then i miss something the package maintainer wanted
    13:05 <rlaager> But to be fair, you do need to have some idea what you're doing, and be willing to take some risk. This is not the common-case scenario, as it's not a distro default. It's not distro supported. etc. etc.
    13:05 <pts0> i always hate that one
    13:05 <pts0> sgdisk     -n2:1M:+512M   -t2:EF00 /dev/disk/by-id/scsi-SATA_disk1
    13:05 <pts0> so what are all the switches
    13:06 <twb> pts0: per the manpage, it creates a new partition, number 2, from 1M to 1+512MB, on the disk with serial number "SATA_disk1"
    13:06 <twb> pts0: oh and sets its GPT UUID (i.e. "partition type") to "EF00", which is presumably the magic number for the ESP
    13:06 <pts0> ok so thats not the uuids i see when i run blkid
    13:07 *** ChanServ MODE +v mahrens
    13:07 <twb> pts0: again per the gdisk manpage, the 4-digit UUID is a shorthand for the "real" UUID
    13:07 <pts0> yeah see i have no idea what the hell that means
    13:08 <jasonwc> pts0, I think you're overthinking this.  Try it in a VM to get a feel for how things work.  If you have a decent understanding of ZFS, you should be fine.
    13:08 <twb> pts0: that's OK.  The HOWTOs on the ZOL wiki assume you already have some experience with these tools.
    13:08 <pts0> I'll go throught it and make a bunch of notes when I have time and try to install it so you guys know what an idiot thinks about so if you want to idiot proof it you can
    13:09 <twb> pts0: you just need some practice, I think.  As jasonwc says, you can do this with a toy VM to avoid risking your "real" setups
    13:10 <pts0> Honestly the complexity just makes me want to use the hardware raid...then you have people saying it's perfectly fine
    13:11 <pts0> anyway, i appreciate all the help
    13:11 <PMT> I mean, if you dislike things you don't understand, you're free to do whatever you want.
    13:12 <pts0> why do you say that
    13:12 <pts0> i know that
    13:12 <pts0> i'm not trying to be a jerk or anything
    13:13 <pts0> I'm tempted to learn this but I haven't been feeling well lately
    13:13 <twb> hardware raid is a huge pain, because you have to buy a spare rescue card in case the main card dies (because different models aren't compatible); you can't SMART to the drives to find out when they're dying; you have to run proprietary software in a RHEL chroot to manage the array, &c &c
    13:13 <jasonwc> pts0, I found it overwhelming at first too, but I wouldn't go back to using anything but ZFS for my root fs.  It's saved me so many times when something broke and I was able to fix it with a quick rollback, live, without restoring from backups
    13:14 <pts0> twb but you'd have to buy a spare hba too right
    13:14 <pts0> the thing that scares me is zfs just dying due to hardware raid
    13:14 <pts0> some people really stress that
    13:14 <PMT> that...doesn't make sense.
    13:15 <twb> pts0: if you're doing RAIDZ, then you're using passthrough HBAs
    13:15 <twb> pts0: which are 1. cheap; and 2. fungible.
    13:15 <twb> pts0: I know that if I put the disks into *any* computer with enough SAS slots, I can recover the data.
    13:16 <PMT> If you hide the multiple disks from ZFS, then yes, it can't recover from corruption, because it doesn't have any sort of redundancy even if there's a HW raid under it, but that's not really a ZFS-ism, it's just a fact.
    13:16 <twb> (When I was talking about buying a "rescue card" for hardware raid, "card" = "HBA")
    13:16 <pts0> check out my posts on reddit to see what I was looking at:https://www.reddit.com/user/5tzr/posts/
    13:16 <zfs-bot> [REDDITOR] 5tzr | Link: 6 | Comment: 35
    13:16 <zfs-bot> [ 5tzr (u/5tzr) - Reddit ] - www.reddit.com
    13:16 <PMT> no
    13:17 <pts0> PMT you're free to do whatever you want.
    13:18 <jasonwc> pts0, I think the first post from fengshui is on point
    13:18 <jasonwc> https://www.reddit.com/r/zfs/comments/bg4t3w/any_thoughtsopinionscorrections_on_this_article/
    13:18 <zfs-bot> [REDDIT] Any thoughts/opinions/corrections on this article? (https://mangolassi.it/topic/12047/zfs-is-perfectly-safe-on-hardware-raid/9) to r/zfs | 0 points (33.0%) | 4 comments | Posted by 5tzr | Created at 2019-04-22 - 17:27:15UTC
    13:18 <zfs-bot> [ Any thoughts/opinions/corrections on this article? : zfs ] - www.reddit.com
    13:20 <pts0> I'm not sure if I should trust that Perc card even in passthrough mode
    13:21 <pts0> maybe I should get another card?
    13:21 <pts0> One guy said you could flash the full perc but not the mini
    13:21 <pts0> I have the mini
    13:21 <jasonwc> Ah
    13:21 <jasonwc> I mean, you can get a 9211-8i pre-flashed with IT mode for $50 on Ebay
    13:21 <jasonwc> I've got 6 of them and they work great
    13:22 <pts0> That's what you would recommend for a Dell R720?
    13:22 <pts0> Do you have a link?
    13:22 <twb> perc 5 HBA can't do passthrough IIRC
    13:22 <twb> It can only do single-disk RAID0s, and you lost all your SMART
    13:23 <pts0> perc h310 mini
    13:23 <pts0> that's the model I have
    13:23 <twb> Last time I was on a server that came with a RAID HBA, I just paid $20 to replace it with a passthrough HBA
    13:24 <pts0> What HBA would you recommend for a R720?
    13:24 <twb> pts0: whatever the first-party one is
    13:24 <pts0> I don't know what that is
    13:24 <jasonwc> pts0, I mean, there are a ton of them on Ebay.  Here's one from a seller I've actually purchased from in the past that I trust
    13:24 <jasonwc> pts0, https://www.ebay.com/itm/LSI-SAS-9211-8i-8-port-6Gb-s-Internal-IT-MODE-ZFS-JBOD-HBA-IR-MODE-RAID/352612690487?hash=item52195aaa37:m:mbEDowx1AV5jIlfu6fiaMHg
    13:24 <zfs-bot> [ LSI SAS 9211-8i 8-port 6Gb/s Internal (IT-MODE) ZFS JBOD HBA / (IR-MODE) RAID | eBay ] - www.ebay.com
    13:24 <twb> pts0: so call your dell salesdroid and ask
    13:25 <pts0> I don't know what you mean by first party
    13:25 <twb> jasonwc: what's "IT mode" mean, there?
    13:25 <twb> pts0: "first-party" means Dell sells it to you and guarantees it'll work in their server
    13:25 <jasonwc> pts0, Each SAS port will give you 4x 6 Gbit bandwidth, so it'll give you full bandwidth for 8 drives, but if you're using HDDs and have a backplane with a SAS expander, you can use a single HBA for 24 or more drives
    13:25 <jasonwc> twb, IT mode means it's just a pure HBA, no RAID features, just pass-through
    13:25 <jasonwc> twb, full SMART access etc.
    13:25 <twb> jasonwc: ah cool
    13:26 <twb> jasonwc: that's not a shibboleth I knew, but I'll remember it for next time
    13:26 <jasonwc> twb, These LSI cards are sold under a bunch of names, and you can flash different firmware.  IR is RAID IIRC, IT is what you want for ZFS
    13:26 <pts0> and this will work with the Dell disk case?
    13:26 <jasonwc> twb, The 9211-8i are pervasive but there are newer and faster variants with SAS 12 Gb support
    13:27 <jasonwc> pts0, Dell disk case?
    13:27 <jasonwc> You mean Dell Chasis?
    13:27 <jasonwc> I think a lot of people use 9211-8i's from various manufacturers with the IT firmware.  Unless Dell is doing something to block non-Dell hardware, I don't see why it wouldn't work.
    13:28 <pts0> Hell I don't know
    13:28 <jasonwc> You can ask on Reddit.  Lots of people use these cards.
    13:28 <pts0> I thought you had to cable something to it
    13:28 <jasonwc> They're just dumb HBAs
    13:28 <jasonwc> yeah, you cable it to the backplane
    13:28 <pts0> the backplane where all the disks plug into?
    13:28 <jasonwc> with a direct attach backplane, you'll have one SAS port for every 4 drive bays
    13:28 <jasonwc> With a SAS expander, you may have one or two ports for all drives
    13:29 <jasonwc> pts0, Yes, drives attach to the backplane, and then on the back of the backplane there will be SAS ports that go to the HBA
    13:29 <jasonwc> pts0, How many drives are you going to be using?
    13:30 <pts0> well i was thinking 8, 2 mirrored for the boot/root stuff
    13:30 <pts0> but is that not what the guide says to do?
    13:31 <jasonwc> pts0, This what you have? https://www.youtube.com/watch?v=a5toVeaLqRA
    13:31 <zfs-bot> [ PowerEdge R720: Hard Drive Backplane - YouTube ] - www.youtube.com
    13:31 <pts0> i think that's the xd with a bunch of drives
    13:31 <pts0> mine just has 8
    13:31 <pts0> R720xd vs R720
    13:32 <pts0> I just have the R720
    13:32 <pts0> So you think that card will work?
    13:33 <pts0> I can buy a more expensive one if it's better
    13:33 <jasonwc> you're right about the mini card
    13:33 <jasonwc> "If you're using a mini card don't attempt flash it as you'll brick the card instead configure the disks as non raid to pass them through. The mini card looks like this."
    13:33 <jasonwc> https://www.serverhome.nl/media/catalog/product/cache/1/image/9df78eab33525d08d6e5fb8d27136e95/p/e/perc-h310-mini-mono.jpg
    13:33 <pts0> this midget porn collection is very important to me
    13:33 <pts0> bad joke
    13:34 <jasonwc> " If you're using a non mini card, I'd suggest you flash it to a pure HBA using 9211-8i firmware."
    13:34 <jasonwc> Apparently the non-mini card is just a 9211-8i
    13:34 <pts0> yes i have the mini
    13:34 <jasonwc> So I don't see why buying one online wouldn't work
    13:34 <pts0> someone told me exactly what you just posted
    13:34 <jasonwc> if it makes you feel better, you can get the dell one
    13:35 <jasonwc> https://www.ebay.com/itm/LSI-9211-8i-P20-IT-Mode-for-ZFS-FreeNAS-unRAID-Dell-H310-6Gbps-SAS-HBA/253955813684?epid=19006955695&hash=item3b20f23134:g:kKkAAOSwMjpb11RL
    13:35 <zfs-bot> [ LSI 9211-8i P20 IT Mode for ZFS FreeNAS unRAID Dell H310 6Gbps SAS HBA | eBay ] - www.ebay.com
    13:35 <jasonwc> So this is just a Dell H310 flashed with the IT firmware
    13:35 <jasonwc> given that the firmware is the same, it's silly to pay more for it
    13:35 <jasonwc> You're better off to asking on Reddit if anyone had problems using a 9211-8i that was not sourced from Dell.  I suspect it'll work just fine.
    13:36 <pts0> I wonder if I do Dell firmware updates if it will brick it
    13:36 <jasonwc> What firmware updates would you need to do?
    13:36 <pts0> Dell has that automated firmware updates through the lifecycle controller, etc
    13:36 <jasonwc> Other than update the BIOS on my motherboard
    13:36 <pts0> and with an iso
    13:37 <pts0> oh it updates disk firmware too
    13:37 <jasonwc> Wouldn't that assume the disks came from Dell?
    13:37 <pts0> yeah good point
    13:37 <pts0> hell im going to try it
    13:37 <pts0> screw it
    13:37 <jasonwc> I've had good experience with UnixSurplus
    13:38 <jasonwc> They are based in CA and I've bought a few servers from them
    13:40 <pts0> Does that guide create a separate pool/mirror for the boot/root stuff or is it all one big raid6 or something
    13:41 <jasonwc> pts0, The guide can be modified to do whatever you want.  I personally use a mirror of SSDs for my root pool.  Mirrors provide better IOPS and are more efficient than raidz2 for storing small blocks (8 and 16K).  With raidz/2/3, you get the random IOPS of a single disk, but sequential reads and writes scale with the # of disks - 1/2/3 disks based on your parity selection.  Mirrors will give you the write IOPS of a single
    13:41 <jasonwc> disk and the read IOPS of the # of disks in the mirror.
    13:42 <jasonwc> pts0, Are you using just SSDs or will this system also have HDDs?
    13:44 <pts0> It will just have HDDs
    13:44 <pts0> Man I just can't understand what that guide is doing
    13:44 <pts0> I want my boot/root disks to be mirrored
    13:44 <jasonwc> I don't think you want a single raidz2 in that case.  You'll get the IOPS of a single disk and that's not great.
    13:45 <pts0> It seems like that woudl be the default
    13:45 <jasonwc> pts0, The guide can be used to create a mirror, raidz/2/3, or a singleton
    13:45 <pts0> but how
    13:45 <pts0> you have to boot off one disk
    13:45 <pts0> and if that one dies, you have to boot of the other
    13:45 <pts0> unless you just don't want to boot
    13:46 <jasonwc> pts0, so, the guide shows how to create a pool using a single disk
    13:47 <pts0> ok so then what
    13:47 <jasonwc> pts0, To simplify, zpool create -o options rpool mirror Disk1 Disk2
    13:47 <pts0> that disk dies and I'm screwed
    13:48 <jasonwc> pts0, for raidz, zpool create -o options rpool raidz Disk1 Disk2 Disk3
    13:48 <jasonwc> pts0, The guide actually covers this: "If you are creating a mirror or raidz topology, create the pool using zpool create ... rpool mirror /dev/disk/by-id/scsi-SATA_disk1-part4 /dev/disk/by-id/scsi-SATA_disk2-part4 (or replace mirror with raidz, raidz2, or raidz3 and list the partitions from additional disks). "
    13:48 <pts0> I would bet good money it wouldn't jsut boot off that second drive then
    13:48 <jasonwc> pts0, The guide assumes some knowledge of ZFS
    13:49 <pts0> if i pulled the first out
    13:49 <jasonwc> pts0, It would if you install Grub to both drives, which the guide instructs you to do
    13:49 <jasonwc> pts0, I've pulled a drive and it boots
    13:50 <pts0> So it starts the partitions off at an offset or something so you can put grub on it
    13:56 !!! irc.freenode.net: connection broken by remote peer (closed)
    13:56 !!! irc.freenode.net: connection broken by remote peer (closed)
    > The other advantage of all first-party hardware is that 14:01 /join
      #zfsonlinux
    14:01 *** services. 328 #zfsonlinux http://zfsonlinux.org
    14:01 <pts0> sgdisk     -n2:1M:+512M    ok from 1M to 512m but then this just overwrites it sgdisk     -n3:0:+512M
    14:02 <pts0> the third one overwrites the second one
    14:02 <pts0> and it overwrites the first one because it starts at 0
    14:03 <pts0> there's just nothing in the gude about it...ah well
    14:04 <pts0> maybe i'll be smarter in the future...anyhow thanks everyone
    14:04 <pts0> take care
    14:04 <jasonwc> no, the third is +512M...
    14:04 <jasonwc> so it doesn't overwrite it
    14:04 <jasonwc> first is 24K-1M, second 1M-512M, third starts at 512M
    14:04 <jasonwc> pts0, I would recommend using ZFS for your data pool to get familiar with it, and you can revisit using ZFS for a boot pool when you feel more comfortable
    14:07 <jtara> i'd agree
    14:07 <pts0> unfortunately i don't think i can
    14:07 <pts0> i can either enable hardware raid or not
    14:07 <jtara> it's best used on a small scale first if you're still learning it
    14:07 <pts0> why are there three ts
    14:07 <pts0> t1, t2, t3
    14:08 <jasonwc> pts0, You can use two disks for a mdadm mirror for your boot, and give the remaining disks to ZFS for a raidz/2 pool
    14:08 <pts0> n1, n2 and n3 are partitions but the ts are types right
    14:08 <jasonwc> pts0, mdadm should be very simple
    14:08 <pts0> lol
    14:08 <pts0> yeah i looked at that guide too
    14:08 <pts0> there is a script that supposedly makes it simple
    14:08 <jasonwc> pts0, And if you're not booting off of ZFS, you don't have to worry about kernel compatibility.
    14:09 <pts0> i think, truth is, you just have to understand all this to do a non-standard install
    14:09 <jasonwc> You can setup a RAID1 mirror using mdadm with either the Debian or Ubuntu installer
    14:09 <pts0> ok, yeah, i'll try that
    14:09 <pts0> last question though why are there three ts
    14:09 <pts0> t1, t2, t3
    14:09 <jasonwc> so, use 2 disks for your / with mdadm and LVM
    14:09 <pts0> if it's just a partition type
    14:09 <jasonwc> it'll do that for you
    14:10 <jasonwc> then I would probably do a raidz2 with the 6 disks unless you need IOPS in which case you can do 3 mirrors striped
    14:10 <pts0> truth is im' just learning
    14:10 <pts0> jsut a cheap box to play with
    14:10 <pts0> seriously though what are the three ts
    14:11 <pts0> in the guide in that partitioning section
    14:11 <pts0> t1, t2 and t3...n1, n2 and n3 are the partition numbers
    14:11 <jtara> if you're just learning, leave hw raid off, maybe even plan on two separate pools
    14:11 <jtara> so you can back up everything to one if you need to blow away the other, or something
    14:11 <jtara> plus some things are best appreciated with a couple different pools
    14:12 <jtara> personally i would only do sw raid for a boot drive, but that's probably more of a solaris perspective than linux
    14:12 <jasonwc> jtara, Why? mdadm should be well tested.  Raid5/6 has the write hole but RAID10 should be fine.
    14:14 <jtara> i'd rather have a sw raid setup because i don't trust boot disks sitting behind proprietary controllers, for the most part
    14:14 <jtara> and i feel that sw raid gives better indications of predictive failure for root mirroring
    14:14 <jasonwc> Yeah, I meant why would you trust a hardware RAID controller over a hardware RAID solution for RADI10?
    14:14 <jtara> plus zfs makes it really easy on solaris to do
    14:14 <jasonwc> "personally I would only do sw raid for a boot drive"
    14:14 <jtara> i didn't say i would, i don't think
    14:15 <jasonwc> Or do you just mean you would use ZFS for anything else?
    14:15 <jtara> i would use sw raid either with a mirror on a zpool or with something non-zfs i guess
    14:15 <jtara> back in the bad old days of sunos we used something very mdadm-ish that worked reasonably
    14:15 <pts0> -n3:0:+512M
    14:16 <pts0> does that mean to start where the last partition ended
    14:16 <jtara> oh i think i understand the misunderstanding
    14:16 <jtara> i mean, "for a boot drive, i would only use sw raid, not hw raid"
    14:16 <jasonwc> ah, got it
    14:16 <jtara> english is such a stupid language lol
    14:16 <jasonwc> pts0, Yeah, it starts at +512M
    14:16 <jtara> doesn't even have proper lambdas
    14:17 <jasonwc> Yeah, I interpreted it as I would only use SW raid for this one situation, and hardware RAID in all others.
    14:17 <jtara> ahh
    14:17 <pts0> i thought it meant to start where the last one ended and go 512M more
    14:18 <pts0> i'll play and see what it does
    14:18 <pts0> thanks guys for putting up with me
    14:18 <jasonwc> pts0, Since you seem uncomfortable with sgdisk, you can also achieve the same results with fdisk, which is definitely easier to use
    14:18 <jasonwc> pts0, But for a howto, it's better to use instructions that minimize the opportunity for error
    14:19 <jasonwc> You don't need to become an expert on sgdisk to use ZFS.
    14:19 <jtara> just don't be in a hurry to put mission critical stuff that requires all kind of stuff on it
    14:19 <jtara> play with it, destroy some pools, create some more, get a feel for it
    14:19 <jasonwc> yeah, I created several VM root pools before I did one on my real system
    14:19 <pts0> i see what the ns and ts match up now
    14:20 <jasonwc> And I had used ZFS for a non-root pool for a few years before using ZFS on root
    14:20 <pts0> it's specifying the partition number in both
    14:20 <pts0> so t4 to change the type of partition 4
    14:20 <jtara> we still don't use zfs for root here, but i'm trying to get it widely adopted for containers
    14:20 <pts0> i have nothing important to screw up so
    14:20 <pts0> meh
    14:20 <jtara> considering how many pools i'm jamming into a single physical it works pretty well
    14:20 <twb> Strictly, sgdisk is GPT sfdisk.  gdisk is the GPT fdisk.  The "s" is the non-interactive version, IIRC.
    14:21 <twb> I personally prefer libparted over g/fdisk, but the feature sets aren't identical.
    14:21 <jtara> pts0: that's an awesome place to be in!
    14:21 <pts0> well i'm also sickly, ugly and old so
    14:22 <jasonwc> jtara, Why have so many pools on a single disk?
    14:22 <jtara> our original use case had one pool per local container, as well as one "backup" pool for each container on remote block storage
    14:23 <jtara> going forward we may have multiple containers per pool but this has been the cleanest way to give them isolation so far
    14:23 <pts0> twb so sgdisk is fdisk
    14:24 <jtara> it lets each container either manage its own pool or delegates that to a backup pod
    14:24 <twb> fdisk and sfdisk are MBR; gdisk and sgdisk are GPT.  GPT is required for UEFI.  GPT is required for ≥4TiB disks.
    14:25 <jtara> plus it's mostly ssd so we're not nearly so iop constrained as long as each pool io plays nice
    14:26 <pts0> sfdisk is not fdisk?
    14:26 <jasonwc> On an unrelated topic, what's the best way for a consumer to obtain enterprise SSDs?
    14:27 <fling> Does not zfs support badblocks?
    14:27 <jasonwc> I only see them offered by 3rd party sellers that are surely not authorized retailers.  Do you have to buy in bulk?
    14:28 <pts0> the ones with s in front of them are for non-interactive?
    14:30 /join #7z
    14:30 /join #7zip
    14:48 <PMT> fling: I don't believe there's any mechanism, no
    14:50 <twb> jasonwc: what do you mean by "enterprise" SSDs?
    14:50 <twb> Different FTL?
    14:50 <fling> enterprise bytes!
    14:51 <jtara> different duty cycles, more overprovisioning, often pre burned in
    14:51 <twb> Fair enough
    14:51 <fling> jtara: what is pre burned?
    14:52 <twb> I didn't know they were A Thing, presumably because I'm not enterprisey enough to even get them offered to me :-)
    14:52 <jtara> normally ssd performance degrades over time as blocks fill
    14:53 <jtara> burning them in basically gets you to steady state performance instead of facing a steeply falling curve
    14:53 <jtara> in a lot of places consistent performance is more important than brief maximum performance basically
    14:53 <jtara> and big overprovisioning really diminishes erase cost
    14:55 <jtara> the other thing is...if you can run with big blocks on zfs and not raidz them, you make really sequential writes
    14:55 <jtara> so you can greatly minimize ssd erase cost
    14:56 <PMT> I mean, I imagine you could probably implement a badblocks equivalent assuming the region isn't in use, by permanently leaking it. :V
    14:57 <twb> Is badblocks even A Thing still?  Doesn't the HDD firmware have the equivalent internally, and when that runs out, you should throw the disk away
    14:57 <PMT> twb: in theory.
    15:32 <twb> http://ix.io/1GWp/rst  are my notes about my plan for booting w/ ZFS
    15:34 <rlaager> twb: #1 is not applicable. The initramfs will set noop on the disks in the root pool.
    15:34 <rlaager> I think "crap feature set" is a bit harsh. That was _the_ feature set not that long ago.
    15:34 <twb> OK :-)
    15:35 <twb> I haven't looked at exactly what the featureset is
    15:35 <rlaager> If you're going to use a bpool, what's the point in using refind instead of GRUB? Because you can't mirror the ESP? What type of motherboards are you using?
    15:36 <rlaager> Oh, nevermind, I misread.
    15:41 <rlaager> twb: You might be able to save some steps with this approach: 1) Always keep your FAT32-formatted zvol mounted at /boot. Mount it with "sync" for extra safety, especially in light of... 2) rsync the zvol to /dev/disk/by-id/TOSHIBA-xxxx "Whenever flashrom would run"
    15:42 <twb> ah so copy it in the other direction, basically
    15:42 <rlaager> Exactly. Then it should be "safe" to keep it mounted all the time, which is what I assume you were trying to avoid with the flash drive, which causes the dual re-mount steps.
    15:43 <twb> That would do (slightly) more writes to the USB key, but probably fine.  It would also hose any writes by the bootloader or mainboard (or other OSs) to the USB key...
    15:43 <rlaager> Would it write more?
    15:44 <rlaager> But yes, it would overwrite any changes from elsewhere. That is a downside.
    15:44 <twb> depends if the rsync blocks line up with the FAT and/or erase blocks, I guess
    15:46 <rlaager> I'm still not sure why you can't have multiple ESPs.
    15:46 <twb> You can have multiple disconnected ESPs, but you can't have them all be mirrored together without fighting the UEFI standard (AFAICT)
    15:47 <twb> If you have multiple ESPs, then things that aren't Linux that write to them will write to *one* of them, maybe the wrong one
    15:47 <rlaager> Correct. Do your systems actually write to them?
    15:47 <twb> I don't know :-)
    15:47 <twb> UEFI conformant devices are allowed to
    15:48 <twb> and IIUC refind supports it as an alternative to writing to the efi boot vars, because they're on a PROM with a very limited number of writes, and it's soldered onto the mainboard
    15:48 <rlaager> Also, to answer your question about NOP writes... if you set the checksum to one of the cryptographically secure choices, then ZFS will avoid writing blocks to disk that are the same as the existing blocks. That's the nop-write thing. That's pretty small potatoes here, though.
    15:49 <twb> Looking at your HOWTO, I must be reading it wrong, because it looks like you're turning on most features on the bpool, not the rpool?  e.g. feature@async_destroy=enabled
    15:49 <rlaager> I'm reasonably confident that my Supermicro boards do not have any features that write to the EFI partition and do not care about EFI variables. They're basically doing the "scan the disks" approach from BIOS booting, but with EFI. So for a setup like that, dd'ing them is pretty safe. Your systems may vary.
    15:50 <rlaager> You have different goals than me, so you have reached a different result. I think your approach is sound and well-thought-out.
    15:50 <rlaager> twb: The bpool has features disabled with -d, then individual features turned back on. The rpool takes the default, which is all features enabled.
    15:50 <twb> Ah, so *that's* what the -d does
    15:51 <twb> So let's see what features are actually different between bpool and rpool on a buster system...
    15:51 <rlaager> The bpool enables those features which GRUB supports, _plus_ all "read-only compatible" features, because GRUB is only reading the pool. That may change if we can get GRUB to write to the pool, even in a limited way, for the grubenv file.
    15:51 <lundman> Didn't I see a commit recently that finally lets us feature=disabled too
    15:52 <rlaager> There's also a reasonable debate to be had here about whether we should enable features "just because we can" or whether we should only able features that "matter" on the bpool.
    15:52 <rlaager> lundman: I think that exists, but I'm not 100% sure. I could use that, but this approach is safer if someone backports a newer ZFS or something.
    15:53 <rlaager> twb: multi_vdev_crash_dump (which is never used anyway), large_dnode, sha512, skein, and edonr.
    15:53 <lundman> yeps - just fresh in my mind as I cherry-picked it over last week
    15:53 <twb> I actually use ext2 for /boot quite often because YAGNI even for the journal and extents :-)
    15:53 <rlaager> twb: Oops, sorry, that was bionic, not buster.
    15:56 <twb> SHA-2 sounds important given that SHA-1 is orange since 2004 http://valaurora.org/hash.html  (damn, that link is dead today...)
    15:56 <rlaager> twb: Buster is the same as Bionic. 0.8.0 adds encryption, device_removal, obsolete_counts (part of device_removal, essentially), and bookmark_v2 (part of encryption, essentially)
    15:57 <rlaager> Neither is sha1. The sha512 feature is to allow you to use checksum=sha512 (which is SHA512/256) rather than checksum=sha256. But the default is checksum=fletcher4 anyway.
    15:57 <twb> Ah cool
    15:58 <twb> Is 0.8 likely to be "ready" in the next six months?
    15:58 <twb> at-rest FDE (without the LUKS hassle) is a "nice to have" for me
    15:58 <rlaager> From a practical standpoint, assuming you don't need device_removal, the main feature difference is large_dnode and/or encryption, with the latter being the killer difference.
    15:59 <rlaager> Yes, 0.8.0 will probably be released soon.
    15:59 <twb> Will it be possible to in-place upgrade from no encryption to encryption?
    15:59 <twb> Like turn it on and then resilver the pool?
    15:59 <FireSnake> turn it on for new data
    16:00 <FireSnake> for new datasets
    16:00 <FireSnake> not for existing
    16:00 <rlaager> Yes, in the sense that you can enable it on your pool on new _datasets_. But if you have sensitive data in the pool already, you may not want to do that, as that would leave sensitive data on the disk. You might want to wipe the disks.
    16:00 <twb> That makes sense
    16:01 <rlaager> Were you looking at Debian then?
    16:01 <twb> So even if I had e.g. /var/mail and made a new encrypted zfs and copied all the files from old-var-mail zfs to new-var-mail zfs, it wouldn't do any kinda secure erase of the old-var-mail disk blocks
    16:02 <twb> So remanence would still screw me unless I manually nwipe'd the drives or similar
    16:02 <rlaager> twb: You might want to compile the 0.8.0rc4 package from git. Here's the not-secret-but-not-linked-from-anywhere experimental HOWTO for that: https://github.com/zfsonlinux/zfs/wiki/Debian-Buster-Encrypted-Root-on-ZFS
    16:02 <twb> rlaager: oh!  I saw that page but my brain assumed it was the LUKS method still.
    16:04 <twb> Another dumb question: if I zfs send from a system without ZFS encryption, to a system *with* ZFS encryption, will the receiving host have FDE on the received snapshots?
    16:05 <rlaager> That depends on the properties inherited and such. It should be straightforward to achieve that result. Unless you need to receive the root dataset; that might be an issue. I haven't tested this, though.
    16:05 <twb> Right now my historical infrastructure looks like:   prod server → rsync → ZFS archive server → rsync → offsite DR server with FDE
    16:06 <twb> And my plan is to have end up with prod server w/ ZFS snaps → zfs send → offsite DR server with FDE
    16:06 <jtara> that can work well, zfs send/receive can work much better for some workloads
    16:06 <jtara> but for some its not that important
    16:07 <rlaager> twb: Right, so if I were you, I'd probably start by replacing the offsite DR server with ZFS encryption. That would result in: prod server → rsync → ZFS archive server → zfs send → offsite DR server with FDE
    16:07 <rlaager> That's a minimal change. Assuming that works, then convert to the end game.
    16:08 <twb> that's fair, although right now the budget prefers me to upgrade the prod server first
    16:09 <rlaager> I proposed a software change. Unless you're talking about labor budgets, there's no spending involved. ;)
    16:09 <twb> as long as the offsite host will have FDE, I don't *really* care
    16:09 <twb> rlaager: well, I gotta have enough capacity to have the old and new setups running side-by-side briefly, or be confident enough to do it in-place
    16:10 <rlaager> Sure, there's nothing wrong with doing it the other direction.
    16:10 <jtara> do you want to apply the incrementals on the remote end
    16:10 <jtara> or just store periodic full and incremental sends
    16:10 <jtara> both work, just different approaches
    16:11 <twb> The end goal is to have all the archive snapshots on the offsite host.  Right now, the offsite host only has the latest state
    16:11 <rlaager> If bandwidth is limited, it may actually be better to start with the prod server. This could avoid the need to send two full copies during the migration process.
    16:11 <twb> Right now the bandwidth is about 1Mbit/s, but hopefully that'll go to about 10Mbit/s in a couple of months
    16:12 <twb> But for initial syncs I can just give someone a USB HDD
    16:12 <rlaager> Yeah, do the prod server first. Get your zfs send | recv worked out on site, between prod and ZFS archive server. Once that's golden, then it's just a matter of converting the DR server to ZFS and doing one big send | recv.
    16:12 <twb> The offsite host is only DR ("the building burns down") anyway, so it's not a major concern
    16:13 <rlaager> For your initial sync, you can do send | recv to a pool on a USB drive, then have someone drive it to the DR server, then send | recv from USB to the DR pool. Thereafter, just do incrementals.
    16:14 <twb> yeah or even just zfs send >/mnt/usb-hdd/zfs-send.img
    16:14 <rlaager> Sure, that works too.
    16:21 <twb> What does device_removal get me?  If a disk is dying, I want to replace it while the system is running.
    16:29 <twb> FTR, https://valerieaurora.org/hash.html  was the URL I was trying for earlier.
    16:29 <zfs-bot> [ Lifetimes of cryptographic hash functions ] - valerieaurora.org
    16:34 <rlaager> device_removal allows you to remove a top-level vdev, subject to some limitations. For example, if you accidentally `zpool add` instead of `zpool attach`. Or if you want to reduce something from N striped mirrors to N-1.
    16:36 <twb> But not relevant for "disk WDC-xxx is dead and I need to replace it with new blank disk WDX-yyy" ?
    16:37 <rlaager> Correct. That's just a regular replacement that's worked since "forever".
    16:37 <twb> Yay


    18:13 <twb> https://github.com/zfsonlinux/zfs/wiki/Debian-Buster-Encrypted-Root-on-ZFS says- oh never mind.  It's written for the future, after ftpmasters move 0.8~rc from NEW to experimental.
    18:15 <sphalerite> jtara: not so much a use case as "what happens if I accidentally move the disks around badly, and how afraid should I be of it?"
    18:15 [twb tries to remember why CCM or GCM is better than plain old CBC or CTR]
    18:16 <lundman> position based ivset generation?
    18:16 <twb> Ah, they appear to do the authentication AND encryption as mode, rather than chaining CBC/OTR and a separate MAC
    18:17 <twb> Originally they were separate, which meant that everyone screwed up combining them
    18:17 <twb> And most of my textbooks are from that era :-)
    18:19 <twb> GCM was first published in 2005; CCM in 2003.
    18:21 <twb> Ohhh does 0.8 add zfs-mount-generator for systemd?
    18:21 <twb> Because that's something I've been worrying about
    18:21 <twb> (the lack of it)
    18:23 <jtara> sphalerite: basically if you totally remove a disk and then import a pool it will come up as degraded
    18:23 <jtara> and you'll need to take steps to really break the mirror so to speak
    18:25 <jtara> my security guys are really happy about tightly coupled authenticated encryption, i think that's going to be a deciding factor
    18:26 <twb> Yeah it makes sense
    18:27 <twb> I'm just 15 years behind best *current* practice :-)
    18:27 <jtara> yes well :)
    18:28 <jtara> the thing a lot of places have issues with is that pool metadata itself is not encrypted
    18:28 <jtara> volume names, properties, on-disk zfs structure metadata
    18:28 <twb> Not at all, not ever?
    18:29 <jtara> the whole point is you want a pool to be able to manage data it doesn't have the key for
    18:29 <jtara> so all the payload data itself is secure
    18:29 <twb> IIRC ecryptfs had that problem
    18:29 <jtara> well none of the structural metadata contains anything revealing
    18:29 <jtara> it's just "this chunk is linked to this node"
    18:30 <twb> Oh hang on.  When you say pool metadata, you don't mean dirents
    18:30 <jtara> right
    18:30 <twb> OK you just mean like the equivalent of LVM and md metadata -- "I'm a RAIDn array with three disks named X, Y and Z" sort of metadata
    18:31 <jtara> well, kind of
    18:31 <jtara> basically the metadata block for each leaf data block has things like its checksum
    18:31 <jtara> and where the data block itself is stored
    18:31 <twb> Are the names and snapshot times of the zvols/zfses encrypted?
    18:31 <jtara> so you might know that there is a file or volume X, which has at position Y a chunk of data Z
    18:31 <jtara> but you don't know what X is or what the data inside Z says
    18:32 <twb> Or can the attacker see e.g. there is a snapshot called illegal-porn@2007-01-01
    18:32 <jtara> snapshot and volume names are in the clear
    18:32 <jtara> so that as you apply deltas you can manipulate them
    18:32 <twb> Oh, another dumb question then
    18:32 <jtara> like, you can apply an incremental zfs send of encrypted data, then delete an old snap to do an incremental roll
    18:33 <jtara> and you never need the encryption keys to do so
    18:33 <twb> If I zfs send encrypted data to an offsite backup, does and offsite host just holds it, never "uses" it, does the offsite host *ever* have to decrypt it?
    18:33 <jtara> honestly if you have people who work with you who manage to hide useful information in pool or dataset names, more power to you, i can't get mine to :p
    18:33 <jtara> it never has to decrypt it
    18:34 <twb> that's awesome
    18:34 <jtara> you can zfs receive into that pool without the key, and zfs send out of it without the key
    18:34 <jtara> yeah
    18:35 <jtara> and you can rekey things without reencryption
    18:35 <jtara> hopefully i can take advantage of that to give my backups a specific key per time period, while keeping all the source pools with totally random keys

    18:52 <twb> rlaager: BTW, you can do "apt-get build-dep" on a directory, rather than having to copy things from debian/control Build-Depends by hand
    18:52 <twb> e.g. git clone .../zfs.git && sudo apt-get build-dep ./zfs
    19:14 <twb> 18:21 <twb> Ohhh does 0.8 add zfs-mount-generator for systemd?
    19:14 <twb> ...I can see in git that it does.
    19:32 <twb> Boy the ZOL codebase does a lot of $(MAKE) -C
    19:33 [twb waves the RMCH paper around grumpily]

    19:41 <twb> X: zfsutils-linux: missing-systemd-timer-for-cron-script etc/cron.d/zfsutils-linux
    19:41 <twb> 24 0 8-14 * * root [ $(date +\%w) -eq 0 ] && [ -x /usr/lib/zfs-linux/scrub ] && /usr/lib/zfs-linux/scrub  # Scrub the second Sunday of every month.
    19:41 <twb> That looks pretty straightforward...
    19:42 <twb> zfs-scrub.timer: [Timer] OnCalendar=Sun/2 *-*-*
    19:43 <twb> zfs-scrub.service: [Service] Type=oneshot ExecStart=/usr/lib/zfs-linux/scrub
    19:43 <fling> Lalufu: not even in master yet?
    19:43 <Lalufu> I couldn't say
    19:43 <Lalufu> it might be
    19:43 [fling checking]
    19:44 <fling> Is not trim eating data anymore?
    19:44 <twb> Ah my math might be a little wrong, I think mine would run "once per fortnight, on sundays" rather than "once per month, on the second sunday of that month"
    19:45 <twb> OnCalendar=Sun *-*-8..14 00:24:00
    19:48 <twb> looks good to me
    19:48 <twb> "NEXT: Sun 2019-05-12 00:24:00 AEST  LEFT: 2 weeks 4 days left "

    19:54 <twb> Wow.  I discovered arc_summary, and looking at a prod ZFS pool, I can see L1ARC "Actual Hit Ratio" is 82%, where L2ARC "Hit Ratio" is only 5%
    19:54 <twb> Meaning (I think) that the L2ARC really isn't helping much

2019-04-23 ##eros-chat::

    18:24 <twb> MTecknology: so are you moving things from NEW queue still
    18:24 <twb> MTecknology: because as it happens zfs 0.8 is in there right now, wanting to move from NEW to experimental
    18:24 <twb> MTecknology: and that version fixes systemd integration AND adds full-disk encryption
    18:25 <MTecknology> that sounds like an unpleasant review
    18:25 <twb> unpleasant for you maybe but convenient for me when it's done ;P
    18:25 <twb> AIUI it's only going into experimental ANYWAY so it's not like it should be a big deal, except maybe licensing?
    18:26 <twb> I don't know where the ftpmasters keep their notes about NEW queue, though
    18:30 <MTecknology> It's in a postgresql db that's kind of unpleasant to attach to.
    18:30 <MTecknology> I don't see any notes on it
    18:47 <twb> righto
    18:48 <twb> Do you know the URL to dget stuff from NEW queue?  It's kinda hidden because they don't want people playing with it
    18:53 <MTecknology> I don't see anything available. That should probably be a new addition
    18:54 <twb> don't worry the upstream instructures are building it from a git on salsa anyway, which is near enough
    18:56 <MTecknology> Have you ever used pbuilder or sbuild?
    19:11 <twb> MTecknology: not for a decade

2019-04-24 #flashrom::

    18:43 <twb> I have a dumb question.
                I have a boring Debian x86_64 system, with a boring FAT32 ESP, and
                I want to copy /boot into the ESP anytime /boot changes.
                Somebody told me "flashrom does basically that, for more complicated hardware,
                  where the number of erase cycles is really small.
                  So it should already have OS integration to know WHEN to reflash the kernel and ramdisk."
    18:43 <twb> But I'm looking at the flashrom package and all I see is the binary itself,
                no hooks into apt or update-initramfs or anything.
    18:44 <twb> Am I looking wrong, or was my friend too optimistic?
    18:54 <twb> Ah sorry to bother you.  It seems I was confusing flashrom with flash-kernel
    19:14 <Hellsenberg> twb: I think you did :P

2019-04-24 #debian-arm::

    18:48 <twb> Hey, suppose I have a dumb system, where just dropping a new kernel and ramdisk into /boot isn't enough, but some extra flashing process has to happen afterward.  What part of Debian makes that actually happen?  I thought flashrom had a update-initramfs hook or dpkg trigger, but I don't see it.
    18:50 <hrw> twb: flash-kernel?
    18:51 [twb starts poking around the installation-guide-armhf and-]
    18:51 <twb> ah, thanks
    18:51 <hrw> flashrom is a tool to update bios chips and other flashable firmware chips
    18:54 <twb> My use case is pretty silly.  I'm actually on x86-64 UEFI, and for stupid reasons I want to just copy /boot into the ESP anytime /boot changes.  Rather than guessing /etc/kernel/postinst.d/ or something, I wanted to see see how other people were already solving this, and steal their answers
    18:55 <hrw> twb: you know that you do not need to have kernels in ESP?
    18:56 <twb> I know, but then I have to move /boot out of my main ZFS pool
    18:56 [pabs3 would just run rsync from an apt hook]
    18:59 <twb> pabs3: yeah that would be the 90% right solution.  That wouldn't cover e.g. tweaking /etc/initramfs-tools/conf.d/ and then doing an "update-initramfs -u"
    19:00 <pabs3> ack, I would only ever tweak that via apt
    19:00 <hrw> twb: maybe you need to tweak grub to read zfs?
    19:00 <twb> hrw: grub2.02/efifs1.3 can't read ZFS pools with some newer features enabled - notably ZFS-native encryption
    19:01 <twb> http://ix.io/1H3C if you want the gory details
    19:10 <hrw> twb: make non-encrypted pool for /boot maybe?
    19:10 <twb> hrw: at which point, it needs to live somewhere
    19:13 <hrw> true
    19:23 <twb> Looks like flash-kernel adds /etc/initramfs/post-update.d/ *and*
                                             /etc/kernel/{postinst,postrm}.d/, and also
          a dpkg trigger for ITSELF, and also
          an initramfs-tools hook to (I think) resolve root=UUID=X in advance,
          in case udev/libblkid isn't available at boot time.

2019-04-24 #zfsonlinux::

    19:50 <twb> OK help me out here.  I have a pool of disks and I want to take down the pool and wipe the metadata, prior to creating a new pool
    19:51 <twb> umount -a -t zfs and zpool export -a are refusing to get rid of some stuff, even though lsof says nothing is actually open
    19:51 <twb> so I went "sod it" and tried to just do "wipefs -a /dev/sda", and even THAT fails: wipefs: error: /dev/sda: probing initialization failed: Device or resource busy
    19:56 <twb> Unrelated: for a small SATA/AHCI server, is it better to use /dev/disk/by-id/wwn-XXX or /dev/disk/by-id/ata-XXX ?  The FAQ isn't very clear.

2019-04-24 #parted::

    21:32 <twb> gparted can resize the *filesystem* in a FAT32 ESP GPT partition.
    21:32 <twb> AFAICT, parted can't actually do that.
    21:33 <twb> But gparted appears to just be asking libparted to do it, so... WTF?  Is filesystem resizing limited to people without visual impairment?
    21:33 <twb> (parted used to have "resize", but current versions only seem to have "resizepart", which explicitly doesn't affect the filesystem, only the partition)
    21:35 <twb> libparted definitely has libparted/fs/r/fat/resize.c:fat_resize()
    21:35 <twb> So how do I access that functionality, without a GUI?
    21:49 <twb> A-ha!  This isn't supported by any of the normal fat packages (e.g. dosfstools), but there is a package that does *just* this, using libparted for the actual work.  http://sf.net/projects/fatresize
    21:49 <twb> Sorry I lost my temper

2019-04-24 #emacs::

    <RANTING ABOUT #PARTED>
    21:46 <npostavs> twb: https://manned.org/fatresize.1 maybe?
    21:47 <twb> npostavs: ah thanks
    21:47 <twb> That exists in Debian
    21:47 <twb> I was looking under dosfstools and similar
    21:50 <mkletzan> And it looks like fatresize is in portage as well.  Stupid me for trying to eat more healthy and get some exercise...
    21:51 <twb> It looks at a glance like fatresize was basically someone who was in the same position as me, but a couple of years ago
    21:51 <twb> And unlike me, they actually cared enough to solve it for other people
    21:52 <mkletzan> otoh the efi partition is usually small enough to copy the data elsewhere, reformat and copy back
    21:53 <twb> mkletzan: that requires me to get things like the fat size correct, and it's easy to fuck up such things
    21:53 <twb> e.g. mkfs.vfat on a 4MB disk will be FAT12 or 16 by default, and then SOME firmwares will ignore it
    21:53 <mkletzan> `-F 32` ?
    21:54 <twb> yeah, but you have to notice that and remember to use it
    21:54 <twb> "grow the thing that's already there and works" is (hopefully) harder to fuck up
    21:54 <mkletzan> agreed
    21:54 <twb> hahaha
    21:54 <twb> Note ==== You can't resize FAT32 partition lesser than 512Mb because Windows(R) doesn't work properly with small FAT32 file system. Use FAT16.
    21:55 <twb> I think they mean MiB not Mbit

2019-04-26 #systemd::

    15:18 <twb> Hey, you know how having systemd in the ramdisk is generally a Good Thing?
    15:19 <twb> And debian and arch don't do that by default, and for arch it's just a simple config option.
    15:19 <twb> On debian, is "just use dracut (not initramfs-tools)" the best way to get a systemdized ramdisk?
    15:21 <Xogium> twb: yeah that's what I do here. Except I generate the initramfs as a standard cpio archive that I embed into the kernel, via buildroot. My initramfs contains systemd as init, busybox for all the shells and utilities, and whatever deps from util-linux is required. 4 mb in size compressed in xz
    15:21 <Xogium> I'm just not sure why systemd doesn't seem to recognize that it is inside an initramfs
    15:22 <twb> Xogium: are you doing that via dracut, or are you doing some kind of hand-rolled business?
    15:22 <Xogium> I do have /etc/initrd-released inside, or forcing initrd.target on kernel cmdline would have failed spectacularly
    15:23 <Xogium> twb: no dracut, no mkinitcpio. But buildroot is able to create a cpio archive suitable for use as an initramfs with whatever packages you selected in your rootfs
    15:23 <twb> My immediate use case is I'm rolling a modern Debian Buster system with multi-disk ZFS for root (with a zfs-mount-generator), and I'm hoping dracut will be more future-proof-y.
    15:24 <twb> #systemd used to get people with confused systems because init was plain sh and / was multi-disk btrfs, which made me afraid
    15:24 <twb> I mean /bin/init in the initrd was a sh script
    15:24 <Xogium> heh I can understand
    15:25 <grawity> Arch's sh-based initramfs just runs `btrfs device scan`
    15:25 <grawity> not sure what archzfs stuff does, but it also *seems* to work fine with my pool, so I haven't really tried systemd initramfs on the fileserver yet
    15:26 <grawity> (it doesn't help that the initial 242 release completely broke systemd-initramfs for everyone)
    15:26 <Xogium> here /bin/init points to systemd, but it is very minimal, contains only one or two other things, not even networkd, since I don't actually need it, having a serial console whenever I want
    15:26 <Xogium> grawity: did it ?
    15:26 <grawity> so say the people affected by it
    15:26 <twb> Xogium: yeah I accept that it works for you, but you're being a bit more "unique snowflake" than I have in mind for my current system
    15:27 <grawity> Xogium: https://git.archlinux.org/svntogit/packages.git/commit/trunk?h=packages/systemd&id=aa6c53bf7c18ef733da2bcf3e33324dda151b39f
    15:27 <Xogium> I'm on systemd 242.0-1 and I had no problem, though I've seen people complain a LOT
    15:28 <grawity> my dev system went from 241-git to 242-git and I haven't actually booted it to Linux for two weeks anyway, so I haven't experienced this personally
    15:28 <Xogium> twb: hehe well I'm simply trying to use this new systemd.volatile=overlay for my tiny system here
    15:29 <twb> I'm not familiar with that
    15:29 <twb> I haven't really kept track since v219
    15:29 <Xogium> and I do need an initramfs for this, so I had to figure how to do it in buildroot
    15:30 <Xogium> new stuff in 242, mounts the root device read-only with a writable tmpfs on top using overlayfs, that way anything you modify there is lost after a reboot, perfet for what I plan here, making the system go back to sane default
    15:30 <Xogium> *perfect even
    15:31 <twb> Looks like I'll be on v241, at least initially.
    15:32 <Xogium> grawity: haha I made sure to reboot right after the update, just to see what it'd do, and I didn't have such problem lucky for me I imagine
    15:33 <Xogium> that being said I think someone had an even weirder issue, this is the second time I see a machine getting stuck on 'loading initial ramdisk...'
    15:37 <Xogium> anyway, for what I'm doing here, clearly using rd.systemd.unit= on cmdline is a workaround, but I'm still not understanding why it treats the initramfs as a normal rootfs without this, I also have /etc/initrd-release into it
    15:38 <Xogium> might be coming from the fact I have a fully fledged systemd in the filesystem, and not just a couple binaries and services with targets such as multi-user.target for example, I have no idea
    15:39 <Xogium> I've been trying to do the best of both worlds with buildroot in this case
    15:43 <twb> Xogium: without that, how does systemd (running in the initrd) know whether the initrd is the "final system", or if it's destined to mount /root and switch_root?
    15:44 <twb> Because you might just be packing the final rootfs into the initrd and then stopping, e.g. for a smartbulb firmware
    15:46 <Xogium> I'm not sure that's what I've been trying to find out, how does it determine to be in an initramfs at all… boucman_work found that it apparently just check for /etc/initrd-release, so I'm guessing that I have a bunch of stuff that I'd need to take out of my initramfs or something, because it doesn't react at all how I expect it to
    15:52 <Xogium> hmm
    15:53 <Xogium> okay so by examining the archlinux initramfs they just pack a lot less systemd stuff than me
    15:54 <Xogium> but they've made the default target be initrd.target in the first place, so I could also just do the same and it would work, my default target is multi-user.target because that's how buildroot makes it by default
    15:55 <twb> bootup(7) has some info about that
    15:58 <Xogium> yeah, the charts are a bit difficut to parse with my screen reader, but I think I got the basic idea of it
    15:58 <Xogium> *difficult
    15:59 <twb> Xogium: ah ouch.  I can manually convert it to graphviz-format if it helps
    15:59 [twb is sighted]
    16:00 <Xogium> hehe I've never used graph, so I'm not at all sure how helpful that would actually be
    16:00 <twb> Ruh Roh, in Debian, intel-microcode might not work with dracut
    16:01 <Xogium> well that's annoying
    16:01 <twb> amd-microcode, by comparison, explicitly supports either initramfs-tools or dracut or tiny-initramfs
    16:01 <twb> (tiny-initramfs is similar design to RH ash - bare minimum all in one C binary as /init)

2019-04-26 #debian-kernel::

    15:31 <twb> Hey, dumb question.  Does/will buster have something like ksplice, so I can get kernel bugfixes without a full reboot?  ISTR there was mainline replacement for ksplice in the news recently...
    15:34 <pabs> there isn't a Debian service producing patches yet, Ubuntu has one though
    15:34 <pabs> I haven't heard of anyone working on one either
    15:35 <twb> Oh.  I assumed that happened automagically by just having the finished kernels in /boot :P
    15:36 <twb> It looks like the low-level functionality is in mainline 4.0, shared between kpatch (RH) and kgraft (SUSE)
    15:37 <pabs> I got the impression it needs a service, not sure tho https://linux-audit.com/livepatch-linux-kernel-updates-without-rebooting/
    15:38 <twb> FTR the high-level goal is to bypass the "file a Change Request and wait 6 weeks for approval" step.  The actual reboot itself wouldn't be a big deal.
    15:38 <pabs> shame ksplice didn't get merged, this new feature looks pretty useful: https://blogs.oracle.com/linux/using-ksplice-to-detect-exploit-attempts
    15:39 <twb> pabs: AFAICT oracle bought ksplice and shut it down
    15:39 <twb> pabs: which was when RH et al spun up their own replacements
    15:40 <pabs> seems like it is still available, gratis for Ubuntu/Fedora https://ksplice.oracle.com/
    15:42 <twb> "Support for RHEL changed to a free 30-day trial for RHEL customers as an incentive to migrate to Oracle^[5]^[6] and use of the Oracle Unbreakable Enterprise Kernel (UEK) became mandatory for Ksplice on production systems.^[7]"
    15:42 <twb> So yeah, it's available on desktops but they clearly want to lock in all the serevrs
    15:42 <pabs> apparently one can build ones own patches, the tools aren't in Debian tho http://chrisarges.net/2015/09/21/livepatch-on-ubuntu.html
    15:43 <pabs> (linked from https://blog.dustinkirkland.com/2016/10/canonical-livepatch.html)
    15:44 <pabs> woops, kpatch is in Debian
    15:45 <twb> Can I do that from pre-built debian kernels, or would I have to roll my own kernels from the .DSCs?
    15:53 <pabs> kpatch-build apparently relies on building the source twice with extra options. kgraft seems to use debug info from the old/new kernels
    15:54 <twb> cool
    15:54 <twb> but kgraft isn't in debian yet
    15:55 <twb> So probably this won't be "Just Work" until debian 11
    15:55 <pabs> (also not sure if that is outdated info)
    15:55 <pabs> I think this is the first time I've seen anyone in/around Debian show interest in the livepatching stuff
    15:57 <twb> I'm just sick of $customer's admin people dicking me around
    15:57 <twb> I don't care enough to do a lot of work, but if you'd said "oh yeah just apt install XXX" then I'd be super happy :-)
    15:58 <twb> kexec used to be that simple before systemd
    15:58 <twb> so all my servers with 5 minute POSTs got nicer overnight
    16:05 <pabs> hmm, can't find the code to kgraft userspace
    16:12 <pabs> aha https://git.kernel.org/pub/scm/linux/kernel/git/jirislaby/kgraft.git/
    16:13 <pabs> oh that is the old kernel stuff
    16:17 <pabs> Gentoo's solution: https://wiki.gentoo.org/wiki/Elivepatch https://linuxplumbersconf.org/event/2/contributions/258/attachments/55/62/Elivepatch_Kernel_Summit_20182.pdf

2019-04-26 #zfsonlinux::

    12:40 <twb> Why does rlaager's Debian HOWTO use normalization=formD?  Didn't Apple used to force NFKD in HFS+, and realized it was a bad idea, and stopped doing it in APFS?
    12:41 <twb> (Specifically, the badness happens when you have paths inside a file, e.g. in a .diff or .tar, created on a system with a different normal form.)
    12:41 [twb checks the zfs manpages]
    12:49 <CompanionCube> iirc the way apple normalizes things does indeed suck
    12:49 <twb> CompanionCube: so basically the way ZFS does it, doesn't have the same problem?
    12:51 <WrongDevice> hello
    12:51 <WrongDevice> This pool uses the following feature(s) not supported by this system:   org.zfsonlinux:userobj_accounting       org.zfsonlinux:project_quota
    12:51 <WrongDevice> i can disable those ?
    12:51 <WrongDevice> without recreate the zpool ?
    12:52 <WrongDevice> im trying to mount my zpool created with ZOL 0.8.0-rc4
    12:52 <CompanionCube> on?
    12:52 <WrongDevice> i want to mount it on mojave
    12:53 <WrongDevice> using openzfs osx 1.9.0
    12:53 <WrongDevice> last version
    12:53 <WrongDevice> cannot be disabled those features ?
    12:53 <WrongDevice> removed
    12:53 <WrongDevice> i cannot write
    12:54 <WrongDevice> and i moved from btrfs just to write my files from mac to linux
    12:54 *** ChanServ MODE +v behlendorf
    12:54 <twb> WrongDevice: I think you mean "can I disable those features, so I can use the pool from MacOS?"
    12:54 <WrongDevice> thats why i moved to zfs inthe first place
    12:54 <WrongDevice> yes
    12:54 <WrongDevice> yes
    12:55 <WrongDevice> no
    12:55 <WrongDevice> ok yes
    12:55 <CompanionCube> you can disable those features...at pool creation time
    12:55 <WrongDevice> im confused
    12:55 <twb> I don't know if you can disable them from an existing pool.  You can definitely make a new pool with those features never turned on.
    12:55 <WrongDevice> omg
    12:55 <Selavi> I don't think so. enabling features changes the on-disk format. a ZFS without those features won't know what to do with the extra info, and you don't want to write to a pool with unknown features as it could corrupt things
    12:55 <WrongDevice> no
    12:56 <WrongDevice> ok
    12:57 <Selavi> those are both read-only compatible, so you can read from it at least
    12:57 <WrongDevice> and whats the way to create a pool
    12:57 <twb> Could WrongDevice do something like "zpool set -o featureX=disabled pool", then resilver?
    12:57 <WrongDevice> no
    12:57 <CompanionCube> twb: nope.
    12:57 <CompanionCube> WrongDevice: if you don't know that...how did you make the pool in the first place?
    12:57 <WrongDevice> zpool create -f zroot /dev/disk/by-id/id-to-partition-partx
    12:57 <WrongDevice> that
    12:57 <WrongDevice> ^
    12:58 <WrongDevice> command
    12:58 <CompanionCube> OK then
    12:58 <WrongDevice> what i need to add to that command disabling the two features i dont want
    12:59 <WrongDevice> ?
    12:59 <twb> WrongDevice: -o xxx=disabled, I think, for each of the "bad" features
    13:00 <WrongDevice> ok
    13:01 <Selavi> do you have to use the full flag name with the project prefix, or just simple name? I assume simple?
    13:01 <twb> I *think* there is also an option called something like "grub" or "portable" which tries to turn off bad things
    13:01 <CompanionCube> nope
    13:02 <WrongDevice> what i need to do for future reference to make a pool compatible with freebsd too
    13:02 <WrongDevice> and the truebsd is using zol now , all the zol features are compatible with trueos zol ?
    13:03 <twb> There's a table in the wiki somewhere of all the features and what systems support which ones
    13:04 <WrongDevice> where ?
    13:04 <twb> http://www.open-zfs.org/wiki/Feature_Flags
    13:04 <zfs-bot> [ Feature Flags - OpenZFS ] - www.open-zfs.org
    13:04 <twb> Sorry I'm a bit slow today, I'm not in my real office
    13:04 <jasonwc> twb, There is discussion about making such a flag.  It does not exist at this time.
    13:05 <twb> jasonwc: ah thanks.  I misremembered that it existed but didn't work very well :-)
    13:05 <jasonwc> WrongDevice, First off, do you need write access to this pool? If not, you can simply mount it read-only on OS X since those flags are read-only compatible.
    13:06 <twb> Is userobj_accounting similar to ext4's usrjquota?  i.e. per-UID EDQUOT block and inode caps?
    13:06 <jasonwc> WrongDevice, Relatively few features are not read-only compatible.
    13:06 <WrongDevice> yes i want to write
    13:07 <jasonwc> twb, man zpool-features says " This feature allows administrators to account the object usage information by user and group."
    13:08 <twb> I want to limit $HOME for human users to 1GB each, but I planned to do that by just giving each one a separate dataset with a cap (i.e. zfs create -o quota=1G pool/home/alice)
    13:09 <jasonwc> WrongDevice, Based on the feature flag chart, for OS X, you'll want to disable encryption, resilver_defer, allocation_class, large_dnode, project_quota, and userobj_accounting
    13:09 <lundman> depends what you run? 1.9.0 only project_quota and userobj_accounting
    13:10 <twb> lundman: 12:53 <WrongDevice> using openzfs osx 1.9.0
    13:10 <lundman> i only read 5 lines above, more is too heavy!
    13:10 <CompanionCube> but question was 'what flags for freebsd' though?
    13:10 <jasonwc> WrongDevice, The other ones are likely enabled but not active so they aren't causing issues.  Encryption won't be active unless you create an encrypted dataset.  resilver_defer is only active during a deferred resilver. Allocation classes would require adding a special vdev.  Large_dnode would require setting dnode size to something other than the default of legacy
    13:10 <CompanionCube> though the answer to that will be changing in the medium-term :p
    13:11 <jasonwc> WrongDevice, but you're getting hit by userobj_accounting and project_quota because they become active as soon as they are enabled and can't be disabled
    13:11 <jasonwc> the others can be disabled if not active
    13:11 <WrongDevice> lundman: hello
    13:11 <lundman> o/
    13:11 <WrongDevice> yeah 1.9.0
    13:11 <lundman> I only cut that this morning
    13:12 <jasonwc> lundman, The feature chart should be updated then.  It shows resilver_defer as unsupported on OS X master
    13:13 <WrongDevice> ok recreating now
    13:13 <lundman> it should
    13:13 <WrongDevice> i have to reboot to a liveusb
    13:13 <twb> WrongDevice: good luck! :-)
    13:13 <WrongDevice> yeah
    13:14 <WrongDevice>  thanks
    13:14 <lundman> good thing I added feature=disabled too
    13:15 <WrongDevice> i will no need to do disable anything else for use freebsd , i mean freebsd is too broken for my laptop atm , and too damm old graphic stack , this hackintosh works 100 times better than freebsd
    13:16 <WrongDevice> lundman: nice so this means i can create the pool here now and use it for linux
    13:16 <WrongDevice> maybe not good idea
    13:16 <WrongDevice> ...
    13:18 <jasonwc> WrongDevice, From looking at the feature chart, assuming you don't change defaults, if you disable project_quota and userobj_accounting, you should be fine on FreeBSD 12 as well
    13:18 <WrongDevice> lundman: one thing how i create a hostid spl hostid on mac ? or show i set a spl hostid like i set it on the linux kernel command line ?
    13:19 <WrongDevice> jasonwc: good to know thank u
    13:19 <WrongDevice> the table is not easy to read for me
    13:19 <WrongDevice> but i understand what i was looking for to solve my little issue with osx
    13:19 <lundman> it generates a hostid if not set. So either set it with sysctl, or just let it set for you
    13:20 <WrongDevice> lundman: i need to set the . same spl hostid for all my oses
    13:20 <lundman> because?
    13:20 <WrongDevice> i know how to do this before but i forgot
    13:20 <WrongDevice> to not have to import -f and export all the time
    13:21 <WrongDevice> im using  clover
    13:21 <lundman> just import -f on boot, no need to export
    13:21 <WrongDevice> any way to set it in clover ?
    13:21 <twb> WrongDevice: probably /etc/hostid doesn't match in your live system and real system
    13:21 <twb> WrongDevice: just copy /etc/hostid from one system to the other, so they have the same value
    13:21 <WrongDevice> i have two linux using the same hostid
    13:22 <WrongDevice> no is for osx
    13:22 <WrongDevice> that only maybe work on linux
    13:22 <WrongDevice> i dont know
    13:22 <twb> WrongDevice: ah, I misunderstood the question, sorry
    13:23 <CompanionCube> heh, you can see the results of https://github.com/zfsonlinux/zfs/pull/8641 on one of the feature tables :p
    13:23 <zfs-bot> [GitHub] [zfsonlinux/zfs #8641] rlaager: zpool-features(5) and other man page fixes | ### Motivation and Context This fixes some issues with zpool-features(5) and zfs-module-parameters(5)...
    13:24 <WrongDevice> lundman: ok how is set with sysctl ?
    13:25 <lundman> sysctl kern.hostid=
    13:25 <WrongDevice> where i can see my current spl.hostid
    13:25 <lundman> afaik
    13:25 <WrongDevice> ok
    13:25 <twb> and sysctl -a should print all the things it knows about, so "sysctl -a | grep hostid" should tell you something useful.
    13:26 <WrongDevice> hmm this number is bigger than the ones on linux
    13:26 <WrongDevice> on linux are jsut 8 chars
    13:26 <lundman> sysctl -x kern.hostid
    13:26 <twb> WrongDevice: AFAIK hostid is always a 32-bit integer
    13:26 <twb> WrongDevice: so the biggest value should be 4294967296
    13:27 <lundman> sysctl kern.hostid=0xde816bec
    13:27 <WrongDevice> nice
    13:28 <WrongDevice> yeah now i just need to use this one from osx on all the others linuxes
    13:28 <WrongDevice> thank u
    13:28 <twb> https://bugs.debian.org/595790  has a discussion of the "backstory" of hostid
    13:28 <zfs-bot> [ #595790 - The value from gethostid() should be more unique and not change when the host IP changes - Debian Bug report logs ] - bugs.debian.org
    13:29 <lundman> Fowler/Noll/Vo FNV-1a hash of your ioplatformuuidstr
    13:29 <lundman> if you want the details :)
    13:29 <lundman> (on osx)
    13:30 <twb> Is ZFS ever likely to get Zstd or LZMA2?  I'm thinking mainly of /var/log/ (write-once read-never).
    13:32 <jasonwc> twb, There's already a working PR for zstd on ZoL
    13:32 <twb> Cool
    13:33 <jasonwc> twb, https://github.com/zfsonlinux/zfs/pull/8044
    13:33 <zfs-bot> [GitHub] [zfsonlinux/zfs #8044] BrainSlayer: Support zstd compression (port of Allan Judes patch from FreeBSD) | This Patch adds zstd compression support zo ZFS ...
    13:33 <jasonwc> I was told it works fine aside from requiring more memory than other compressors
    13:33 <jasonwc> should be faster than gzip with better compression but much slower than lz4
    13:33 <twb> What abuot blake2? :-)
    13:33 <CompanionCube> originated from a freebsd diff, no? Still not merged in either, though.
    13:34 <jasonwc> yeah
    13:34 <jasonwc> Allan Jude wrote the FreeBSD one
    13:35 <jasonwc> I believe there is an issue if compressed ARC is disabled, but the option to disable compression for ARC may be removed
    13:41 <CompanionCube> twb: btw about the normalization thing, google finds good stuff like https://zfs-discuss.opensolaris.narkive.com/3NqQVG0H/utf8only-and-normalization-properties
    13:41 <zfs-bot> [ utf8only and normalization properties ] - zfs-discuss.opensolaris.narkive.com
    13:43 <twb> CompanionCube: thanks
    13:44 <CompanionCube> the difference is basically *when* the normalization happens
    13:46 <twb> Hrm, OK.  I assumed utf8only (no normalization=) meant that it'd allow non-normalized (i.e. mixed-normalization) codepoint sequences in file namse
    13:47 <CompanionCube> no normalization is nornalization=none though
    13:48 <twb> So surely that means when = never
    13:49 <twb> (unless the userland application chooses to normalize)
    13:49 <CompanionCube> (btw i meant 'diference' vs apple's method)
    13:50 <twb> ah
    13:50 <twb> That makse more sense :-)
    14:15 <rlaager> twb: normalization in ZFS preserves the bytes of the filename, in whatever form they are in. It only affects the indexing. So it won't break tarballs with a different normal form. However, it implies utf8only, which will be a problem if you unpack a tarball with non-UTF8 filenames.
    14:15 <twb> What does "indexing" mean, there?
    14:17 <rlaager> Sorry, the index of directory entries. So if you try to open("file_in_NFC") or open("file_in_NFD"), they open the same file.
    14:18 <twb> Whereas if there was no normalization, those would be different byte sequences and therefore different files?
    14:19 <rlaager> Correct.
    14:22 <twb> Righto
    14:24 <CompanionCube> and also isn't it nfd in particular because the other choices are either identical or slightly slower?
    14:29 <twb> Is the SMB and SID stuff in zfs manpages at all relevant to a linux samba AD/SMB server?  AIUI the answer is "no, that all happens inside samba and user xattrs, and is invisible to ZFS"
    14:58 <twb> rlaager: in your HOWTO, you try to keep the "OS" separate from the user/OS data.  But e.g. rpool/home and rpool/var/mail are still in separate trees.  Wouldn't it make more sense to have only 2 or 3 top levels, like rpool/OS/debian and rpool/USER/home rpool/USER/mail rpool/USER/www ?
    14:59 <rlaager> Yes, that's a perfectly reasonable approach.
    15:00 <twb> Any reason you did't already do that in the howto?
    15:00 <javashin> lundman, hi
    15:01 <rlaager> twb: I'm not 100% sure yet, and I'd like to avoid making changes that later get undone.
    15:01 <javashin> on osx there is a zfs-import service that try to import everything at boot ? if yes how i disable that ?
    15:02 <PMT> IMO rpool/.../USER/{www,mail,...} makes sense for a multi-user env, but at that point a lot of people might have .../USER on a different pool.
    15:02 <lundman> o/
    15:02 <twb> rlaager: righto
    15:02 <lundman> javashin: there is a launchctl script that run on boot
    15:02 <lundman> which executes the zpool-import-all.sh scipt
    15:02 <lundman> just comment out "zpool import" line in it
    15:02 <twb> Another thing I didn't understand: why are you making zfs's for intermediary paths, e.g. you have a "real" / and a "real" /var/mail, but you also make a /var that doesn't seem to do anything
    15:03 <javashin> what is the location of that script ?
    15:03 <javashin> and thanks by the way
    15:03 <lundman> scripts/zpool-autoimport.sh
    15:03 <lundman> but you probably meant once it is installed
    15:04 <javashin> yeah
    15:04 <lundman> one sec
    15:04 <javashin> no prob
    15:04 <lundman> the plist is in /Library/LaunchDaemons/org.openzfsonosx.zpool-import-all.plist   ... and
    15:05 <lundman> it executes /usr/local/libexec/zfs/launchd.d/zpool-import-all.sh
    15:05 <javashin> nice
    15:08 <rlaager> twb: The /var "container" is just to keep a 1:1 relationship between the dataset hierarchy and the filesystem hierarchy, such that inheritance works, avoiding the need to set mountpoints on rpool/var/*.
    15:10 <twb> But it's not 1:1 already bcause you have /root under /home in the pool, but not in the mountpoints
    15:11 <rlaager> Sure. That feels like it should be in /home. ;)
    15:14 <twb> So if I skip those empty mounts, all I have to do is set -o mountpoints on the descendants, and I'm A-OK?
    16:24 <twb> Can a zfs dataset have separate "soft" and "hard" byte limits?
    16:25 <twb> Right now on ext4, my users can go 20% over their soft quota for up to 1 week, which gives them "wiggle room", e.g. to do a "git repack" when they're over quota
    16:33 <twb> Is moving a directory tree between two datasets on the same pool and expensive operation?
    16:37 <CompanionCube> well, cross dataset 'mv' is actually 'cp+rm' if that's what you mean
    16:53 <twb> yeah I was just thinking that
    16:53 <twb> there's no way for the userland to do a move that just changes a couple of top-level dirents
    16:54 <twb> The main use case I'm thinking of is "oh I really want to move /var/foo out of /var and into its own dataset"

2019-04-29 #mailman::

    13:34 <twb> Does mailman typically store its mail under /var/mail/<listname>, or what?
    13:34 <twb> (It's been 10 years since I did any mailman and I have literally no memory of it)
    13:37 <twb> (What I have currently is just each LDAP group gets a mailbox that dovecot exposes as a separate shared namespace, because mailman 2 insisted on installing an httpd on my mail server.  IIUC mailman 3 lets me have a mail-only mailman and never install the passwords-and-CGI web UI)

2019-04-29 #zfsonlinux::

    13:16 <twb> In the olden days under solaris, you could only recv a replication snapshot to the root of a pool.  Is that still the case?
    13:16 <PMT> No.
    13:17 <twb> Cool.
    13:17 <PMT> And when was that the case?
    13:17 <twb> $coworker's memory from opensolaris-era ZFS
    13:17 <twb> My use case is basically I have 1 big server and 1 little server onsite, and I want to replicate all of <little server>'s pool onto <big server>, and then later replicate all of <big server> (including the copy of <little server>) to an offsite DR backup
    13:19 <PMT> -R will work on non-root datasets. I don't actually recall when that was the case, though I'm sure commit logs from illumos do.
    13:23 <twb> So I can do like ssh little zfs send -I @yesterday -R pool@today | ssh big zfs recv pool/little-backup/
    13:23 <twb> Maybe the "problem" was when recv gets -F...
    13:29 <PMT> twb: it'd be just -I, -R is implicit with capital I
    13:29 <twb> okey dokey
    13:29 <PMT> No, wait, I'm wrong, -I just means all snapshots between the two, -RI would be a replication of [...]
    13:29 <twb> It's been a while since I've done this stuff and the manpage is a bit overwhelming
    13:30 <PMT> Sorry, not enough caffeine, presumably.
    13:39 <PMT> (It's okay, I'm reasonably confident someone would have called out that I was objectively Wrong if I hadn't noticed. Accuracy by eventual consensus. :) )
    13:40 <twb> Let me just check who's likely to be awake... http://ix.io/1HvO
    13:41 <twb> Bad time of day for help from europeans or americans

2019-04-30 #refind::

    10:47 <twb> I notice that https://sourceforge.net/projects/refind/files/0.11.4/refind-flashdrive-0.11.4.zip/download  uses FAT12 instead of FAT32.  Is that intentional, or is that just an artefact of running "mkfs.vfat" without an -F32 on a small partition?
    11:45 <Hello71> perhaps it is more portable
    11:47 <Hello71> is it actually causing a problem for you
    11:50 <twb> fatresize and gparted (both using libparted) seem to be getting confused
    11:50 <twb> It's not clear to me how much this is the 12-bit-ness
    11:51 <twb> It's also not clear to me if UEFI requires FAT32, or FAT12/FAT16.  The Wikipedia wording makes it sound like it has to be FAT32 on internal hard disks, but has to be FAT12/FAT16 on USB hard disks.  Which is bloody stupid, but that's par for the course for UEFI specs.

2019-04-30 #dracut::

    12:50 <twb> If this is the dev channel, is there a user channel?
    12:52 <twb> I'm migrating from initramfs-tools to dracut on Debian 10 Buster, purely so I have systemd in the ramdisk, purely so my root-on-ZFS mount generators work nicer.  I want to ask some dumb questions like: does intel-microcode Just Work with dracut on Debian 10?  What's the equivalent of break=bottom?
    12:59 <twb> (By break=bottom I mean that it should mount the rootfs, but just before switch_root, drop me into a busybox ash rescue shell.  Exiting the shell would resume the boot process.)
    12:59 <twb> Also, on Debian 10, does dracut switch *back* to the initrd during shutdown, so that systemd's journal crap won't prevent my ZFS pool from being cleanly torn down?
    13:02 <twb> "rd.shell" seems to only give a shell iff the rootfs fails to mount; I haven't found one that ALWAYS gives you a shell
    13:34 <twb> Hrm, mount -t overlay support is Debian-specific
    14:54 <twb> I can see early_microcode defaults to yes, then it looks for AMD and/or Intel microcode in (by default) /lib/firmware/updates:/lib/firmware:/lib/firmware/$(uname -r)
    17:32 <twb> Why is dracut including less in the ramdisk?  bash I can sort of understand, because no busybox, but... less?
    17:32 <twb> And ls, which is just gross.  If RHEL is still using ls to enumerate files in sysconfig, I shall be very cross.
    17:35 <twb> In the dracut root /shutdown is declared as a #!/bin/sh, but it includes bashisms, so presumably it's nontrivial to just swap in busybox for all the GNU stuff and save 5MB
    18:15 <Mrfai> twb: I'm the Debian maintainer of dracut. I will look into your questions later today.
    18:15 <twb> Thanks
    18:15 <twb> Feel free to ignore any of my comments that were too grumpy :-)
    18:55 <twb> Should I worry that dracut is saying things like "dracut: zfsexpandknowledge: pool omega has device /dev/disk/by-id/ata-MB0500EBZQA_Z1M0FBG7-part1 (which resolves to /dev/sdb1)"
    18:55 <twb> Because those /dev/sdb1 resolutions aren't static and might not be the same next boot
    18:56 <twb> Also the root= looks pretty wrong: dracut: Stored kernel commandline: root=/dev/block/ rootfstype=zfs rootflags=rw,relatime,xattr,posixacl
    18:57 <twb> I'm in a live environment trying to set up the ramdisk for the first regular boot later on; it might be auto-guessing wrong because it's third-part "zfs-dracut", or it might be guessing wrong because I'm in a live environment
    19:21 <twb> Aha, this is good!  I saw debbugs comments that dracut triggers weren't A Thing yet, but it seems like in buster, they are.  Installing busybox caused dracut to rebuild.
    19:24 <twb> Does dracut understand $SOURCE_DATE_EPOCH (https://reproducible-builds.org) ?  Setting it doesn't seem to affect the timestamps as reported by lsinitrd.
    19:25 <twb> I think that must be a bug in lsinitrd, because the actual checksum doesn't change if I rerun dracut with SOURCE_DATE_EPOCH set
    19:26 <twb> except.. changing SOURCE_DATE_EPOCH doesn't then change the checksum, so... >confused<
    19:27 <twb> oh, oh, I see.  dracut without arguments writes to the wrong file (initramfs-4.19.0-4-amd64.img not initrd.img-4.19.0-4-amd64)
    19:28 <twb> Yeah, looking at initramfs-4.19.0-4-amd64.img, it clearly isn't doing the Right Thing with SOURCE_DATE_EPOCH
    19:32 <twb> apt install amd64-microcode intel-microcode did NOT trigger the dracut trigger :-(
    19:39 <twb> Even using datefudge @$SOURCE_DATE_EPOCH doesn't Just Work, because the mtimes are initialized by the kernel
    19:40 <twb> it would need a patch like this (from initramfs-tools's mkinitramfs script): http://ix.io/1HDd/bash http://ix.io/1HDe/bash
    20:44 <twb> is dracut smart enough to see add_dracutmodules+=" busybox " omit_dracutmodules+=" bash dash " and then later see another module depending on bash, and go "hey, this won't work!"
    20:44 <twb> Or will it just build it and break the next boot?
    20:55 <Mrfai> twb: the main dracut script dracut.sh is a bash script, so you always need bash in the initrd.
    20:56 <twb> dracut.sh *builds* the ramdisk
    20:56 <twb> it shouldn't affect what runs *inside* the ramdisk, which should be mostly systemd
    20:56 <twb> (right?)
    20:56 <Mrfai> In dracut there's no break=bottom. See dracut.cmdline for the vaild options: rd.break={cmdline|pre-udev|pre-trigger|initqueue|pre-mount|mount|pre-pivot|cleanup}
    20:56 <twb> ah rd.break=pre-pivot should be close to break=bottom
    21:00 <Mrfai> yes, you are right. No bash scripts are executed inside the initrd.
    21:02 <twb> Mrfai: /etc/shutdown inside the ramdisk contains bashisms, though, and dracut seems to be bin/sh -> bash inside my ramdisk.  I'm not sure if that's just traditional RH laziness.
    21:03 <Mrfai> Do you mean dpkg triggers for dracut? Currently there's one trigger. which IMO is not perfect. But it's not easy do detect when to rebuild the initrd, after some packages on the host have changed.
    21:04 <twb> Mrfai: the one that makes me especially cranky is (AFAICT) a dracut rebuild isn't triggered by installing dracut-config-generic
    21:05 <twb> Which seems like the most important case-  "I'm about to transplant my disks to a new computer, so make the ramdisk portable"
    21:06 <Mrfai> I'm not a trigger expert. But I would like to see better trigger support in dracut. Do you know how to write such a trigger?
    21:06 <twb> I know vaguely
    21:07 <twb> AIUI basically you say "dear dpkg, if any file under /etc/foo changes, tell me".  And what dpkg actually does is run your postinst with some options
    21:08 <twb> I have a config package that removes "weekly" and "yearly" words from /etc/logrotate.d files, so I register my own trigger for that tree.  In my case, it's idempotent, so I don't actually bother to check the args properly
    21:08 <twb> In your case I think you'd register a trigger for something like /usr/lib/dracut/modules.d/, but I'm definitely not an expert
    21:09 <Mrfai> For dracut a trigger that watches /etc/dracut.conf.d and /etc/dracut.conf should be the most important.
    21:10 <twb> I think /etc/dracut.conf itself would be moot, because only you yourself manage that (apart, possibly, from diversions)
    21:10 <twb> but I agree re /etc/dracut.conf.d/
    21:12 <twb> It's about 2 hours past my bedtime, so I have to go.  I should be back tomorrow, though.  If I forget and you want to talk packaging, I'm always in #debian-au on OFTC.
    21:12 <twb> Thanks for your help so far

2019-04-30 #debian-au::

    13:36 <twb> pabs: hey, you seem to know all the debian ML discussions.  Do you know whether there's a general plan to switch Debian to systemd and/or dracut in the ramdisk?
    13:43 <pabs> dracut BoFs have happened several times at recent DebConfs IIRC. not sure when/if that will happen though
    13:45 <twb> The driver for me is that, basically, upstream systemd doesn't support systems with non-systemd ramdisks
    13:45 <twb> Which mostly manifests in obscure edge cases, such as:
    13:45 <twb> 1. multi-disk btrfs doesn't work properly with systemd is not the ramdisk; systemd tries to mount ALL the nodes, not ANY of the nodes
    13:46 <twb> 2. journald keeps /var/log/journal open, which prevents the root (and its underlying disks) from being torn down cleanly during shutdown, unless the OS switch_root's *BACK* into the ramdisk during shutdown
    13:46 <twb> These were problems I encountered as at systemd v215 in Jessie, so *might* be moot nowadays
    13:53 <rjsalts> DKIM works too for being able to validate From: header is legit
    13:54 <pabs> twb: not sure about those issues, I suggest testing buster (in a VM perhaps) and then bringing it up on debian-devel
    13:56 <twb> Well, right now my attitude is basically "fuck it, let's try dracut"
    13:56 <twb> I don't normally use it because 1. it pulls in 50MB of perl Depends and 100MB of cryptsetup/luks/mdadm/dmraid/lvm2 Recommends; and 2. it doesn't do boot=live OOTB
    13:56 <twb> Although I just discovered a Debian-specific patch which does 50% of boot=live as "rootovl"
    13:57 <rjsalts> I don't think unstable is using dracut
    13:58 <rjsalts> but I don't think I have problems with root unmounting cleanly on shutdown
    13:58 <rjsalts> not sure on the btrfs, as I'm using xfs as my fs of choice
    14:02 <twb> rjsalts: I'm not suggesting dracut is the default _yet_
    14:02 <twb> I'm asking if that's a goal debian has
    14:05 <pabs> I believe the dracut maintainer wants it to be the default yes
    14:05 <pabs> not sure about the kernel team
    14:12 <rjsalts> I was getting suggestions to move to plymouth last time I ran apt update on unstable, being dragged in by recommends

    14:36 <twb> I hate that dracut is one big bash script, just like initramfs-tools
    14:36 <twb> Like, surely by now you can use python or perl
    14:38 <twb> It's full of shit like if ! [[ ${KERNEL_INSTALL_MACHINE_ID-x} ]]; then exit 0; fi
    14:38 <twb> Like... do you even?
    14:39 <twb> [[ $dracut_rescue_image != "yes" ]] && exit 0   implies they're running it with error handling disabled
    14:39 <twb> So if anything goes wrong, you just ignore it, instead of stopping and asking the user to fix you
    14:39 <twb> lulz they're adding the exit codes together, so it'll wrap around
    14:42 <twb> lulz dracut.css isn't a CSS file
    14:42 <twb> dracut.asc appears to be asciidoc, rather than a GPG signature
    14:43 <twb> all the bash scripts in dracut are setting LANG=C instead of LC_ALL=C, so if e.g. I have LANG=en_AU.UTF-8 LC_CTYPE=fr_FR.UTF-8, it won't remove the latter
    14:45 <twb> Why the fuck does dracut have its own logfile
    14:57 <twb> https://bugs.debian.org/753752  grrrr
    15:08 <twb> # This is kinda legacy -- eventually it should go away.
    15:08 <twb> case $dracutmodules in ""|auto) dracutmodules="all" ;; esac
    15:08 <twb> grrrrrr
    15:08 <twb> (FTR, initramfs-tools is just as bad)
    15:08 <twb> literally the only reason I haven't just replaced initramfs-tools in debian is I don't want to have an argument with all the people RELYING on its bugs
    15:17 <rjsalts> initramfs-tools is debian native, yes?
    15:17 <twb> yes
    15:17 <twb> It's really really old
    15:17 <twb> busybox was also written by debian for initramfs
    15:18 <rjsalts> yeah, I think I remember the transition to it in etch
    15:19 <rjsalts> from some even more kludgy collection of shell scripts
    15:20 <twb> as a trivial example, in initramfs-tools cryptsetup scripts, you get 3 goes to get your passphrase right.  After that, it just hangs.
    15:21 <twb> like, enough that ctrl+alt+del even won't help
    15:21 <rjsalts> why 3?
    15:21 <twb> because that's how many were coded into the script
    15:21 <rjsalts> not a while loop?
    15:22 <twb> correct
    15:22 <twb> presumably because they didn't want to trigger an infinite busy-loop if /dev/tty was wonky
    15:23 <rjsalts> couldn't you put a test for that inside the while with a continue or whatever?
    15:24 <rjsalts> can you run systemd itself in the initramfs?
    15:24 <twb> rjsalts: initramfs-tools doesn't support systemd init
    15:24 <twb> rjsalts: dracut ONLY supports that, which is why it's what I'm trying to use today
    15:25 <rjsalts> is a simpler system than dracut that worked with systemd possible?
    15:26 <twb> it's theoretically possible
    15:26 <twb> I don't know if anyone has done it.
    15:26 <twb> tiny-initramfs exists but it is using its own C program a la RH's old pre-systemd "nash" ramdisk, where e.g. the NFS syscalls and baked into the stand-alone /init program
    15:27 <twb> Actually, I know someone in #systemd who is doing a non-dracut systemd ramdisk, but they're doing very Unique Snowflake stuff
    15:27 <rjsalts> busybox does that on initramfs-tools initrds?
    15:27 <twb> rjsalts: not to the same extent
    15:28 <twb> busybox uses klibc-utils for nfsmount and ipconfig
    15:28 <twb> And when busybox ash is told to e.g. run "ping", it'll look for /sbin/ping, and run that as a separate process, even if it's a symlink back to busybox.
    15:28 <twb> That's configurable in the busybox compile-time config, but Debian has told it to behave that way
    15:28 <twb> CONFIG_PREFER_APPLETS or something
    15:29 <twb> CONFIG_FEATURE_PREFER_APPLETS=y
    15:32 <twb> The main things needed for systemd in the ramdisk is for default.target to be a symlink to initrd.target (not multi-user.target), and for /etc/machine-id and friends to be copied in
    15:32 <twb> The main things needed for systemd in the ramdisk is for default.target to be a symlink to initrd.target (not multi-user.target), and for /etc/machine-id and friends to be copied in
    16:29 <twb> rjsalts: oh btw re plymouth, at least some systems are incapable of correctly prompting for recovery from a fsck problem, unless plymouth is in the ramdisk
    16:30 <twb> rjsalts: even if it's only plymouth's pseudo-text version
    16:30 <twb> I *think* I saw that on Ubuntu, but I'm not too sure.
    16:44 <themill> «scrub /dev/sda3» nom nom
    17:30 <twb> dracut includes bash in the ramdisk
    17:30 <twb> SAVAGES
    17:30 <twb> And *not* busybox at all, which is bullshit, because it's super handy for recovery
    17:30 <twb>   lrwxrwxrwx 1 root root    4 2019-04-29 19:26 loginctl -> true
    17:30 <twb> lulz
    17:33 <twb> This is dumb, they're including less and GNU cp, mv, ls, sed, &c
    17:33 <twb> 6MB instead of 1MB

    19:01 <twb> haha origin is <NULL> on my bodge-arse repos
    19:29 <twb> dracut doesn't understand SOURCE_DATE_EPOCH
    19:29 <twb> so e.g. a Debian Live ISO built using dracut instead of initramfs-tools will always be irreproducible

2019-04-30 #zfsonlinux::

    15:25 <twb> So when I screw up making a pool, I want to tear down the existing pool before making a new one.
    15:25 <twb> "zpool export omega" and "zpool export -f omega" both fail with "umount: /mnt: target is busy.  cannot unmount '/mnt': umount failed"
    15:25 <twb> lsof says it's not in use.
    15:26 <twb> "zfs umount -a" exits happily.  "umount -a -t zfs" gives the same error as "zpool export"
    15:31 <twb> Also when I try "wipefs -a /dev/sda", I get "wipefs: error: /dev/sda: probing initialization failed: Device or resource busy"
    16:10 <twb> OK, after a reboot, the pool is auto-imported, but I can "zpool export" it at least
    16:10 <twb> And then I can wipefs the underlying drives

    18:41 <twb> Oh boy, I forgot how slow dpkg is when you have to wait for an actual disk
    18:42 <twb> (I usually use dpkg with --force-unsafe-io, on tmpfses, so the fsync spam is nopped away)
    18:46 <nils_> yeah it's very slow especially with ZFS or on spinning rust, I was thinking of maybe doing a snapshot before and after and then disabling fsync() with force-unsafe-io.
    18:47 <twb> The case that was screwing me before (on ext4) was collecting spamming randomized writes to one LV, and dpkg installing something to another LV on the same disks
    18:48 <twb> The syncs at ~200ms intervals prevents the kernel from linearizing the RRD writes for more than that, so the entire OS crashed to a halt and had to be hard rebooted
    18:49 <twb> I "solved" that by telling collectd to do its own RRD buffering in RAM for up to 30min, and to stagged the individual flushes to disk for good measure
    18:50 <twb> Which just meant that if you look at the pretty performance graphs, they're up to 30 minutes behind real-time
    19:01 <rlaager> twb: You might want to set sync=disabled while running dpkg.
    19:01 <twb> is that per dataset, or per pool?  /me checks manpage
    19:03 <twb> I mean, to be fair, dpkg is doing it out of entirely reasonable paranoia.  dpkg doesn't know when the proprietary HDD controller firmware is going to wig out
    19:04 <rlaager> The history on this is long and complicated. See at least this, and everything it links to, including the references to the whole ext4 file renaming thing: https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=605384
    19:04 <zfs-bot> [ #605384 - d-i should use dpkg --force-unsafe-io to optimize installation time - Debian Bug report logs ] - bugs.debian.org
    19:05 <rlaager> The sync= property is per dataset, but of course is inherited like everything else, so you could set it on the pool and just let it inherit down through everything.
    19:06 <rlaager> I haven't tested an install both ways, so I'm not sure how much it matters. I'm assuming that debootstrap does NOT use dpkg --force-unsafe-io, but I don't actually know. I could use eatmydata in the HOWTO, but that name is scary, and it's another dependency.
    19:07 <rlaager> I'd like to set sync=disabled in the HOWTO, but I'm afraid someone might miss the later step to turn it off, and then their data is at risk.
    19:07 <twb> eatmydata is a huge pain and force-unsafe-io is a better replacement.
    19:07 <rlaager> AFAIK, force-unsafe-io only addresses dpkg's syncs, not those from maintainer scripts and things they call, though.
    19:08 <twb> debootstrap is just a crappy bash script; it will default to calling dpkg if you run it on a system with dpkg, but I don't know if you can pass options to it.  Obviously you need some of the rootfs to be present before you crate  /etc/dpkg/dpkg.cfg.d/10-racing-stripe
    19:08 <twb> rlaager: correct, but dpkg sends a minimum of 2 syncs per directory (it used to be 2 per file, but it changed several times to work around bugs I found in dpkg-on-btrfs, and I lost track)
    19:09 <rlaager> Anyway, hopefully one day this will be in the installer, which can safely use sync=disabled for speed without the risk of a human forgetting to undo it. :)
    19:10 <twb> --extractor=ar is probably enough to bypass the syncs when using debootstrap
    19:10 <twb> rlaager: BTW what does devices=off do in your howto?  I read the manpage but I didn't understand it
    19:11 <rlaager> twb: devices=off == nodev
    19:11 <twb> oh, I see
    19:11 <twb> So really nodev,nosuid,noexec should be set on basically everything except like /usr
    19:12 <rlaager> It's the one "security" type thing I left after I removed all the nosuid/noexec stuff. These days, nobody really needs device nodes.
    19:12 <rlaager> If you dig through the history, you'll find more nosuid/noexec. I removed those because they break random things, like Postfix (which chroots into /var so needs at least exec).
    19:12 <twb> I wish Kees Cook would just make linux default to all three so you had to mount the rootfs with -o exec
    19:13 <rlaager> Yeah, you're clearly the target audience for the old approach. :) Dig through the history of that wiki page (a git checkout is easier than the web interface for that).
    19:13 <twb> no worries
    19:13 <pink_mist> 11:11 <twb> So really nodev,nosuid,noexec should be set on basically everything except like /usr <-- uhm, I would like the stuff in /bin and /sbin to continue working
    19:13 <twb> pink_mist: handwave
    19:14 <pink_mist> also I would like to be able to compile and run code from my home dir
    19:14 <twb> I do get people whinging occasionally because they can't run scripts from /home
    19:14 <rlaager> To be clear, I was setting the restrictions only on /var, then unsetting on /var children as needed.
    19:15 <rlaager> Well, or something like that. You could exec from /home.
    19:15 <twb> OTOH most of my end users are detainees (convicted or remanded into custody)
    19:17 <twb> noexec doesn't actually stop you using things like bash, python, tcc.  It just stops ./a.out.  It's about as hard to break out of as chroot(2).

    19:33 <twb> rlaager: have you tried zfs-dracut on Debian?
    19:47 <jtara> rlaager: remember that even if sync=disabled, you still have a guarantee of data durability at 5 second intervals (by default) and stronger ordering guarantees than sync=standard
    19:47 <jtara> though
    19:47 <twb> plus obviously a COW
    19:47 <jtara> i would ask why dpkg is issuing sync writes
    19:48 <jtara> twb that doesn't change at all though
    19:48 <jtara> you always cow.  cow cow uber alles.
    19:48 <twb> It's basically paranoia because dpkg doesn't have any confidence that stuff is actually hitting a disk.
    19:49 <twb> Because like, for all they know the rootfs is a fuse driver on top of iSCSI on top of IP-over-pigeon
    19:49 <twb> You can argue that spamming syncs doesn't help, but I can totally understand how they got there
    19:50 <jtara> a lot of programs abuse fsync, they really do
    19:50 <jtara> but it shouldn't be that damaging if you avoid indirect sync and rmw
    19:50 <twb> Obviously what you really want is for dpkg to unpack to a purely functional filesystem like zypperfs :P
    19:50 <jtara> well that would be nice too :)
    19:51 <jtara> and a kitten
    19:53 <jtara> anyway it's one reason why i recommend a slog so much, they help an unbelievable amount when you have apps spamming fsync
    19:53 <jtara> otherwise if you have async writes to a file, and then an fsync to seal it off...all those async writes get committed to the pool as indirect sync
    19:53 <jtara> and that really toads the wet sprocket
    19:54 <twb> Is that tanker jargon?

    20:05 <twb> jtara: re "why does dpkg even sync?" Message-ID: <19699.43281.82329.482744@chiark.greenend.org.uk> is a pretty good snark (from rlaager's earlier link)
    20:08 <jtara> ah thanks
    20:09 <jtara> i'm of the opinion that everything in the universe should do writeahead logging, but reality hasn't caught up :p
    20:11 <twb> I use sqlite WAL and NFSv4 occasionally bricks it
    20:11 <twb> Although obviously that's NFSv4's fault
    20:12 <jtara> yeah i could see that
    20:12 <jtara> i'm on this massive kerberized nfs rollout right now
    20:12 <twb> only sec=sys here
    20:12 <jtara> credential rotation and management ends up being a colossal hassle
    20:12 <jtara> we deal with PCI, PII and every other TLA i can think of so
    20:13 <twb> I have given up on non-AD krb as too much hassle
    20:13 <jtara> yeah i'm letting our AD team manage that end of things
    20:13 <jtara> they just give us an OU to manage under
    20:13 <twb> cool
    20:14 <twb> I did a samba 4.0.0 AD rollout that was.. exciting
    20:14 <twb> I had to hire one of the samba core devs to help me
    20:15 <jtara> my sympathies
    20:16 <jtara> i'm stuffing gssproxy into a container and ducting auth stuff to it through the pachinko machine that is gss
    20:17 <jtara> so that we can make it generally available on kube/docker
    20:18 <twb> http://cyber.com.au/~twb/tmp/docker.pdf
    20:18 <jtara> tbh the messy part is when you have the same keytab on a few hundred hosts and you need to handle automatic rotation cleanly
    20:19 <jtara> without interruption
    20:19 <twb> Did you know pachinko machines were originally based on a device used to demonstrate the normal distribution to stupid politicians
    20:19 <jtara> hah, that's great
    20:20 <twb> https://www.youtube.com/watch?v=AUSKTk9ENzg  (sorry, I can't find the wikipedia link easily)

2019-05-01 #dracut::

    11:36 <twb> Mrfai: hey, so re "if I omit the bash module, will it stop me, or will it just break?"
    11:37 <twb> It will definitely Just Break things, because lots of in-ramdisk scripts are written as "#!/bin/sh", but use bashisms, and do not declare a depends() on bash
    11:44 <twb> As a trivial example, modules.d/98dracut-systemd/dracut-cmdline-ask.sh needs /bin/bash (for sleep 0.5), but modules.d/98dracut-systemd/module-setup.sh does not depends() { echo "bash ..."; }
    11:55 <twb> Coming at this project cold, I'm surprised that the dependencies aren't done in a declarative file (a la debian/control), and the install rules aren't usually declarative (a la debian/install), that conflicts declarations are done indirectly (if ! dracut_module_included "PACKAGE"; then return 1; fi)
    11:56 <twb> I'm also really surprised that bash (heavyweight and slow) is still being used when part of the salespitch for systemd is that everything is little C programs with faster spin-up times.
    11:56 <twb> Possibly that's because RH switched from nash to dracut earlier than I remember, before systemd was A Thing
    11:59 <twb> Some scripts try to detect errors by adding exit statuses together, e.g. 51-dracut-rescue-postinst.sh:    ((ret+=$?)) --- this is not safe if the exit codes wrap around, or add up to an exit code with special meaning (e.g. 127 means "command not found", 126 means not an executable regular file)
    12:01 <twb> shellcheck also detects a large number of sh/bash newbie mistakes that should be easy to fix, such as treating an array as a scalar
    12:14 <twb> http://ix.io/1HHU  (some of them are false positives; I haven't gone through it in detail yet)
    13:13 <twb> In modules.d/05busybox/module-setup.sh, it's post-processing the help output of busybox.  In busybox versions since 2010, it can instead just use the output of busybox --list or busybox --list-full.
    13:14 <twb> It can also just use busybox --install -s to create the symlinks directly.
    13:14 <twb> None of these approaches (including the approach in current dracut) will work with busybox "make allnoconfig" (minimal build).
    13:21 <twb> What is $mount_needs ?
    13:22 <twb> Oh, it appears that check() can be called to do *two different* kinds of checks, and looking at whether $mount_needs is defined allows check() functions to know which kind of check they are supposed to be checking.
    13:31 <twb> Looks like dracut will not correctly pick up on any changes the sysadmin makes with "systemctl edit", since those go into /etc/systemd/system/foo and dracut is only checking /lib/systemd/system/foo

2019-05-01 #debian-au::

    13:26 <twb> How hard would it be to construct the ramdisk out of udebs
    13:27 <twb> I mean... this dracut code is all shitty shell scripts with *NO* error handling, and 95% could be trivially replaced by declarative rules a la debhelper
    13:27 <twb> And udebs are already set up to e.g. have the kernel modules split up into as-needed sections, and they have the docs stripped out &c
    13:28 <twb> And (hopefully) anna-install is smart enough to refuse to install conflicting udebs
    13:28 <twb> Right now, the dracut code all just goes like "if /bin/sh exists, silently do nothing, otherwise install myself as /bin/sh"
    13:36 <pabs> probably pretty easily. debirf can create initrds from a normal system
    13:38 <twb> pabs: hrmm, I didn't know about debirf
    13:39 <twb> pabs: oh that's *THAT*.  Wow that's cool, I've been thinking about building that for ages
    13:40 <twb> SIGH, another bash script
    13:40 <twb> goddammit people stop using bash
    13:40 <twb> At least this one is 1KLOC not 8KLOC like dracut
    13:44 <twb> http://ix.io/1HI8/bash

    14:41 <pabs> Debian has support for drop detection via the hdapsd package
    14:41 <twb> Cool
    15:54 <rjsalts> also https://www.backblaze.com/blog/hard-drive-stats-for-2018/ is educational in respect to the drives they buy

    14:01 <twb> pabs: /usr/share/debirf/modules/z0_remove-locales  suggests they're being at least slightly silly (not using udebs, or not using dpkg --path-excludes a la localepurge)
    14:01 <twb> Ahaha, I just noticed it's explicitly keeping "en*" locales
    14:39 <twb> pabs: debirf didn't Just Work on buster, I think probably because recent debootstrap minimal no longer includes "init"
    14:39 <twb> http://ix.io/1HIr
    14:42 <pabs> twb: probably worth filing a bug about that issue

    16:27 <twb> «Removable media without either GPT or MBR formatting is considered a "superfloppy"»
    16:27 <twb> Stay classy, Microsoft.
    16:28 <rjsalts> I couldn't make a udf super floppy with a usb hd in windows
    16:28 <twb> What I'm trying to tell is whether this is a valid ESP:
    16:28 <twb> SEC_TYPE="msdos" LABEL_FATBOOT="ESPOMEGA" LABEL="ESPOMEGA" UUID="C08E-115B" TYPE="vfat" PARTLABEL="ESP-omega" PARTUUID="1971ae22-c5e5-4fab-ac35-2d32029e1962"
    16:28 <twb> This one, definitely is:
    16:29 <twb> SEC_TYPE="msdos" LABEL_FATBOOT="ElTorito" LABEL="ElTorito" UUID="FE20-DD95" TYPE="vfat" PARTUUID="c271db03-317d-44ed-aad3-1a7c34d614c7"
    16:29 <rjsalts> for the live image?
    16:29 <twb> for a server
    16:29 <twb> ElTorito is, I think, just Rod smoking crack
    16:29 <twb> wget --content-disposition https://sourceforge.net/projects/refind/files/0.11.4/refind-flashdrive-0.11.4.zip/download
    16:29 <twb> ^ that's the second one's origin

    <see the #cyber IRC log from the same day -
     the problem is that on gigabyte mainboard,
     the mainboard can't see refind/refind_x64.efi on a USB key's ESP,
     though it *CAN* see boot/bootx64.efi on a USB key's ESP.
     I think the problem is that it only allows efibootmgr magic
     for internal AHCI hard disks, not
     for external USB hard disks.>

    17:01 <twb> Oy, MacOS spells "FDE" with a W
    21:57 <twb> Sveta: hey hey hey
    21:57 <twb> remember when you were getting "no such file" for "mount -t vfat /dev/sdx1 /mnt"
    21:58 <twb> in the ramdisk
    21:58 <twb> Well, I can reproduce that problem on Debian 10 Buster
    21:58 <twb> MOST likely is that mount is from util-linux there, and the file that isn't found is the dynamic linker
    21:59 <twb> damn, "busybox mount" has the same problem

2019-05-01 #zfsonlinux::

    13:49 <lundman> bah "Fix the spelling of deferred" contains a bunch of code too
    13:52 <twb> Hey are you fixing spelling in ZOL codebase?
    13:52 <twb> Because lintian auto-detected about 5 typos
    13:53 <twb> http://ix.io/1HIa
    13:53 <lundman> no, I'm cherry-picking in commits - and thought I was doing non-code related commits
    13:53 <lundman> only to find compiling breaks
    13:53 <twb> Curses
    14:19 <twb> May 01 13:19:33 jasmine kernel: INFO: task txg_sync:1440 blocked for more than 120 seconds.
    14:19 <twb> That sounds pretty scary
    14:28 <rlaager> lundman: It certainly does not contain a bunch of code. The commit consists of one spelling correction in zpool-features.5 and one spelling fix in a comment in zil.c.
    14:41 <twb> So, under the hood, dracut is really bad.  As bad as initramfs-tools.  Right now I'm leaning towards "stick with the devil you know" (i.e. initramfs-tools).
    15:00 <twb> I guess what I need to do is look at zfs-dracut vs. zfs-initramfs specifically
    15:01 <twb> If zfs-dracut is correctly using 0.8's systemd zfs-mount-generator, that would be a strong argument in its favour
    15:01 <lundman> rlaager: yeah checked ZOL commit and it doesnt, git blame said it did
    15:02 <lundman> either way, corrected locally
    15:02 <prometheanfire> I forget if it does, I'm running it on systemd, any easy way to tell?
    15:03 <prometheanfire> usr-portage-distfiles.mount                                                            loaded active mounted   /usr/portage/distfiles
    15:03 <prometheanfire> looks right?
    15:04 <twb> prometheanfire: I _think_ the recommended way is to test for the existence of /run/systemd.  There is also "systemctl is-systemd-running", but that does something else.
    15:04 <twb> prometheanfire: sorry, I misread
    15:04 <prometheanfire> /usr/lib/systemd/system-generators/zfs-mount-generator
    15:05 <prometheanfire> I'm guessing that's what you want
    15:06 <twb> prometheanfire: yeah but dracut doesn't copy all the systemd units into the ramdisk; it cherry-picks them :-/
    15:07 <prometheanfire> ya, dracut, well dracut just mounts the rootfs as defined in the zpool
    15:07 <prometheanfire> then chroots to it
    15:08 <twb> But it has to mount it in a way systemd understands, otherwise systemd will screw up
    15:08 <twb> Which is what the generators are for
    15:08 <twb> It looks like there's a separate dedicated one: usr/lib/systemd/system-generators/dracut-zfs-generator
    15:09 <prometheanfire> ah
    15:09 <prometheanfire> /usr/lib/dracut/modules.d/90zfs/zfs-generator.sh (installed)
    15:11 <twb> Yeah the entry point is module-setup.sh in that dir.  It is sourced by dracut.sh at build time in the build environment.  The other files in that dir (typically) run inside the initramfs
    15:12 <twb> I hate that this code is all crummy bash scripts
    15:12 <prometheanfire> your welcome :P
    15:12 <prometheanfire> I wrote some of that
    15:13 <twb> did you run shellcheck on it? ;-)
    15:16 <twb> Urk, it just says "install awk" without worrying about which awk will actually be used.
    15:18 <twb> Looks like that's specific to zfs-dracut.
    15:20 <twb>         if [ $(zpool list -H -o feature@encryption $(echo "${ZFS_POOL}" | awk -F\/ '{print $1}')) = 'active' ]; then
    15:20 <twb> should probably be
    15:20 <twb>         if [ "$(zpool list -H -o feature@encryption "$(echo "${ZFS_POOL}" | awk -F\/ '{print $1}')")" = 'active' ]; then
    15:21 <twb> That can be replaced by cut -d/ -f2
    15:22 <twb> depends() does not declare a dependency on systemd-initrd, but it is calling "systemctl is-failed" in zfs-load-keys.sh
    15:23 <twb> Ah, zfs-load-key is intentionally a NOP if systemd isn't installed, so that's OK
    15:37 [twb looks inside /etc/zfs/zfs-functions and starts crying]
    15:37 <twb> "# Of course the functions we need are called differently on different distributions - it would be way too easy otherwise!!"
    15:38 <twb> This is like sausages and legislation
    16:33 <lundman> disappointing you guys went with /sys/module/zfs/version for version control
    16:35 <twb> lundman: that seems to be "0.8.0~rc4-1" for me, i.e. the debianized zol version
    16:36 <twb> But I've got like /sys/module/tcp_cubic/version = 2.3, so I guess that's vaguely standard?
    16:37 <lundman> i meant more - its not portable anywhere else :)
    16:38 <twb> ah, you wanted something in sysctl I guess
    17:16 <twb> If I have an slog drive, and it dies, what happens to the pool?
    17:21 <lundman> slog moves back into pool storage as if you didnt have an slog
    17:22 <twb> so basically it will recover on its own?
    17:22 <twb> What happens to the writes that were in-flight in the slog?
    18:04 <twb> How come "zfs create -V 4G foo/bar" reports foo/bar as using 4.13G ?
    18:05 <twb> Is the 0.13 additional metadata, or is it the difference between 4GiB and 4GB ?
    18:05 <JMoVS> 4GiB is 4.294967 GB
    18:05 <twb> 4GiB - 4GB is about 281MiB, so it can't be that.
    18:12 <twb> cp --target-directory="$DESTDIR" --parents $(find /lib/ -type f -name libgcc_s.so.1)
    18:13 <twb> ...why doesn't that just do find /lib/ -type f -name libgcc_s.so.1 -execdir cp -pt "$DESTDIR" {} +
    18:19 <twb> Hahah "# With pull request 1476 (not yet merged) comes a verbose warning if /usr/bin/net doesn't exist or isn't executable. Just create a dummy..."
    18:20 <twb> Why is /usr/share/initramfs-tools/scripts/local-top/zfs talking about LVM?
    18:50 <Marbug> Good morning! I'm looking into an issue I have with user id mapping on a partition shared with sharenfs, in the documentation about the options of sharenfs, they redirect you to the options of exports, although, when I for example specify sharenfs=mapall.. it says it's an invalid option, is it even possible to map user id's to a new id? lets say, user id 2345 to 1000?
    18:59 <twb> Why can't I ever "zpool export -af"
    19:00 <twb> It just says "pool is busy", but won't tell me WHY
    19:00 <twb> Nothing is mounted according to /proc/self/mountinfo, and "zpool iostat 5" shows no reads or writes
    19:01 <twb> There's nothing in dmesg or journal (syslog) either.
    19:07 <twb> >super frustrated!<  I'll patch out the "zfs import -aN" from my recovery image and then reboot again.  It won't fix the "I can never export" problem, but it will at least let me import with -R
    19:21 <twb> OK.  When I disable boot-time ZFS magic, I can manually "zpool import -R /mnt omega" and then "zfs export omega" works fine.
    19:23 <twb> But THEN, I can't mount omega/ROOT again, because it's set to be mounted by hand, but all the other datasets are set to automount, so they've already gotten mounted before I got there!
    19:25 <twb> "zpool export omega", "find /mnt/ -mindepth 1 -depth -exec rmdir {} +" "zpool import -N -R /mnt omega", "zfs mount omega/ROOT", "zfs mount -a"
    19:25 <twb> OK, THAT is working.  AND I can still "zpool export".
    20:08 <twb> Is /etc/hostid supposed to be copied into the initramfs-tools ramdisk?  IIRC zfs-dracut did it...
    20:08 <DeHackEd> probably....
    20:08 <twb> Hrm... contrib/initramfs/hooks/zfs.in:COPY_FILE_LIST="/etc/hostid @sysconfdir@/zfs/zpool.cache"
    20:09 <twb> So it's *supposed* to be there.  Probably I screwed something up
    20:09 <twb> contrib/dracut/90zfs appears to support setting it at boot time, as well
    20:15 <twb> SUCESSSSSS
    20:16 <twb> I have gigabyte -> refind -> zfs-initramfs -> zfs rootfs working
    20:16 <twb> As usual, systemd is stupid and doesn't umount /var/log/journal at shutdown time, though, so probably the entire pool is imported when power is cut :-/
    20:36 <twb> rlaager: one downside of zfs managing all the mounts, is that "df -h" is not sorted anymore!
    20:38 <twb> Also oddly, all my stuff is mounted even though I don't see any output from zfs-mount-generator .mount units...
    20:38 <twb> I guess "mount -a" happened as some kind of fallback behaviour, because /etc/zfs/cache wasn't properly populated
    20:39 <twb> * /etc/zfs/zfs-list.cache
    20:45 /join #systemd
    21:08 <twb> rlaager: aha! after setting up /etc/zfs/zfs-list.cache/<pool name>, and rebooting a couple of times, the zfs-mount-generator has kicked in
    21:09 <twb> i.e. it's working
    21:19 <twb> OK, let's see how well zfs-dracut works...
    21:23 <twb> "don't know how to handle root=zfs=omega/ROOT"
    21:23 <twb> I think simply doing "apt install zfs-dracut" causes dracut to be installed, to build the ramdisk, *then* zfs dracut support to be installed, and the ramdisk isn't rebuilt (because ugh)
    21:24 <twb> https://bugs.debian.org/720081 &c
    21:24 <zfs-bot> [ #720081 - Add support for dpkg triggers - Debian Bug report logs ] - bugs.debian.org
    21:27 <twb> Nope, lsinitrd confirms that zfs support was compiled in
    21:29 <twb> That was a ramdisk built with "prefer busybox over shitty bloated bash", so maybe that's the problem.  Lemme try again without that...
    21:36 <twb> It's not clear from contrib/dracut/README.dracut.markdown if I *have to* "zpool set bootfs=", of if that's only for autodetect, and rootfs=zfs=X/Y will suffice
    21:36 <twb> Ah yeah it is clear
    21:36 <twb> "Alternatively, <just do root=...>"
    22:23 <twb> Booting Debian 10 with a zfs-dracut ramdisk, and "rd.shell" in the boot options, I get a debugging shell.  Unfortunately, the keyboard doesn't work --- probably because dracut has helpfully omitted the USB EHCI or USB HID drivers :-/
    22:23 <twb> I'll investigate that tomorrow

2019-05-02 #dracut::

    11:29 <twb> OK so yesterday I finally tried actually booting via dracut, and the zfs-dracut code isn't working.  I added "rd.shell" to debug it, and I was taken to a debug prompt... but the USB keyboard doesn't work.  Do I have to do something clever to force EHCI and HID drivers to load?  In initramfs-tools, there are explicit modprobes for things like that when the debug shell is triggered
    12:33 <twb> plugging in a "dumb" USB keyboard (instead of my fancy KVM) didn't help.  I'm about to pull apart the ramdisks now to see if e.g. the EHCI driver is even in the ramdisk
    12:49 <twb> -rw-r--r--   1 root     root       174868 Mar 15 13:16 usr/lib/modules/4.19.0-4-amd64/kernel/drivers/usb/host/ehci-hcd.ko
    12:49 <twb> ...so that's OK, at least.
    12:50 <twb> Is netconsole.ko there, so I can at least get a copy of dmesg?  ...no, I'll have to patch that in.
    16:03 <twb> OK so to catch you up:  1. dracut with an ext4 fs Just Worked.  2. dracut with spl_hostid=0xFFFFFFFF failed, and gave me a recovery shell with a working terminal.  3. zfs-dracut is supposed to create /etc/hostid inside the initrd at build time, but isn't, and I can't see why.
    16:03 <twb> So the key problems appear to be with zfs-dracut specifically, not dracut in general

2019-05-02 #debian-next::

    13:00 <twb> OK so I am trying dracut (instead of initramfs-tools).  dracut fails to find the rootfs, and hangs.  Adding "rd.shell" gives me a debug shell prompt... but the USB keyboard doesn't work, so I'm still stuck.  ehci-hcd.ko is in the dracut ramdisk.  netconsole.ko isn't.  What can I try next to debug dracut booting?
    13:42 <twb> rd.shell rd.driver.post=ehci-hcd,hid,usbhid rd.break=pre-pivot isn't enough to let me type into the recovery shell... :-/
    16:08 <twb> FTR, 16:03 <twb> OK so to catch you up: 1. dracut with an ext4 fs Just Worked.  2. dracut with spl_hostid=0xFFFFFFFF failed, and gave me a recovery shell with a working terminal.  3. zfs-dracut is supposed to create /etc/hostid inside the initrd at build time, but isn't, and I can't see why. 16:03 <twb> So the key problems appear to be with zfs-dracut specifically, not dracut in general

2019-05-02 #systemd::

    11:47 <twb> You know how /var/log/journal doesn't get unmounted during shutdown?
    11:47 <twb> I vaguely remember something about switch_root back into the initrd, so that the rootfs can be torn down cleanly (including /var/log/journal), including any RAID or iSCSI or whatever is under the rootfs
    11:48 <twb> Where is that actually documented?  Is it maybe a Fedora-only or dracut-only thing?
    11:48 <twb> (I'm currently trying to bring up Debian via dracut, to just see what happens experimentally, but I'm running into some unrelated bugs.)
    14:34 <grawity> twb: the "shutdown initramfs"? I don't know about the docs, but if you have an executable at /run/initramfs/shutdown, then systemd-shutdown will pivot to /run/initramfs and run that instead of powering off
    14:34 <twb> Hrm, that at least gives me something to look for
    14:35 <grawity> twb: it is used on Arch with mkinitcpio, where it's a separate thing and not even part of the main initramfs
    14:35 <twb> That would also explain why dracut's ramdisks have a shitty bash script called /shutdown
    14:35 <twb> (even when /init is systemd :-/)
    14:35 <grawity> https://github.com/scaleway/initrd/issues/31
    14:45 <grawity> https://lists.debian.org/debian-kernel/2017/04/msg00079.html
    14:48 <twb> Oh that looks interesting
    14:49 <twb> I'm setting up a Debian 10 ZFS server, and I have been looking at dracut (so I get systemd in the ramdisk), mainly to help making setup/teardown work better.
    14:49 <twb> But dracut is being a huge pain, and that post looks like the only *important* part I'm missing for doing this on "the devil I know" (initramfs-tools)
    15:01 <pronoun> ooh
    15:01 <pronoun> i wonder if that could help with the various hung shutdown states where mountpoints are in use etc
    15:01 <pronoun> especially when lvm/luks/dm are involved
    15:03 <grawity> that's specifically what it is made for, so hopefully yes
    15:03 <pronoun> unmount.c btw was recently modified to use libmount and cleaned up to better support containers and unmounting of stuff no longer using /proc/self/mounts
    15:04 <pronoun> was looking at that last night...
    15:06 <twb> AFAIK this problem affects systemd more than pre-systemd because journald just *will not ever shut down*, so it holds open your rootfs
    15:06 <pronoun> still seemed to sigtem/kill itself before things fully unmonuted though =/ /proc /dev /sys were ignored but didnt see any handling for var log/journald let me find that issue
    15:06 <twb> Whereas on a pre-systemd system, rsyslogd or busybox syslogd is just a regular program and it's killed off early on, and doesn't auto start up again
    15:06 <grawity> well
    15:07 <grawity> systemd execs to systemd-shutdown during the regular process
    15:07 <grawity> and when systemd-shutdown SIGTERMs everything, it doesn't exclude journald
    15:07 <pronoun> systemd-shutdown[1]: Received SIGTERM from PID 1 (systemd-shutdown).
    15:07 <grawity> and at this point, the main service manager is not running anymore, so there are no .services that could be restarted
    15:07 <pronoun> including itself >_>
    15:07 <grawity> just systemd-shutdown in a tight kill() loop
    15:07 <twb> grawity: I'm 100% sure that on regular debian 9 installs, on shutdown I always get [FAIL] umount /var/log/journal
    15:08 <pronoun> yeah i get it too
    15:08 <grawity> yes, but that's an early part of the shutdown process
    15:08 <pronoun> well.. using sd initrd
    15:08 <twb> grawity: so you're saying that it's expected and harmless, despite the error?
    15:08 <pronoun> it causes people with write caching on lvm problems
    15:08 <pronoun> becusae somehow the code that is removing the dm doesn't happen
    15:08 <grawity> twb: probably annoying to see in the logs, but it doesn't really affect the kill/umount/kill/umount loop that systemd-shutdown does once main systemd stops
    15:09 <grawity> (it also tears down device-mapper stuff)
    15:10 <pronoun> https://github.com/systemd/systemd/issues/11821
    15:11 <pronoun> some links to related stuff there, and the commit mentioned... includes some changes to .service files for lvm and crypt
    15:12 <twb> I think the main thing I need is to "zpool export -a", which will fail while /var/log/journal has open fds
    15:13 <twb> Probably also / itself
    15:13 <pronoun> theres directives for services for requiring mounts and conditional path stuff-- idk about open fds though, probably just do a before shutdown
    15:14 <twb> So I guess I basically need a /run/initramfs that contains a copy of zpool (and necessary libraries), and a /run/initramfs/shutdown that does "zpool export -a" and whatever else it "normally" needs to do
    15:15 <twb> pronoun: I never worried too much about it because in principle everything should be mounted -o remount,ro by then
    15:15 <pronoun> yeah but thats fs level, who knows about the backing
    15:15 <pronoun> in case of lvm for instance or luks
    15:15 <twb> yeah
    15:15 <pronoun> https://github.com/systemd/systemd/commit/874d3404cbf2363604106c8f86683db4082691ea#diff-a55dd3f995e0ee0a2ba83f24f9492eebR381
    15:15 <grawity> and just to clarify it, the shutdown pivot happens as the very last step (in place of powering off), so it won't fix [FAIL]s that systemd-main shows
    15:15 <twb> I haven't noticed any actual data loss with a conventional md RAID1 + LVM + ext4 rootfs
    15:16 <twb> grawity: thanks; understood
    15:16 <pronoun> someone was complaining yesteday that lvm with caching got dirty
    15:16 <grawity> unless you can configure systemd to not umount /var/lib/journal *at all*, maybe? I vaguely remember something like that
    15:16 <twb> grawity: can I fix the fails?
    15:17 <twb> My (probably incorrect) assumption was that during shudown, systemd stopped journald and then tried to umount /var/log/journal, but by then something had tried to log something, so journald had auto-started again
    15:17 <pronoun> that issue they talk a bit about using conflict or after, another thing i was suggesting to the guy, no idea if zfs has support but old school write barriers
    15:17 <twb> To me, the "obvious" right thing is for journald to write its shutdown messages into /run/, just like it does for early boot
    15:18 <grawity> twb: I dunno, maybe if journald's .socket is stopped it ought to not re-activate the service
    15:18 <grawity> well
    15:18 <grawity> if journald writes to /run that won't help because journald itself is started from /
    15:18 <grawity> brb
    15:19 <twb> Well, it would get rid of *one* of the fails, at least ;-)
    15:19 <pronoun> seems like unmount should happen "inside root" and then again after pivoting back to initrd
    15:19 <pronoun> when journald is stopped
    15:21 <twb> pronoun: yeah there's two separate whinges here: 1. (important) data loss because no /run/initramfs/shutdown means complicated storage backends aren't torn down cleanly; and 2. (minor) you shouldn't get "FAIL" messages on working systems, because it's bloody scary
    15:22 <pronoun> they mention network fs, virtual fs, containers and stuff in the comments of unmonut.c
    15:22 <twb> I wonder if nobody has fixed this because "normal" users have plyouth splash and so don't see the errors
    15:22 <pronoun> but i swear systemd-shutdownd is killing itself before some of this code is run
    15:22 <grawity> "normal" users have /var/log/journal on rootfs, I guess
    15:23 <twb> grawity: that wouldn't help though, would it?  You'd just get [FAIL] Unmount /
    15:23 <twb> Unless maybe / is handled specially
    15:23 <pronoun> i think its complicated too, like when do you stop logging, what if there was errors during shutdown etc
    15:23 <grawity> nah, I have a feeling / is probably skipped because systemd-shutdown will remount it --ro later, can't really umount it
    15:23 <grawity> unless you have a "shutdown initramfs" of course
    15:23 <pronoun> one odd thing i read in comments was scenario where you loose route to your nfs
    15:24 <pronoun> they were saying unmounting was causing it to become dirty which sounded odd to me
    15:24 <grawity> dirty nfs share? now that's something I've never heard of
    15:25 <pronoun> they do handle / /proc /sys /dev /usr "mount_point_is_api" "mount_point_ignore" differently
    15:25 <pronoun> well the wording was networked fs, not nfs, but maybe it was iscsi.. but if the route is gone.. fail to see how anything could impact it =p
    15:26 <twb> pronoun: so maybe I can avoid the scary FAIL message by setting something like x.systemd-is-essential=yes for /var/log/journal
    15:27 <pronoun> /new_root should get unmounted in the rd after logging and all that and after pivot is what i think would be desirable
    15:27 <twb> (Although on ZFS, the fstab actually lives inside ZFS and I can't add x.foo things to it, so...)
    15:28 <pronoun> no idea I tried to solve this problem. It's complex lol
    15:28 <grawity> why not
    15:29 <pronoun> mkinitcpio.conf can copy files for ya, i think it makes a copy of your /etc/fstab and has some fstab generator as well
    15:29 <twb> grawity: I mean there is no actual text file called fstab.  Each ZFS dataset has "mountpoint" and "canmount" metadata attributes
    15:30 <pronoun> it'd be the initrd though that you'd want to edit? or if its internal metadata would just set normally?
    15:30 <pronoun> not understanding i guess
    15:33 <pronoun> there is a unmount.target btw
    15:35 <pronoun> twb https://lists.freedesktop.org/archives/systemd-devel/2015-January/027323.html
    15:36 <twb> Hrmm, so maybe what that means is I could do
    15:37 <twb>  /etc/systemd/system/var-log-journal.mount [Unit] Conflicts=
    15:37 <twb> ...and remove the conflict on umount.target
    15:37 <twb> This is pretty confusing
    15:37 <pronoun> might be able to leave that one but thats what i am kinda thinking, and yes very
    15:38 <pronoun> feel like an explicit sync would be nice
    15:38 <pronoun> .. somewhere
    15:39 <twb> I think that's a bit cargo-culty
    15:39 <pronoun> the sync?
    15:40 <twb> Depends if you mean fsync() on /var/log/journal, or more like the sync in REISUB
    15:40 <pronoun> thinking more the backings sync / flush
    15:40 <pronoun> (write caching)
    15:41 <twb> FWIW, umount(8) should implicitly block until that has finished
    15:41 <twb> Although not necessarily for block-level access, e.g. ntfs-3g probably wouldn't Just Work
    15:45 <pronoun> I think users of lvmcache though for instance, the fs unmounts / unblocks because the write cache has been cleaned or whatever fd closed etc
    15:45 <pronoun> but the writes may have not propigated all the way down to the pv
    15:45 <pronoun> but im not super informed about this stuff
    15:47 <pronoun> no idea if lvchange -a n or something would block / help =/ but i dont think the dirty shutdown is always safe
    15:47 <pronoun> https://bugs.centos.org/view.php?id=15729
    15:49 <pronoun> didn't help things lol
    15:50 <twb> are you allowed to lvchange -an a mounted LV?
    15:50 <pronoun>   Logical volume vg0/downloads contains a filesystem in use. apparently not
    15:51 <twb> thought not
    15:54 <pronoun> umount -RrlA; looks interseting.... Recursive... readonly.... lazy... All
    15:55 <pronoun> oh yea; this does neat things for sure. sudo umount -RlA /; haha
    15:56 <pronoun> going to shutdown for the night... i bet i dont get hung on var now though

2019-05-02 #zfsonlinux::

    13:34 <twb> AHA!
    13:34 <twb> I just noticed I've accidentally used zfs-dracut 0.7 not 0.8
    13:42 <twb> Fixing that didn't actually let me boot
    14:17 <CompanionCube> what is problem again?
    14:18 <twb> Debian 10 + zfs 0.8~rc4, / is a ZFS dataset.  With zfs-initramfs, I can boot.  With zfs-dracut, dracut just goes "nah I can't mount this".  I can get a shell, but I can't get a keyboard, so I haven't been able to debug it further.
    14:25 <CompanionCube> photo/screenshot of the mount errors?
    14:25 <twb> I don't have a camera or a BMC
    14:25 <twb> It looks to me like the zfs-dracut code isn't loading at all
    14:26 <twb> The actual error message is the generic one from dracut itself, one sec...
    14:27 <twb> die "Don't know how to handle 'root=$root'"
    14:28 <CompanionCube> so what's getting passed in as root= on both systems?
    14:28 <twb> root=zfs=omega/ROOT
    14:29 <twb> I've just prepped a plain ext4 rootfs to see if dracut can at least boot from that
    14:31 <twb> root=LABEL=FNORD works, where FNORD is an ext4 rootfs.  So apart from the lack of keyboard, the problem seems to be specific to dract + zfs root
    14:33 <twb> dracut doesn't include netconsole.ko either, so getting logs out is tricky.  lsinitrd definitely confirms that zfs support is getting compiled into the initrd, at least.
    14:33 [CompanionCube uses the bootfs method on desktop]
    14:34 <CompanionCube> though it's likely not the 'right' way :p
    14:34 <twb> I did try doing some things like "zpool set bootfs=omega/ROOT", but zpool said I wasn't allowed to set it on my pool
    14:34 <CompanionCube> huh?
    14:38 <twb> # zpool set bootfs=omega/ROOT omega
    14:38 <twb> cannot set property for 'omega': operation not supported on this type of pool
    14:39 <twb> That's trying to follow the rules from zfs.git:contrib/dracut/README.dracut.markdown
    14:40 <twb> I don't know *why* it's "not supported".
    14:42 <twb> The zfs-dracut docs aren't consistent in using root=zfs= or root=ZFS= or root=ZFS: but the source code looks like it'll try to handle any of them
    14:43 <twb> Hrm, it's possible the only problem is hostid isn't set correctly inside the ramdisk...
    14:44 <twb> Let me try explicitly setting spl_hostid=0xFFFFFFFF, since od /etc/hostid ==> 177777 177777
    14:45 <twb> (Why it doesn't just copy /etc/hostid in, I don't know)
    14:46 <CompanionCube> it *should* iirc
    14:47 <twb> Aha!  Awesome!  It still doesn't work correctly, but at least I have a working keyboard this time.
    14:47 <twb> I guess zfs-dracut bugs out in a way that breaks other parts of dracut, when there's no hostid
    14:49 <CompanionCube> https://github.com/zfsonlinux/zfs/blob/master/contrib/dracut/90zfs/module-setup.sh.in#L85
    14:54 <twb> yes, I see that
    14:54 <twb> I definitely don't have /etc/hostid inside the ramdisk, I'm not sure why
    14:55 <twb> spl_hostid=0xffffffff set the hostid to 00ffffff instead of ffffffff which is why I got a recovery shell --- it failed to import the pool, instead of failing BEFORE it even TRIED to import the pool
    14:58 <twb> OK, switched back into a normal boot, let's have a look
    15:01 <twb> "hostid" works where dracut runs
    15:03 <twb> I can trace it and see it's definitely doing + echo -ne '\xff\xff\xff\xff'
    15:05 <twb> I can also see that usr/lib/systemd/system/initrd.target.wants/zfs-import.target is getting made, right below the hostid code
    15:05 <twb> So... wtf
    15:52 <twb> How bad is it if my OS doesn't "zpool export" during shutdown?
    15:52 <twb> "zpool import" seems to be quite slow (like 5 seconds), but that might just be my crappy 5400 RPM disks
    15:52 <CompanionCube> twb: zpool export mostly just resets the hostid value
    15:52 <twb> OK
    15:53 <twb> If (say) my rootfs dataset is remounted -o ro, but is never unmounted, how bad is that?
    15:53 <CompanionCube> isn
    15:53 <CompanionCube> isn't it normal for the rootfs to never be unmounted
    15:53 <twb> well it's funny you should say that
    15:54 <twb> because "modern" linux has a second initrd that it switches to during shutdown, if you need to tear down the rootfs fully
    15:54 <twb> And I'm trying to work out if I need to backport that capability to avoid ZFS getting unclean shutdowns
    15:55 <CompanionCube> introduced by systemd, no? It's a decent idea if you want to have a go at it
    15:57 <CompanionCube> twb: https://github.com/zfsonlinux/zfs/blob/master/contrib/dracut/90zfs/export-zfs.sh.in
    16:06 <jtara> twb the pool will always be consistent so there won't be a consistency issue at least
    16:06 <twb> jtara: heh, cool
    16:07 <twb> CompanionCube: yeah that's (I think) part of the "shutdownrd" teardown that dracut implements, which I would basically be backporting to initramfs-tools (that, or fix my bugs with dracut)
    16:08 <jtara> if you somehow manage to do some async writes really late in the shutdown process and keep a txg commit from being issued before shutdown, you might manage to lose writes
    16:08 <jtara> but iirc a txg commit will always run before shutdown
    16:08 <jtara> and async writes are expendible anyway :)
    16:09 <twb> Is it normal for zpool import to take ~5 seconds?
    16:10 <jtara> that's kind of long unless it's doing a bunch of zil block replay
    16:11 <twb> All of the drives are relatively slow, because this is a test system.
    16:11 <jtara> maybe do some iostats to see if it's reading and writing a lot, or just sitting there stupidly with an occasional io until it's imported?
    16:11 <twb> jtara: you mean look at "zpool iostats 1" while the import is happening?
    16:11 <jtara> you may not see it show up in zpool iostat at the point in the process it's having issues
    16:11 <jtara> you may have to just use the system iostat, or blktrace, or something
    16:13 <twb> I don't *really* care unless it's indicative of a worser problem (such as, unclean shutdown -> lots of journal to replay)
    16:14 <jtara> if you see a lot of writes during the import, you can probably conclude that is happening
    16:15 <jtara> or there's some zdb command you can use but I forget what it is, tbh
    16:16 <MilkmanDan> Does unmounting all datasets modify any of the mount* properties?
    16:16 <MilkmanDan> I would expect not but I've never tested it.
    16:17 <jtara> I would hope that it would not

2019-05-06 #systemd::

    14:49 <twb> Sigh.  My network interfaces have re-enumerated themselves, from enp11s0 to enp7s0, so the network didn't come up at all.
    14:50 <twb> Isn't the whole point of not calling things "eth0" that this won't happen?
    14:50 <twb> I haven't physically reconfigured the host *at all*; it's had dozens of reboots today without the iface name changing
    14:55 <twb> After another reboot, it's back to enp11s0
    15:01 <mingdao> I call mine eth0 and wlan0 and *never* have such a problem.
    15:03 <twb> If I had >1 iface I would actually give them logical names like "upstream" and "dmz"
    15:03 <twb> Using /etc/udev/rules.d/, because last time I tried to do it via .link files, it Just Didn't Work, and I gave up
    15:03 <mingdao> twb: Just use net.ifnames=0 on your kernel command line and be done with this.

2019-05-06 #apparmor::

    15:05 <twb> Hey, on a Debian 10 install, why does "aa-status" say a bunch of stuff is in complain mode?  Is it just because the associated binaries don't exist?
    15:26 <jjohansen> twb: debian chose to ship most profiles in complain mode, think of it as an intermediate step, that lets them try things without breaking people.
    15:26 <jjohansen> users can put those profiles into enforce mode with aa-enforce
    15:28 <twb> Righto

2019-05-06 #ntpsec::

    18:42 <twb> If I'm upgrading from ISC ntpd to ntpsec, other than ntp.conf moving into a subdir, are there any gotchas?  Nothing is jumping out at me in quick.html.
    18:45 <twb> I really just need to grab NTP from au.pool.ntp.org (or an IP-capable GPS clock, when airgapped) and shove it out over the LAN where the systemd-timesyncd (SNTP) clients can see it.
    18:46 <twb> Looks like in Debian 10, it's even auto-feeding ntp.conf from DHCP, out-of-the-box.  Yay.
    19:35 <twb> no logarithmic scales on ntpviz? ;-)


2019-05-06 #samba::

    15:12 <twb> OK so I have an old server that was doing plain OpenLDAP + krbless NFSv3, with some ad-hoc scripts to let non-technical staff add and remove users.
    15:13 <twb> I plan to replace that with samba's AD, because, basically, AD has "won".
    15:15 <twb> Everything on the network is Linux, and I probably don't even need file sharing anymore.  I just need, like, PAM/NSS and apache to be able to auth off the AD accounts, and a recipe for adding and removing users and groups.
    15:16 <twb> Am I silly to do it this way?  Last time I tried to hand-roll slapd+heimdall+nfsv4 was a huge failure, but that was a LONG time ago.
    18:12 <twb> I have an existing split-horizon DNS, with a public hand-written zonefile, a bunch of DHCP client hostnames added via dnmasq, and a few copy-pasted into /etc/hosts so they're available during early boot (e.g. for firewall rules).  Would it make sense for my samba AD domain to be a subdomain?  I'm a bit nervous about samba being in charge of *all* DNS.
    18:17 <twb> Aha, https://wiki.samba.org/index.php/Active_Directory_Naming_FAQ
    18:24 <twb> How insane is it for my AD domain to be "invalid." ?  That's what I use in some other places for an internal-only domain that I can't be arsed actually buying to reserve it.
    18:26 <twb> Aha, I just got to the part of that FAQ where it says "yes, twb, if your normal domain is example.com., then a subdomain ad.example.com. is probably the Best Thing"
    18:31 <twb> Samba's DNS resolver is authoritative for the AD domain, and by default *also* recursive resolver for other domains.  How weird is it for me to turn that off, so that it'll only respond for the AD domain?  I prefer to have those jobs separate (like nsd3 + unbound, unlike bind), because when things go wrong, the errors are easier to understand.
    18:37 <twb> Oh, cool, Debian's samba is using slapd's LMDB by default.

2019-05-06 #zfsonlinux::

    17:53 <sveinse> if using a vdev for special storage, are there any ballpark figures or guesses at how big this needs to be in relation to the main pool? I know it is dependent of the type of data stored, but should one scale the size of the special vdev? Any suggestions?
    17:54 <fling> sveinse: you should probably try testing it for yourself.
    17:54 <fling> sveinse: just create a pool for testing from files (on tmpfs) if you don't have your drives yet.
    17:55 <twb> What is "special storage" ?
    17:55 <fling> sveinse: try putting some data to the pool like 2% then 4% etc of your data to meassure what is the used space ratio between the special and general vdevs.
    17:56 <fling> twb: metadata classes
    17:56 <fling> sveinse: then you will know the sizes…
    17:56 <sveinse> fling: sure, I will (once I get my old pool up and running again). This will be exact for the current data contents, but how does one generalize it into a guideline when the usage patterns might change?
    17:56 <twb> How do you actually do it?   I don't see anything relevant in the zpool manpage.
    17:57 <pink_mist> twb: which version?
    17:58 <twb> 0.7.12  (https://manpages.debian.org/testing/zfsutils-linux/zpool.8.en.html)
    17:58 <zfs-bot> [ zpool(8) — zfsutils-linux — Debian testing — Debian Manpages ] - manpages.debian.org
    17:58 <twb> Although I actually have 0.8~rc4 in front of me
    17:58 <sveinse> twb: it's a new feature of 0.8, where you can store the metadata of the pool in a separate vdev. I have a z2 pool with spin drives, and I plan to setup a mirrored nvme SSDs vdev for fast meta-data access
    17:58 <twb> I see it in 0.8 yeah
    17:58 <fling> sveinse: I don't think it is possible without knowing your needs. Imagine the difference between pool storing like 32M of hardlinks and a pool with 32TB of huge files and 16M recordsize
    17:59 <fling> sveinse: the metadata storage needs difference would be huge!!
    17:59 <sveinse> fling: yup, I was just curious if someone had a broad figure. Like 10% of the main pool or similar. But ok, thanks
    17:59 <twb> fling: so let's work out a "typical" case of being a mail server, which should be an overestimate for most other uses
    17:59 <fling> sveinse: also xattrs, small files on raidz, etc
    18:00 <twb> 486GiB data, 13326082 inodes /var/mail
    18:00 <twb> I suppose those numbers aren't enough to work it out...
    18:00 <sveinse> What happens if the special vdev is full? Does it revert to using the main pool for storage, or it then the entire pool considered full?
    18:00 <fling> sveinse: just have a single file and fill the whole pool with hardlinks. Then the data/metadata ratio will be exact filesize/(poolsize - filesize) :P
    18:01 <sveinse> twb: you need to use zdb to list how large the metadata is
    18:01 <twb> sveinse: you can't do some kind of rule-of-thumb estimate?
    18:01 <fling> sveinse: I've not tried it myself yet. I started using cheap 60G ssd as cache devices for my raidz2 pools and got 10x performance improvements. Then I started using lots of ram and I don't need any cache anymore.
    18:02 <twb> I'm thinking of basically mke2fs.conf's "inode ratio" settings
    18:02 <twb> fling: is that instead of, or in addition to, l2arc and/or slog ?
    18:03 <sveinse> twb: zdb -bbb pool apparently (never tried it myself yet)
    18:03 <fling> twb: I replaced l2arc devices with more ram
    18:03 <twb> fling: fair enough
    18:05 <sveinse> My understanding is that this special feature is designed to fix the performance issues of raidz, especially to metadata. Since all metadata is stored at least twice on each drive, this becomes replicated a lot across the entire pool. So metadata access is mediocre on raidz, and the special vdev is supposed to alleviate that.
    18:05 <twb> Is this "dedup allocation class" thing also new in 0.8?  Does that let you do dedup with a lot of SSD instead of a lot of RAM?
    18:05 <sveinse> But this is second hand info, I'm not into the details of this
    18:06 <twb> sveinse: logically only the metadata *writes* would be slow, and even then (surely?) not slower than regular blocks
    18:08 <pink_mist> twb: yes
    18:08 <twb> neato
    18:08 <pink_mist> at least I'm 87% sure that's what it does :P
    18:09 <pink_mist> so I guess I'm not certain
    18:10 <sveinse> twb: In addition, changing metadata from slow spin drives to fast SSD will also contribute tremendously to performance
    18:11 <sveinse> Thus I'm determined to this out on my home NAS and see if I get any noticable effects on the larger linux file trees
    18:12 <twb> sveinse: I think you actually mean the seek time
    18:14 <twb> Wow, that's weird.  These 8TB drives draw significantly more watts than 6TB or 10TB
    18:15 <sveinse> twb: yes...

2019-05-09 #cyber::

    21:20 <twb> https://github.com/blechschmidt/freebind  --- LD_PRELOAD wrapper to make any daemon use IP_FREEBIND (i.e. so it works better with systemd)
    21:22 <twb> The main use case I'm thinking of is new-omega is heading towards having *FOUR* dns servers on it
    21:22 <twb> All of which want :53
    21:22 <twb> (unbound, nsd3, samba, and systemd)

2019-05-09 #debian-au::

    19:53 <twb> pabs: so I was just trying apt-cacher-ng like you like
    19:54 <pabs> how did it go?
    19:54 <twb> pabs: it doesn't let me use https://deb.debian.org/ by default, because that requires TUNNEL which allows firewall bypass
    19:54 <pabs> hmm, I'm using that
    19:54 <twb> The other thing that confused me is I expected to add it to sources.list, not to apt.conf
    19:55 <twb> root@not-omega:~# apt-config dump | grep Proxy ===> Acquire::http::Proxy "http://localhost:3142/";
    19:55 <twb> Err:5 https://deb.debian.org/debian buster InRelease Invalid response from proxy: HTTP/1.0 403 CONNECT denied (ask the admin to allow HTTPS tunnels) [IP: 127.0.0.1 3142]
    19:55 [pabs uses squid-deb-proxy-client instead of manually configuring that]
    19:55 <twb> pabs: so you use squid *and* apt-cacher-ng at the same time? >_<
    19:55 <pabs> no, just apt-cacher-ng
    19:56 <pabs> squid-deb-proxy-client automatically looks up available proxies
    19:56 <pabs> Acquire::http::ProxyAutoDetect "/usr/share/squid-deb-proxy-client/apt-avahi-discover";
    19:56 <twb> ah so squid-deb-proxy-client isn't actually dependent on squid-deb-proxy
    19:56 <pabs> nope, bit of a misnamed package :)
    19:57 <pabs> it relies on an avahi lookup
    19:57 <pabs> see /etc/avahi/services/apt-cacher-ng.service
    19:57 <twb> So probably won't work inside chroots &c
    19:58 <pabs> no, see auto-apt-proxy for that
    19:58 <twb> it also means a rogue host on your switch segment can be a proxy that just says "no new updates to the debian-security repo", no?
    19:59 <twb> That's the main FUD I have abou mdns/dnssd
    20:00 <twb> Looks like auto-apt-proxy assumes the proxy is installed on either 127.1 or the default gateway (assuming you have exactly one default gateway)
    20:01 <twb> So it will do the Wrong Thing for me where I have a small gateway host and a big storage/cpu/ram host, and the caches live on the latter
    20:12 <twb> pabs: so *ANYWAY*, does that mean your apt-cacher-ng has the CONNECT backdoor enabled?
    20:12 <twb> pabs: it appears to be "PassThroughPattern: .*" in etc/apt-cacher-ng/zz_debconf.conf
    20:12 <pabs> thats commented-out here
    20:13 <twb> So how is it doing https
    20:14 <pabs> not sure...
    20:15 <twb> haha /usr/lib/apt-cacher-ng/expire-caller.pl is an ELF
    20:16 <themill> and that's why we don't use file extensions on binaries, boys and girls.
    20:16 <twb> themill: nah nah nah obviously that's the Polska version
    20:17 <twb> apt-cacher-ng also has a yukky thing where a cron job makes an HTTP request to apt-cacher-ng to say something like "hey do some cleanup now"
    20:17 <themill> heh
    20:18 <twb> Which is about as ugly as having * * * * * nobody wget http://localhost/cgi-bin/update-email.php
    20:20 <twb> "apt update" NOP is actually 50% slower with apt-cacher-ng enabled :-)
    20:24 <twb> Also weird: with a cold cache everywhere, "apt download firefox-esr" runs at 500kB/s with apt-cacher-ng, and at 1500kB/s without apt-cacher-ng
    20:26 <twb> One minor benefit of using a full mirror, instead of any kind of "smart" proxy, is that hosts with file:// access to the mirror don't bother copying the files into /var/cache/apt before unpacking them
    20:27 <twb> With a hot apt-cacher-ng cache, it downloads over loopback iface at around 10MB/s, which is "adequate", but not fantastic
    20:28 <twb> It should be able to sustain closer to 150MB/s with the 7200RPM disks alone
    20:29 <twb> Why is solr in debian so fucking old?
    20:30 <Fudge> poke the maintainer
    20:31 <twb> in debian it's 3.6 since 2013 or earlier; upstream it's version 8 or something
    20:34 <twb> Maybe this time I should just use dovecot-lucene instead of dovecot-solr, EVEN THOUGH dovecot discourages it
    20:35 <twb> When I talked to #dovecot last time about using solr 3 instead of solr 8, they noted that dovecot's solr backend hasn't been updated since 2013 either
    20:35 <Fudge> wow
    20:35 <Fudge> I liked dovecot when I used it
    20:36 <twb> "lthough the fts-lucene plugin works, it's using CLucene library, which is very old and has some bugs."
    20:36 <twb> So their actual criticism is that Lucene's C++ version is crapper than Lucene's Java version
    20:37 <twb> Oho, someone has updated the dovecot wiki to know about solr 7 now
    20:40 <twb> CLucene had a version in 2009, and a major new version in 2012, then nothing
    20:41 <twb> The obvious conclusion is that it's "finished" and works perfectly!
    20:46 <twb> Aha, #842706 (I was looking at the wrong bts page)
    20:48 <twb> Also relevant: LibreOffice 6.1 is using C Lucene, so while it's bad, it's a badness *everyone* is exposed to
    20:48 <twb> s/while/if/
    20:50 <twb> CLucene 2.3 claims feature parity with Lucene 2.3; latest is Lucene 8
    20:51 <twb> BLEHHHH
    20:55 <twb> Correction: Libreoffice 5 uses clucene to search its help manuals.  LibreOffice 6.1 has no help browser; you have to install a GUI browser to view LO help now.  LO 6.1 appears to only use CLucene "compile" a search database for the browser to use, similar to what Sphinx does.
    21:01 <twb> libclucene-core.so.1 is linked into a shitton of /usr/lib/libreoffice/program/*.so, so maybe they just link in things willy-nilly

    21:42 <twb> Poking around systemd-nspawn for the first time; debspawn tries to use eatmydata and fails
    21:43 <twb> Looks like that's just during early init though
    22:06 <pabs> hmm, I should try that, need something with better isolation than pbuilder
    22:06 <twb> Hey!
    22:07 <twb> I have /lib -> /usr/lib, even though usrmerge isn't installed!
    22:07 <twb> And so molly-guard was happy to be installed, even though maybe that's Super Bad, because it Conflicts: usrmerge?
    22:07 <pabs> debootstrap does that IIRC, not sure if by default or was by default and is now not default
    22:08 <twb> I assumed debootstrap did that *via* --include=usrmerge
    22:09 <twb> "Merged /usr has been the default since debootstrap 1.0.85, reverted in 1.0.87 and re-enabled in 1.0.102."
    22:09 <twb> It's super confusing that it's done by not marked by an installed package
    22:09 <pabs> current merged usr implementations are such an ugly hack
    22:10 <twb> It's turned off again in 1.0.111 per changelog
    22:10 <twb> #914208
    22:11 <twb> Ah it depends on --variant
    22:11 <twb> I don't hugely care which I get, but I *do* care about things breaking under me
    22:13 <twb> pabs: on ZFS, debspawn is using .tar.zst, which is dumb
    22:16 <twb> I saw systemd-containers pulling in btrfs-tools, but I can't see where it's used yet
    22:17 <twb> Wow, you can put something in fstab and systemd will run resize2fs on the block device every time it mounts it
    22:19 <twb> What I'm **ACTUALLY** trying to do is work out how to have one service, such as nsd3, to have its own netns, that *includes a public address*
    22:19 <twb> So that I can have e.g. unbound and nsd3 both just running, listening on 1.2.3.4:53 and 4.3.2.1:53, but neither can even see the other's iface
    22:19 <twb> (let alone accidentally hijack it)
    22:20 <twb> PrivateNetwork= is too narrow, and IPAddressDeny= is too loose.  systemd-nspawn doesn't seem to let me spin up *only* a netns; it needs a full chroot environment with a local initd and everything else
    22:33 <twb> https://github.com/systemd/systemd/issues/4272  (re "systemd likes btrfs")
    22:34 <twb> lennart's attitude is that "ZFS ain't mainline so we won't ever support it"

2019-05-13 #debian::

    17:19 <twb> "dpkg-reconfigure samba" doesn't prompt me for anything, but it says "Samba is not being run as an AD Domain Controller: Masking samba-ad-dc.service"
    17:19 <twb> How do I actually become an AD DC?  I can't see anything about this on wiki.debian.org
    17:20 <twb> Sigh, I see that's in TODO.Debian: "handle ad-dc stuff"  "have debconf question to configure ad dc"
    17:21 <twb> Looks like that stuff was lots when samba 4.x moved out of experimental and became "the" samba
    17:22 <twb> OK, the masking is happening in response to the content of smb.conf, so that part isn't too bad

2019-05-13 #samba::

    18:10 <twb> "samba-tool domain provision" doesn't understand diceware/xkcd-style passwords
    18:11 <twb> For example, it does not consider this password to meet "quality" standards:
    18:11 <twb> curse acclimate shock plank dubiously bullfight unworthy nibble railing dimmed backpedal coeditor
    18:13 <twb> Hrm, it considers "ℵ₀≡∞⁺" to be less than 7 characters, so it will probably annoy people trying to use Chinese passwords, too (because it's assuming an ASCII-centric amount of entropy per Unicode codepoint)
    18:17 <twb> hahaha, it won't even allow passwords generated via "xkcdpass|eleet", which look like "pr0b3 p05t3r d3b4t4bl3 w1ldf1r3 mu51c l3gg1ng5"
    18:19 <twb> The samba wiki links to a MS Technet article that says you're not even allowed to use only Chinese; you HAVE TO use european digits and/or letters, or it isn't "complex" enough.
    18:29 <twb> I wonder if the okina is considered "punctuation"
    18:29 <twb> Unicode correctly places it in Lm category, so hopefully not
    18:32 <haclusterer> did you consider spaces and quotes?
    18:32 [twb grumpily gives up and resorts to slapping a "A0" on the end of a diceware password.]
    18:33 <twb> haclusterer: spaces and quotes count towards the same category, according to that technet link
    18:33 <twb> haclusterer: so I'd need a majuscule or a west arabic digit as well
    18:34 <haclusterer> > pr0b3 p05t3r d3b4t4bl3 w1ldf1r3 mu51c l3gg1ng5
    18:34 <haclusterer> you said this didn't work, do you know why?
    18:34 <twb> Oh, actually you're right – it should have allowed that because latin minscules + west-arabic digits + spaces = 3 categories
    18:35 <twb> It didn't tell me why, it just said "Administrator password does not meet the default quality standards."
    18:35 <twb> I suppose I might have copy-pasted wrong or something
    18:35 <haclusterer> are you sure it didn't only take the first word because of missing quotes for example?
    18:36 <twb> I was typing it into the --interactive prompt
    18:36 <twb> Funny story; I once set up a password on a system that turned out to be using plain crypt(3)
    18:36 <haclusterer> what happens if you replace space with lets say underscore?
    18:37 <twb> So it had silently discarded all but the first 8 bytes of my password, both on setup and on subsequent auth!
    18:37 <haclusterer> that sounds... very bad
    18:37 <twb> It was extremely racist, yeah
    18:37 <twb> Because 8 bytes of UTF-8 is enough for like 1.8 chinese codepoints
    18:38 <haclusterer> even worse in your case i guess
    18:38 <twb> I'd do some more tests for you, but it was in the "samba-tool domain provision" prompt, and I'm scared to rerun it a couple of times
    18:39 <haclusterer> never touch a running system^^
    18:39 <twb> haclusterer: yeah, the way I discovered it was the password was along the lines of "let me in right now you bloody daft bugger", and changing the swear words still let me in
    18:39 <twb> it's not a prod server, I just can't be arsed tearing down the domain again
    18:40 <twb> Can I use o=Frobozz insteaad of dc=frobozz,dc=com,dc=au as my LDAP root?
    18:40 <haclusterer> not that I am aware
    18:41 <twb> Okey dokey
    18:43 <twb> On plain slapd I also tend to not have *any* root bind pw, and instead just use -Y EXTERNAL.  That is, if I have root (uid=0 + gid=0) on the LDAP server, then I can bind as root.  Otherwise, I can't.
    18:43 <twb> The obvious reason being, if I'm root on the server, I can just stop ldap (or smbd) and edit the files directly using my cap_sys_admin powers
    18:44 <twb> Can I do the same thing on samba ad?  That way, there is no way anyone can EVER guess the root pind pw, because there simply isn't one.
    18:50 <twb> Does Samba AD's built-in LDAP server support the sudo schema?
    18:59 <twb> https://www.sudo.ws/readme_ldap.html  (the instructions for OpenLDAP are pre-cn=config)
    19:02 <twb> Grmph, Debian's samba package strongly doesn't want me to be running AD *and* smbd/nmbd on the same host.  I guess if I want file sharing, I'll have to spin up a systemd container or some crap
    19:06 <twb> After "samba-tool domain provision", "dpkg-reconfigure samba" (2:4.9.5+dfsg-3, Debian 10) masked out nmbd and smbd, but didn't *unmask* samba-ad-dc.service before trying to start it
    19:07 <twb> If I unmask it by hand, it cracks the shits because it wants to bind to *:53 and systemd is already listenong in 127.53:53  ☹
    19:09 <twb> That's second bit is my fault, though.  I want to end up with samba's DNS being authoritative-only (not recursive), and only binding on one address.  My caching recursive resolver will then be told to talk to samba DNS for the samba domain, and not for anything else.
    19:10 <twb> (That way I can debug DNS issues with one or the other in isolation, and I can have different security profiles for recursive vs. authoritative)
    19:23 <haclusterer> dude... on clue
    19:23 <haclusterer> *no
    19:23 <twb> no worries
    19:24 <twb> It's nearly my bedtime anyways
    19:25 <twb> Hrmph, after stopping & disabling resolved, samba-ad-dc still won't be my friend
    19:25 <twb> http://ix.io/1IUg
    19:26 <twb> resolv.conf is still pointing upstream, not at samba's DNS.  resolvconf isn't installed.  I guess the DHCP client did that, because I haven't patched it not to yet.
    19:27 <twb> Gr, samba is logging to flat files instead of syslog by default
    19:28 <twb> This looks pretty obviously bad:   /usr/sbin/winbindd: Failed to exec child - No such file or directory
    19:28 <twb> "apt install windbind" and have another go.
    19:29 <twb> Slightly grumpy that we *still* need lanman crap
    19:29 <twb> OK, samba-ad-dc.service started!
    19:29 <twb> "ss -ntlpad | cat" can see it binding to a heap of stuff
    19:31 <twb> ldapsearch can connect, but can't do anonymous queries (which is expected).
    19:32 <twb> "ldapsearch -xLLL -H ldapi:///var/lib/samba/private/ldapi" gives a funny error when it tries to guess the krb realm for that URL
    19:35 <twb> smbclient -L is working
    19:39 <twb> SRV queries to samba's DNS are working (e.g. kdig @127.1 srv _ldap._tcp.ad.cyber.com.au)
    19:43 <twb> Trying to kinit with the klibc-utils implementation isn't working, so I guess I'll install libpam-krb5 and krb5-user...
    19:44 <twb> wiki.samba.org hasn't heard of ntpsec yet :-(
    19:47 <twb> OK bedtime, thanks for listening

2019-05-14 #systemd::

    16:17 <twb> OK so suppose I have a legacy PHP/Maria app that I want to jail, so that when it gets pwned, the attacker can't break anything else.
    16:17 [b1tninja likes twb's ventures with systemd]
    16:17 <twb> Plan B is qemu with kvm and virtio turned on.  Plan A is to use systemd's container/machined/nspawn stuff, similar to the old OpenVZ
    16:18 <twb> My key goal here is that -- assuming (pretending) the attacker can't pwn the shared kernel --- there is no way for the pwned PHP user to mess with anything running outside the container.
    16:18 <b1tninja> was going to suggest nspawn. I swear I recall some sort of inetdish functionality, use case was isolating ssh sessions
    16:19 <b1tninja> let me see if I can dig that u
    16:19 <twb> That includes things like "use up all the disk I/O so there is none left for ntpd"
    16:19 <grawity> b1tninja: that's still just regular nspawn that's autostarted, isn't it
    16:19 <twb> So my initial question is: what's the "starting point" manpage/doc that I should be reading?
    16:19 <b1tninja> think it was yes
    16:20 <grawity> I think there was an associated blog post about `systemd-analyze security <unit>`
    16:20 <twb> ssh to regular users get their own slices by default, as long as you enable PAM in sshd
    16:21 <twb> grawity: my systemd doesn't seem to have "systemd-analyze security" :-(
    16:21 <twb> Oops.  typo.
    16:22 <b1tninja> http://0pointer.de/blog/projects/socket-activated-containers.html wasnt the particular article but eh
    16:22 <twb> grawity: cool, that analyze security thing is SUPER HANDY.
    16:22 <twb> I tested it on systemd-timesyncd.service, since that's pretty anal by default
    16:22 <grawity> https://a.uguu.se/HBbQzlmC5kET.png most of my system is unhappy :(
    16:22 <twb> By comparison, ntpsec.service is mostly BALLOT CROSSes
    16:23 <grawity> well, they trust their own code that much
    16:23 <twb> grawity: rather, it just means that whoever wrote their unit didn't know about all the "bonus" lockdowns that systemd can add
    16:23 <twb> just like they don't have an apparmor profile yet
    16:24 <twb> How come systemd-timesyncd doesn't have PrivateUsers=
    16:25 <grawity> probably related to it having had DynamicUsers earlier, which Did Not Work Out
    16:25 <b1tninja> does user locale maybe come into play too
    16:26 <grawity> maybe it could have PrivateUsers= now that the latter has been disabled
    16:26 <grawity> b1tninja: the system clock is not dependent on user locale
    16:26 <twb> do you mean PrivateUsers + DynamicUsers didn't work out?  Or that the very idea of DynamicUsers was dumb?
    16:26 <grawity> the latter
    16:26 <twb> haha
    16:26 <grawity> well, it's just that *I* think the very idea of DynamicUsers was dumb
    16:26 <twb> I agree
    16:26 <b1tninja> woah https://www.freedesktop.org/software/systemd/man/systemd.exec.html#SystemCallFilter= is cool
    16:26 <twb> At least, I think it is a lot of risk for little benefit
    16:26 <grawity> but for timesyncd, it was more about a core systemd service / an early boot service
    16:27 <grawity> something about accidental loops, dependency on dbus, etc
    16:27 <b1tninja> dnssec blocking timesync when hostnames require it is an annoyance I have currently
    16:27 <grawity> and well, core systemd components just literally don't need a dynamic uid, so it was disabled
    16:28 <grawity> ah yeah, the ntp <=> dnssec problem isn't limited to systemd, you can get it pretty much everywhere
    16:28 <b1tninja> no chance someone knows how to downgrade dnssec when out of sync for timesync or something
    16:28 <grawity> openbsd has a time-from-TLS thing, google has Roughtime
    16:28 <b1tninja> presumably resolved has a way to allow dnssec for bootstrapping/resolving/verifying cert chains
    16:28 <twb> → Overall exposure level for gpg-agent.service: 9.7 UNSAFE 😨
    16:29 <grawity> twb: most of those options don't really work in per-user units, since AFAIK they're privileged operations
    16:29 <twb> Fair point
    16:29 <twb> systemd-analyze security $(systemctl list-units -t service | awk '/\.service/{print $1}') | grep Overall | sort -t: -nk2
    16:29 <twb> Everything except systemd-* is 8.3 or higher
    16:30 <grawity> (besides, gpg-agent is a secrets holder, it doesn't need to be contained from escaping but the other way around)
    16:30 <twb> I'm curious if more pro-systemd distros like fedora and arch have stronger default there
    16:30 <twb> grawity: contain ALL THE THINGS!  >rawr!<
    16:30 <grawity> lol just run `systemd-analyze security | awk '{print $2}'`
    16:30 <twb> Debian's package checker (lintian) is smart enough to complain if you provide a unit with no lockdown options
    16:30 <twb> grawity: oh thanks.
    16:31 <twb> grawity: I tried running it without arguments early on, but messed up
    16:31 <grawity> arch does not modify upstream units unless it just doesn't work otherwise
    16:34 <b1tninja> kvm would potentially seperate / isolate memory at a hw level in some cases, but more mgmt and resource usage, and attack surface i guess-- but usermode networking might be neat
    16:35 <b1tninja> id still opt for nspawn though
    17:07 <Exec1N> Hi people. I'm trying to set up a timer to run every 48h but I can't make it work. OnCalendar=0/48:00:00 doesn't work and OnCalendar=48h doesn't either
    17:07 <Exec1N> it always gives me this error Failed to parse calendar specification, ignoring: 48h
    17:07 <Exec1N> how can I properly do this?
    17:09 <b1tninja> 48h is 2 days
    17:10 <twb> Exec1N: does daily/2 work?
    17:11 <Exec1N> b1tninja: I know. twb: checking now
    17:11 <twb> Note that OnCalendar= expects a calendrical value; if you want "every 48 hours from whenever you first started", that's something like OnActiveSec=
    17:13 <twb> You might be able to say something like OnCalendar=*-*-*/2 0:0:0, to run on every odd day, but that will have unexpected effects on month edges
    17:13 <twb> OnCalendar=Mon,Wed,Fri,Sun would likewise have edges
    17:14 <Exec1N> Failed to parse timer value, ignoring: daily/2
    17:14 <Exec1N> can I have seconds with step?
    17:16 <twb> Exec1N: havey you looked at man systemd.time yet?
    17:16 <Exec1N> yes
    17:17 <Exec1N> but since 48h didn't work I was trying to get it working differently
    17:18 <twb> I can't see a way to express what you want via OnCalendar
    17:18 <b1tninja> damn 7 day weeks
    17:18 <b1tninja> and non consistent days per month
    17:18 <Exec1N> OnActiveSec also works for me as well
    17:18 <b1tninja> perhaps could be done by calendar date vs day of month
    17:18 <twb> The way cron solves this is simply
    17:18 <twb> run always, then noop if it's the wrong value
    17:19 <twb> you always have that as a plan B
    17:19 <twb> e.g.
    17:19 <twb> Description=[Timer] "24 0 8-14 * * root [ $(date +\%%w) -eq 0 ] && [ -x /usr/lib/zfs-linux/scrub ] && /usr/lib/zfs-linux/scrub"
    17:19 <twb> OnCalendar=*-*-8,9,10,11,12,13,14 0:24:00
    17:20 <twb> (And then the date = 0 check happens inside the ExecStart sh script)
    17:23 <twb> OK so re my own questions, it looks like full containers that systemd "knows about" have their rootfs at /var/lib/machines/frobozz and you enable machines.target and systemd-nspawn@frobozz.service
    17:24 <b1tninja> you could maybe bind ro parts of your system
    17:24 <b1tninja> to avoid second install
    17:24 <twb> and to customize the start options you edit /etc/systemd/nspawn/frobozz.nspawn
    17:24 <twb> b1tninja: I tried that earlier and systemd-nspawn got really mad at me
    17:24 <twb> b1tninja: it was all like "no you can't have / as your rootfs!"
    17:25 <b1tninja> hm intersting
    17:25 <b1tninja> maybe bind specific subs
    17:25 <Exec1N> ok raw number of seconds worked for me
    17:25 <twb> b1tninja: maybe if I do more complicated binding than just "whatever is on / already"
    17:25 <b1tninja> i wonder if the utility path starts function checks empty string against empty string in that case
    17:26 <twb> b1tninja: one of my straightforward goals is to have a host with one NIC, four static IP addresses, and systemd-resolved, nsd3, unbound, and samba DNS servers each binding to :53 on *one* of those IP addresses, and ideally not even able to see the other addresses
    17:26 <b1tninja> wonder if you could have a refrence set like binds in your fstab under var lib machines
    17:26 <b1tninja> and use that
    17:26 <twb> b1tninja: PrivateXXX= are de-facto a list of bind mounts
    17:27 <twb> b1tninja: for .exec units; I'm not sure about .nspawn stuff
    17:27 <b1tninja> the localhost in ns is the one thing that was kinda a gotcha
    17:27 <twb> b1tninja: apt install libnss-myhostname
    17:27 <b1tninja> re net ns
    17:27 <b1tninja> lo
    17:27 <twb> b1tninja: that makes localhost always resolve, without any /etc/hosts
    17:27 <twb> it's part of the systemd codebase
    17:28 <b1tninja> was thinking for the container-host interactions
    17:28 <b1tninja> if you were going to redirect/nat etc
    17:31 <twb> b1tninja: the other annoying thing is that even though btrfs has lost and zfs has won, systemd nspawn/machinectl only have Bonus Magic for btrfs.  On the RFE, Lennart said it was because systemd will not support things that aren't in the mainline linux kernel.
    17:32 <b1tninja> I have personal affliction with btrfs
    17:32 <b1tninja> really like the nspawn unionfs stuff
    17:32 <twb> I run both
    17:33 <b1tninja> i had something to mention re the memory stuff but i totally forget
    17:38 <LordDoskias> hello does libudev use some type of caching or it always queries /dev?
    17:38 <twb> are systemd nspawn containers typically "full" containers, of the sort that debootstrap makes?  Or are they more like "just copy /usr/bin/httpd to /init and install the 10 libfoo.so files it needs, and NOTHING ELSE"
    17:39 <twb> It seems like both kinds are more-or-less supported, but actually creating them is "out of scope" for the nspawn docs
    17:39 <boucman> twb: they are meant to launch systemd, but they work well with light containers too
    17:39 <boucman> yes creating them is out of scope
    17:39 <boucman> (I use buildroot for that)
    17:40 <b1tninja> a generator for such a thing would be neat
    17:40 <b1tninja> you've seen machinectl i assume
    17:40 <boucman> buildroot is a generator for that, so is  yocto, so is debootstrap....
    17:41 <boucman> (well, for the light version I 'm not sure debootstrap could do it)
    17:41 <twb> debootstrap creates a relatively heavyweight container, though
    17:41 <twb> It effectively ALWAYS requires apt inside the rootfs
    17:41 <boucman> we are writing the necessary bits at my company to have BR generate portable containers right now
    17:42 <twb> I think what I want is closer to how dracut and initramfs-tools build their ramdisks
    17:42 <twb> Or not necessarily *want*, but at least asking about
    17:43 <boucman> I won't blame you for not knowing what you want, the terms in the container space are a mess
    17:43 <b1tninja> arch had a "mkarchroot"
    17:44 <twb> My background is with hand-made lxc containers, libvirt or hand-made qemu kvm VMs, and hand-made squashfs live images
    17:44 <boucman> I don't think any non-embedded distro have tools that can reliably make light containers... honestly I only know BR and docker that can do that, and docker's way is a tracability nightmare
    17:44 <twb> I've avoided all the docker/rkt stuff because they don't seem to really have a handle on security yet
    17:45 <b1tninja> i avoid it too, waiting for the container war to settle down
    17:45 <twb> boucman: OK, I'll start with the one for making debian ramdisks that never switch_root ;-)
    17:45 <boucman> honestry, try buildroot, it's really trivial to understand, and it works wonders
    17:45 <b1tninja> this dudes tool was kind of neat https://github.com/tlahdekorpi/archivegen
    17:45 <twb> boucman: isn't that the one you wrote yourself? ;-)
    17:45 <boucman> huh ? no
    17:45 <boucman> I never wrote any tool like that
    17:45 <twb> Never mind, I must be thining of someone else.
    17:46 <boucman> it's a classic, well established tool in the embedded world, it's been around for a decade or so IIRC
    17:46 <twb> Oh, THAT one
    17:47 <twb> I've used the OpenWRT equivalent before
    17:47 <boucman> yes, but openwrt is very specialized, it really works well for routers only... I don't think it's the right tool for light containers
    17:48 <twb> granted
    17:49 <twb> Found it: https://packages.debian.org/buster/debirf
    17:51 <b1tninja> something that maybe looked for executables under a path and grabbed shared libs would be nice
    17:51 <twb> b1tninja: dracut does that :-)
    17:51 <b1tninja> nice
    17:52 <twb> The way dracut and (older) initramfs-tools basically work is you say "I need bash inside the rd" and it goes "OK, copy /bin/bash, and also anything mentioned in ldd /bin/bash"
    17:52 <twb> Then if your bash has a security update, you just rebuild the ramdisk
    17:52 <boucman> that gives you no tracability on what's on your system... I don't know your use-case, but I usually need to know exactly what's on my image for license/security reasons...
    17:53 <twb> boucman: in my use case, the host OS and the container will be fed from the same repo, so I don't care
    17:53 <boucman> ok
    17:53 <twb> boucman: like, I'm not building a static "container image" and then carrying it off to another place to run it
    17:53 <twb> It's more like dracut's "use whatever the host already has"
    17:54 <twb> At least, that's ONE idea I'm CONSIDERING.
    17:54 <b1tninja> ima have to play around with it more but i want a bind ro'd php env now =p so just update the system, but guess maybe legacy apps might want an old version
    17:54 <twb> b1tninja: I have definitely done that 10 years ago with lxc
    17:54 <b1tninja> wonder if there is like a follow symlinks dealio
    17:54 <b1tninja> "flatten symlinks" etc
    17:54 <twb> b1tninja: a straightforward case was something like "run dhclient with a netns but the same rootfs, then see if it gets a different IP address"
    17:55 <b1tninja> client identifier ;p
    17:57 <b1tninja> (systemd-firstboot --setup-machine-id) etc
    17:57 <twb> yeah, these days you'd need to worry about all that crap
    17:57 <b1tninja> wish they hadn't made client id the default >_>
    17:58 <twb> Huh, debirf knows about systemd-networkd, so it can't be as old as I thought
    17:59 <b1tninja> you saw --overlay and --overlay-ro
    18:00 <twb> not yet
    18:08 /join #archlinux
    18:15 <killermoehre> twb: also looked at --volatile?
    18:16 <twb> killermoehre: aha!
    18:16 <twb> killermoehre: I have seen that before, long ago, and forgotten about it
    18:17 <twb> Ah, I would have been looking at that in the context of live CDs, as an alternative to live-boot-initramfs-tools's union mounts
    18:17 <b1tninja> and maybe shutdown /var =p
    18:25 <twb> hahaha --kill-signal reminds me how every init has its own preferred signal for "let's turn off now"
    18:26 <b1tninja> shutdown kills itself >_>

2019-05-14 #debian-next::

    14:46 <twb> On a fresh Debian 10 install with gnupg2 installed, "systemctl --user -t socket" shows that gpg-agent-ssh.socket is up and listening
    14:46 <twb> but "ssh-add -l" can't see any agent running
    14:46 <twb> What magical secret step am I missing?
    15:01 <dwfreed> twb: you still have to set SSH_AUTH_SOCK, as the socket unit doesn't do that for you
    15:01 <twb> ohhh
    15:01 <dwfreed> https://wiki.archlinux.org/index.php/GnuPG#SSH_agent
    15:02 <twb> isn't there some environment crap in systemd now to auto-do-that
    15:02 <dwfreed> possibly
    16:05 <twb> dwfreed: oh btw, libpam_env.so appears to be on by default, reading from the empty-by-default /etc/environment.  So in principle can just drop SSH_AUTH_SOCK in there
    16:05 <twb> (I was thinking I'd have to either do the new systemd thing or the off-by-default ~/.ssh/environment thing)
    16:08 <dwfreed> twb: it should also read ~/.pam_environment
    16:08 <twb> Righto
    16:09 <twb> I think last time I looked, pam_env.so didn't support foo=$bar, so I just kept using .profile

    18:00 <twb> What's the equivalent of molly-guard on a system with usrmerge?
    18:06 <twb> Looking for package descriptions with things like "accident" doesn't find much
    18:12 <dwfreed> just use an alias?
    18:13 <twb> dwfreed: won't affect e.g. "sudo poweroff"
    18:13 <pabs> does molly-guard not work on usrmerge systems?
    18:13 <twb> pabs: Conflicts:
    18:14 <twb> pabs: IIRC because the way it actually works is by putting itself first in $PATH
    18:14 <twb> Can't do that if /usr and / are the same, because there is no first-er place
    18:14 <dwfreed> you could make your own root path in sudoers
    18:15 <pabs> sounds like it should be using diversions instead of PATH trickery
    18:15 <dwfreed> and put /usr/local/sbin first :)
    18:15 <twb> dwfreed: then it won't work if they ssh straight to root, or if they do something like "timeout 10m poweroff"
    18:15 <twb> dwfreed: using /usr/local would probably work (except for dropbear SSH); the only reason molly-guard can't do that is because it's a .deb and isn't allowed to touch /usr/local
    18:15 <dwfreed> twb: at that point there's not much you can do
    18:16 <dwfreed> twb: one could also /sbin/poweroff
    18:17 <dwfreed> molly-guard can always be bypassed (even more easily on systemd); you just have to decide what routes are acceptable to ignore
    18:18 <twb> dwfreed: yeah the goal isn't to stop DELIBERATE bypass, but merely ACCIDENTAL bypass
    18:19 <twb> sshing in as root and sshing in as nobody + sudo are both pretty common
    18:19 <dwfreed> most accidental bypass isn't going to be using timeout :P
    18:21 <dwfreed> yes, but you can do profile and sudoers changes to get /usr/local/sbin first in PATH and put a molly-guard equivalent there
    18:22 <twb> I suppose
    18:22 <twb> no turnkey thing for it tho
    18:22 <twb> ("so write one yourself, twb")

2019-05-14 #zfsonlinux::

    12:36 <twb> What do you guys use instead of bonnie++ for a generic synthetic I/O benchmark
    12:37 <dirtycajun> oh look at that i can run strace on the inside process outside the container. nifty
    12:37 <dirtycajun> twb i use fio
    12:37 <twb> Aha!
    12:37 <twb> My brain kept saying "bio" which I knew was wrong
    12:37 <twb> Huh, fio even has a bundled GUI now
    12:37 <dirtycajun> jtara this is a short snippit from strace of rtorrent : https://bin.cajun.pro/raw/ahepugedew
    12:40 <twb> Grmph, I can't see how to tell gfio "run ssh otherhost fio"
    12:40 <dirtycajun> ive never used the gui so i have no idea there
    12:41 <twb> I figured it would let me be lazy and not bother remembering how to drive gnuplot
    12:42 <dirtycajun> ithe pattern is basically socket receive... socket receive... write. rinse repeat
    12:43 <twb> I think gfio might work like xdu; you just pipe fio output into it
    12:44 <twb> I also discovered that hdparm has a benchmark option (-t / -T)
    12:45 <dirtycajun> yeah. not as safe to use tho
    12:45 <twb> NOW they tell me!
    12:45 <twb> 153MB/s sounds about right for these drives
    12:53 <twb> a *ha*.  gfio doesn't take *any* arguments; you have to instead use File > Open to open a .fio (job description), and *then* it prompts you for a host to SSH into
    12:56 <twb> So you don't do SSH at all, you ssh to your server and do "fio --server", which binds to 0.0.0.0:8765, then you open gfio on your craptop and file>open a .fio and then give it the server address
    12:57 <twb> Then in the bottom-right you click "Connect" then "Send"  and it starts actually doing things
    12:57 <twb> oh, and "Start Job"
    12:58 <twb> Woo!
    13:02 <gyakovlev> twb: found it! https://www.hamiltonshells.ca/zfs-iso/experimental/ <- with encryption support.
    13:02 <zfs-bot> [ Index of /zfs-iso/experimental ] - www.hamiltonshells.ca
    13:04 <twb> gyakovlev: cool
    13:06 <twb> Now my only problem is I don't know offhand what a "herp derp I'm a small business server" is called in examples/\*.fio
    13:12 <dirtycajun> most of the tests are specific
    13:13 <twb> Yeah I looked for something like "smb" or "smtp"; I'm poking through the docs now
    13:15 <twb> Looks like the oldest examples are "1mbs_clients", "aio-read", "tiobench-example".

2019-05-14 #gitolite::

    14:35 <twb> Right now I just have a bunch of bare repos /srv/vcs/myproject.git/ that everyone has group write access to via SSH.  The only real annoyance is the umask is usually wrong, which I kludge with something like "@hourly chmod -r g+w /srv/vcs/\*.git".
    14:35 <twb> I'm looking at gitolite to avoid the crappy cron job.
    14:36 <twb> But!  I have this one repo that isn't bare - the backend for my gitit wiki.  Can gitolite cope with non-bare repos?
    16:47 <sitaram> twb: gitolite ignores non-bare repos
    16:47 <sitaram> (as far as I remember)
    16:48 <sitaram> it generally does not make sense to have access control and multiple people pushing to a non-bare repo anyway
    16:49 <twb> Yeah agreed
    16:50 <twb> I only do it because gitit is stupid and doesn't (or didn't) support bare repos
    16:54 <twb> If you know of a different wiki that can do git + reST, with both git-based edits and web-based edits, I'll switch.  AFAIK the only other option is ikiwiki, which is mostly broken in reST mode.

    19:31 <Habbie> twb, github appears to run a fork of gollum for the wikis - i don't know if that is helpful
    19:32 [twb looks]
    19:35 <Habbie> markdown, rdoc
    19:35 <Habbie> oh, rest plugin
    19:35 <twb> yeah gollum is ruby so it forks out to python2 docutils for the rest
    19:36 <twb> (for comparison, gitit is basically pandoc, so it's all in-haskell, using a competing rest implementation)
    19:36 <twb> I could *almost* just convinced $work to not allow browser-based edits anymore, and then just use sphinx + a commit hook.  Almost.
    19:54 <twb> https://dataswamp.org/~solene/2016-05-17-9.html  is someone else having the same problem with gitit

2019-05-14 #samba::

    12:17 <twb> When samba AD stores passwords, does it use salted SHA-256, or what?
    12:17 <twb> When I looked a long time ago, it was storing them in something weakass like unsalted MD5, for backcompat with NT4

    12:21 <twb> OK so ldbsearch is basically samba's slapcat
    12:23 <twb> There's no cn=config in there.  I can see the top object.  Can't see any operational attributes / rootbindpw crap.  Maybe that stuff lives in kerberos and not LDAP at all
    12:27 <twb> Derp, I was doing -s base
    12:30 <twb> Oh wow, --sorted!

    13:28 <twb> Does samba have a recommended/preferred AD client side for GNU/Linux systems?
    13:29 <twb> Like, if I set up samba (on linux) as the ad dc, do I make AD accounts visible to the linux host via pam/nss-ldap + krb, nss-winbind + ?, sssd, or what?
    13:30 <Zombie> twb, sssd is better.
    13:31 <twb> Zombie: how so?
    13:31 <Zombie> sssd has an actual module, just for Samba 4 AD and manages KRB5 with PAM More reliably.
    13:32 <twb> k
    13:36 <twb> Weird.  Arch's wiki suggests winbind for AD, but SSSD for plain LDAP.
    13:37 <Zombie> winbind is terribly incosistent.
    13:37 <twb> I remember fighting winbind in the past
    13:37 <twb> samba-ad-dc.service wouldn't even start until I installed winbind, though, so I've got the server side on the AD server :-(
    13:43 <twb> I'm starting to wonder if I should just give up and not bother with samba at all.  Really all I want is the kerberization; my existing stack is just plain TLS'd slapd with ppolicy.
    13:43 <Zombie> Explain what you want/
    13:44 <twb> I guess what I was really going for was less minimalism and more "use standard stuff the standard way"
    13:45 <twb> So that later on bolting on some random new thing is easier
    13:48 <twb> but that means there's a lot of learning up-front and all the windowsy bits are yukky
    13:55 <Zombie> Samba 4 can efffectively replace OpenLDAP.
    13:55 <Zombie> and Heimdal Kerberos.
    13:55 <Zombie> If you are willing to learn their idiosyncracies.
    13:56 <twb> I realize that, but I already know how to drive plain openldap and samba ad is (relatively) huge
    13:57 <twb> I was also doing (unkerberized) NFS, but I'm 99% sure I can avoid having any network filesystems in the new setup (except for HTTPS and SFTP)
    13:57 <Zombie> Heimdal Kerberos has an issue that often pushes me over the edge.
    13:57 <twb> I never got mit/heimdal krb working.  I've done a samba 4 ad dc before, but it was a pain
    13:58 <Zombie> it often doesn't survive a reboot.
    13:58 <Zombie> Samba 4 is more predictable.
    14:17 <twb> I should also say that part of my YAGNI feeling is because this channel seems pretty dead, so I'm all like "maybe nobody even uses AD/SMB anymore, because everything is a phone and uses 1drv or gdrive"

2019-05-14 #mailman::

    14:07 <twb> Can I run mailman without any passwords at all, so that all actions have to be done by a privileged user on the mailman server itself?

2019-05-15 #zfsonlinux::

    16:23 <twb> OK, I have a stupid problem.  I have a "set up tree at /path/to/foo/" program.  I want /path/to/foo/ to be a separate dataset.  If I "zfs create" first, the setup program complains because foo/ exists.  If I run the setup program first, /path/to/foo is (initially) on the wrong dataset, and any snapshots made before I move it will be annoying.
    16:23 <twb> Is there a standard answer to this class of problem?
    16:24 <twb> I suppose a "90% right" answer would be to tell setup to run at /path/to/foo/foo (so it's always on the right dataset), then "mv * .." to remove the indirection
    16:29 <DeHackEd> depending on what you're up to, make /path/to/ a dataset ?
    16:29 <twb> it's already a dataset, but with canmount=no
    16:30 <twb> The specific case is /var/lib/machines/myfirstcontainer  being spun up by systemd's machinectl install-tar (which AFAICT is basically "tar xf")
    16:30 <twb> So plan B is actually to just use tar directly
    16:30 <freqlabs> I'd tend to use symlinks to get the paths I want, like a dataset /path/to/foo-private, setup creates /path/to/foo-private/foo, add symlink /path/to/foo -> /path/to/foo-private/foo
    16:30 <twb> freqlabs: hum, or a bind mount, at that point
    16:31 <freqlabs> symlink seems like way less overhead but whatever suits you
    16:31 <twb> fair enough
    16:32 <twb> hahaha
    16:32 <twb> machinectl import-tar doesn't understand compressed tarballs

2019-05-15 #systemd::

    15:15 <twb> When using systemd-networkd with a static config (instead of DHCP), does networkctl consider the link "configured" as soon as the static config is spun up?
    15:15 <twb> Or does it also check for, like, there being a switch on the other end of the cable, and that the gateway is reachable

    16:25 <twb> does "machinectl import-tar" do anything other than tar -x ?
    16:37 <twb> the source in machined-core.c is hard to follow
    16:37 <twb> It appears to just be doing "tar - < foo.tar"
    16:38 <twb> debspawn creates a running container directly using systemd-nspawn.  machinectl can see it.  machinectl can't clone it.  Why not?
    16:39 <twb> http://ix.io/1J5R
    16:48 <twb> What's the equivalent of "systemctl show" for a .nspawn?
    16:48 <twb> "machinectl show" seems to work, as long as the container is actually running
    16:49 <twb> it doesn't list all the fun stuff in the .nspawn though
    17:30 <robert_> huh
    17:35 <robert_> twb: https://gist.github.com/f1fa2ce46ac5af0732014ac6d2d1d1a4
    17:39 <twb> Can I configure how quickly StopWhenUnneeded= gives up?
    17:48 <twb> GRARGH.  I'm trying to use https://github.com/systemd/systemd/issues/2741  to constrain nsd.service
    17:49 <twb> nsd.service needs PrivateTmp=yes, or it crashes.
    17:49 <twb> netns@.service needs PrivateTmp=no, or it crashes.
    17:49 <twb> nsd.service JoinsNamespaceOf=netns@foobar.service is therefore not possible!
    17:51 <twb> My end goal is to run unbound.service and nsd.service, each with a private network that can see eth0 but only one address on eth0.
    17:51 <twb> (Which might be a very silly thing to want; I'm not sure yet)
    17:53 <twb> I suppose what I could do is ExecStart=! for the netns@ lines...
    17:56 <twb> That's just making ip(8) core dump :-(
    17:58 <twb> It's =+ not =! now, but PrivateTmp still affects those
    17:58 <twb> So... blech
    18:12 <boucman_work> twb, I think there is a type of virtual device you could use for that, rather than assigning multiple IP to eth0, you make multiple interfaces all of which ar physically on eth0
    18:12 <boucman_work> and then you give one interface to each NS
    18:14 <boucman_work> (and to answer your question, this is more a support channel than a dev channel, you'd better redirect that question to the mailing list and/or github, the core systemd devs are more active there)
    18:14 <twb> boucman_work: do you know how to actually "give" an interface to a namespace?
    18:14 <twb> boucman_work: I suspect that if I don't care about "named" netns's (for ip netns xxx), I can bring up the ifaces/addresses via systemd.netdev.  But I don't really know what I'm doing here.
    18:14 <boucman_work> twb, with "ip" yes... there is also a nspawn parameter for that.... for a service, I'm not sure.
    18:15 <boucman_work> named netns is an ip thing, not a kernel thing iirc, but network is not my forte...
    18:16 <twb> boucman_work: I agree re named netns
    18:17 <twb> "ip link add veth-twb type veth" does... something
    18:25 <boucman_work> the notify socket is passed from the host systemd to the container systemd, so the container systemd will pet the host-systemd, which in turn pets the hardware watchdog
    18:26 <boucman_work> which also means that the status of containers is properly followed. While the container-systemd is booting, the host-systemd will mark it as starting and only when the container-systemd reports that it is ready will the host-systemd mark the service as ready
    18:27 <boucman_work> so syncing with After= on containers will work as expected
    18:28 <boucman_work> yes and no
    18:30 <twb> boucman_work: OK so what the guys in that issue seem to be doing is creating two "veth" ifaces, which are basically two fake ethernet ifaces glued together.
    18:30 <twb> boucman_work: then on the outside, they bridge/nat/tunnel/whatever veth-OUTSIDE@veth-INSIDE to en0
    18:31 <twb> boucman_work: then on the inside, they add 1.2.3.4/24 and 0/0 via 1.2.3.1 to veth-INSIDE@veth-OUTSIDE
    18:31 <boucman_work> systemd-nspawn implements it (systemd-nspawn is very systemd-aware) but it's "just" about passing an opened socket to the container-init+setting an environment variable
    18:31 <twb> *I think*
    18:31 <twb> boucman_work: yeah but nspawn expects an entire container, not just 1 or 2 services
    18:31 <boucman_work> both of which are already passed/set by systemd
    18:32 <twb> It's not clear to me how I can say "dear nspawn, please run nsd.service with these nspawn options"
    18:32 <boucman_work> twb, I was answering auxloop (which has a full container iiuc)
    18:32 <twb> oh sorry
    18:32 <boucman_work> np
    18:33 <boucman_work> twb, for you, veth are probably not the right type of virtual interfaces, they create a pair of connected interfaces, but they are not linked to a "real HW" interface...
    18:34 <boucman_work> but there are so many types of virtual interfaces in linux that I'm not sure what would be the right one... I know it exists, but I don't know what it's called
    18:35 <twb> systemd.netdev has a list
    18:36 <twb> ipvlan maybe
    18:36 <boucman_work> yeah, that sounds about right
    18:36 <boucman_work> test it manually first, though
    18:37 <twb> Even if I spin that up in systemd-networkd, I don't see how to "share" it into the PrivateNework='d .service
    18:39 <twb> Oh wow, LogLevelMax= is new and cool
    18:40 <twb> (since v215)
    18:40 [Xogium blinks]
    18:40 <Xogium> so its not that new :p
    18:41 <twb> v215 was the last time I went through *every* option in systemd :P
    18:42 <Xogium> hahahah I do it at every release
    18:43 <twb> AppArmorProfile= says the profile "must already be loaded into the kernel".  Does systemd implicitly add an After= on the units that set up apparmor profiles?  The manpage doesn't say so.
    18:47 <boucman_work> I'd say "try and see what happens :) )
    18:48 <twb> from RTFS, it doesn't look like it
    18:48 <boucman_work> but having systemd pet systemd would probably have more chance to be upstreamed than implementing a different watchdog mechanism...
    18:48 <twb> So on a "normal" system, I guess if AppArmorProfile=usr.bin.foo is used, I have to also add Requires= and After= on ... apparmor.service, I guess
    18:50 <grawity> boucman_work: huh, it seems like a feature that should have been there the whole time
    18:50 <grawity> twb: only if your unit has DefaultDependencies=no
    18:50 <grawity> because apparmor.service does have this option and Before=sysinit.target, while normal services are after sysinit
    18:51 <twb> grawity: ah, good catch
    18:51 [grawity thought apparmor profiles are selected automatically based on the executable name]
    18:52 <boucman_work> grawity, it might be already there (that was my understanding) but I am not sure, so I can't be very affirmative about it
    18:56 <twb> grawity: yeah but that only applies to the main binary, I think
    18:56 <twb> grawity: I think the idea is that if you use AppArmorProfile= it can lock down all the ExecStartPre= crap
    18:56 <twb> grawity: that's just a guess, though

    19:50 <twb> Where do you set blackhole routes for private-use address ranges in systemd-networkd?
    19:50 <twb> This is how I used to do it: http://ix.io/1J6E
    19:51 <grawity> maybe [Route] Destination=10.0.0.0/0 Type=unreachable
    19:51 <twb> Type=blackhole exists within .network
    19:51 <twb> But systemd-networkd doesn't manage the lo iface, at least on Debian 10
    19:51 <twb> Maybe I can/should just continue putting them in /etc/network/interfaces, on the lo iface...
    19:51 <grawity> well
    19:51 <grawity> where do you set up the default route?
    19:52 <grawity> the problem doesn't exist when you do not have a default route yet
    19:52 <grawity> so the natural place would be eth0.network or something such
    19:52 <twb> grawity: default route is set up on one or more ethernet interfaces
    19:52 <grawity> "or more"?
    19:52 <twb> grawity: for when I have multiple upstreams for failover reasons
    19:52 <twb> You put a "throw" rule in the default route table, and use firewall marks to send responses over the correct uplink
    19:53 <killermoehre> twb: wouldn't you solve multiple upstreams with a team-device or appropriate metrics?
    19:53 <twb> But yeah, I could probably just shove them in the single upstream.network that I have in most systems
    19:54 <grawity> killermoehre: team/bond only works with multiple identical links (i.e. gives you a single fat cable)
    19:54 <twb> killermoehre: http://cyber.com.au/~twb/doc/dual-uplink.txt
    19:54 <grawity> twb: add the routes in *all* .network files
    19:54 <twb> grawity: hrmmmm
    19:55 <grawity> in my config, I create a dummy0.netdev for reasons, so I'd be placing such rules there
    19:55 <twb> grawity: the routes aren't link-local by default, though
    19:55 <grawity> (although I already have BIRD routing daemon to handle that for me, but otherwise)
    19:55 <grawity> twb: they don't have to be, do they
    19:55 <twb> if you have (say) an uplink.network and a downlink.network, and BOTH of them define the same blackhole routes, then if EITHER link goes down, won't networkctl drop the blackholes?
    19:55 <grawity> hmm I don't see why
    19:56 <twb> because it would remove routes as part of its teardown
    19:56 <twb> (surely?)
    19:56 <grawity> not if it knows that the same route is defined by another device
    19:56 <twb> Oh OK
    19:56 <grawity> which I hope it does
    19:56 <twb> I didn't expect it to be that smart :P
    19:57 <grawity> the advantages of having a central daemon instead of a collection of shellscripts?
    19:58 <grawity> hmm that 'throw default' trick might be useful
    19:58 <twb> yaaaaaay learing
    19:58 <twb> *learning
    19:59 <twb> In other news, *something* created ve-<hostname> and networkctl says it's no-carrier configuring
    19:59 <twb> But it's not mentioned in /etc/systemd/network/
    19:59 <grawity> sounds like a nspawn thing
    20:00 <grawity> the host end of a veth pair
    20:00 <twb> Oh, /lib/systemd/network/80-container-ve.network
    20:00 <twb> Anyway, possibly because of that, when I force-reload systemd-networkd, it tells me wait-online will never happen, which makes me cranky.
    20:00 <twb> systemd-networkd-wait-online[21108]: Event loop failed: Connection timed out
    20:00 <twb> systemd[1]: Failed to start Wait for Network to be Configured.
    20:01 <killermoehre> twb: you want probably your own wait-online.service
    20:01 <twb> Can't I just tell it not to care about the ve-* thing?
    20:01 <twb> Its logs suggest it's already ignoring lo
    20:02 <twb> Oho, /lib/systemd/systemd-networkd-wait-online --help says it has --ignore
    20:05 <twb> Also, I just realized it's named after one of the nspawn'd hosts, rather than after the host OS.  So now I understand what's going on better.
    20:06 <twb> If I "machinectl stop not-alamo", systemd-networkd-wait-online completes immediately
    20:07 <twb> http://ix.io/1J6K  you can see the ve getting confused during "machinectl start"
    20:08 <twb> But that's probably my fault for not setting up the /var/lib/machines/not-alamo/ tree the way machined expects
    20:08 <grawity> networkd could use some verbosity by default imho
    20:12 <Xogium> so could timesyncd… Doesn't even signal a dns resolution failure of any kind and just reports idling away when checking the status… I had to run ntpd to understand it was dnssec failure every time
    20:12 <grawity> though lennart will of course just say "yeah and I'd like a pony"

2019-05-16 #ntpsec::

    19:56 <twb> Why isn't /usr/lib/ntp/rotate-stats just /etc/logrotate.d/ntpsec ?
    21:10 <twb> Woo, using rotate-stats as my test for "tell systemd to lock down all the things", and the script is still correctly gzipping and find -delete'ing.
    21:10 <twb> http://ix.io/1JcP

2019-05-16 #systemd::

    16:23 <twb> Can I "systemd analyze security" an ordinary file, that I haven't installed into a running systemd yet?
    17:15 <jelle> twb: man page mentions [unit...] not a regular file
    17:15 <twb> jelle: yeah thanks.  I thought there might be some way I hadn't seen
    17:16 <twb> The use case was comparing lockdown between e.g. competing MTAs, where I can't install >1 at a time.
    17:17 <twb> Or doing a bulk check of all the units in Debian, without installing anything
    17:17 <jelle> well you can always lock them down yourself
    17:17 <twb> jelle: yeah granted.  I want to know which upstreams had already made an effort
    17:17 <twb> Becaues if they have, they probably care about security in general
    17:17 <jelle> I wouldn't agree :p
    17:18 <twb> haha
    17:18 <jelle> it's pretty new, and systemd files can also come from your distro
    17:19 <twb> granted
    18:21 <twb> When I'm locking down a unit, is there something like strace or audit to tell me all the things it TRIED to do?
    18:22 <twb> Like, my test case is an irc daemon, and with ProtectSystem=strict, some of its helper processes fail, but it's not immediately obvious WHY.
    18:34 <jelle> well strace -e open $process shows files it opens
    18:35 <twb> Yeah, adding that to the front of ExecStart= is basically plan B
    18:35 <twb> I was hoping you'd say "oh, use systemd-supercoolthing"
    18:35 <jelle> I wonder if systemd has thought of showing violations or even neater, analyzing a process and generating rules
    18:35 <twb> apparmor has a huge set of helper tools to help automate aa lockdown
    18:36 <twb> like, it's smart enough to do things like "hey, looks like this is doing DNS stuff, so I'll suggest @include dns-common"
    18:36 <twb> rather than just the individual specific things it saw the daemon doing while in complain mode
    18:43 <twb> OK so FYI, I re-remembered that I do like "aa-genprof /usr/bin/irssi", then in another window run irssi and do some stuff.  Then in the first window hit "s" and "f", and I have an /etc/apparmor.d/usr.bin.irssi example ruleset
    18:44 <twb> Which lets me see that it needs read access to /etc and write access to $HOME, for example, so ProtectHome= won't be its friend
    18:45 <twb> And it executed /bin/dash, so removing fork(2) won't work
    18:45 <twb> Oh.  Oh damn.  irssi already HAD an aa profile.  Let me try a different test program :-/
    19:02 <xenefix> Hi hi there
    19:08 <xenefix> https://www.irccloud.com/pastebin/1ZJrZXv8/br1.network
    19:08 <xenefix> I have following networkd network
    19:09 <xenefix> and RA's are sent out correctly
    19:09 <xenefix> but the hop limit is set to 0, so the receiving machine doesn't configure the bridge as default gateway
    19:36 <xenefix> In radvd this option is called AdvRouterAdd https://www.irccloud.com/pastebin/LuBcKSPl/
    20:35 <twb> How do I use SystemCallFilter=
    20:36 <twb> I'm doing a test lockdown of what is basically logrotate.sh
    20:37 <TheBrayn> man 5 systemd.exec has some more information on that
    20:37 <twb> http://ix.io/1JcB
    20:37 <twb> TheBrayn: yeah I'm reading that but I'm clearly too tired to understand what I'm doing wrong
    20:39 <twb> Oh maybe SystemCallFilter= isn't supported on this kernel
    20:39 <twb> let's look for that "herp derp no BPFs" message that timesyncd sometimes emits
    20:40 <twb> Can't see it...
    20:41 <twb> I have CONFIG_CGROUP_BPF=y at least
    20:44 <twb> Brainwave: look for existing SystemCallFilter= examples
    20:45 <twb> lib/systemd/system/nsd.service:SystemCallFilter=~@clock @cpu-emulation @debug @keyring @module mount @obsolete @resources
    20:45 <twb> systemd-analyze security claims that's unlocked, too
    20:45 <twb> All the other examples use a whitelist, not a blacklist
    20:45 <twb> e.g. lib/systemd/system/systemd-hostnamed.service:SystemCallFilter=@system-service sethostname
    20:46 <twb> "systemd-analyze security systemd-hostnamed" shows green for most (but not all) SystemCallFilter= lines.
    20:47 <twb> So WTF
    20:54 <twb> I used SystemCallFilter=@system-service for now, which is "90% right"
    20:54 <twb> I also noticed that "systemd-analyze security" is reporting that User= and PrivateUsers= aren't locked down, even though I've set them and they show up in "systemctl show"
    20:57 <twb> And I can see from the files it's creating that it's definitely running as User=
    20:59 <killermoehre> hmm, intersting problem: can I list a units with type X where property Y has value Z?
    20:59 <killermoehre> *all units
    21:00 <twb> killermoehre: I only know how to do that by brute force
    21:00 <twb> list-units | show | grep
    21:00 <killermoehre> twb: yeah, brute-force is easy. but something with busctl?
    21:03 <twb> ARGH.  I was doing "up up up ret" to check the systemd-analyze after editing
    21:03 <twb> But I was running the wrong command out of my history
    21:03 <twb> Now I run the right command, things work!
    21:03 <twb> → Overall exposure level for ntpsec-rotate-stats.service: 0.3 SAFE 😀

2019-05-16 #apparmor::

    19:13 <twb> Woo, I'm using aa-genprof like a grownup!
    19:13 <twb> This one looks funny, though: http://ix.io/1Jcm  isn't totem a video playing thing?
    21:20 <twb> FTR, abstractions/totem *is* the movie-playing thing
    21:21 <twb> No idea why it was suggested; I skipped past it and got to a point where the daemon was still segfaulting, but genprof wasn't finding any more auditd items, so I gave up for tonight

2019-05-16 #debian-au::

    16:18 <twb> A-ha! https://upload.wikimedia.org/wikipedia/commons/d/d5/IRCd_software_implementations3.svg
    16:19 <twb> wow, Exchange has a built-in IRCd
    16:21 <pabs> what?
    16:22 <twb> pabs: based on that graph
    16:22 <twb> $ apt download ircd-irc2  charybdis inspircd ircd-hybrid ircd-ircu foxeye; for i in *.deb; do dpkg-deb -c "$i"; done | grep systemd
    16:22 <twb> ===> only ircd2 and inspircd have native systemd units at all
    16:22 <pabs> wow
    16:23 <pabs> that graph is amazing, IRC folks don't seem to like to work together
    16:23 <twb> pabs: bear in mind some of that happened in the 1980s
    16:24 <pabs> ack, I guess there are way fewer active codebases these days
    16:24 <twb> pabs: also separate networks tend to be highly autonomous; efnet/undernet/freenode/oftc don't have much overlap between them
    16:24 <twb> Both their systemd units are pretty crap
    16:29 <twb> → Overall exposure level for charybdis.service: 9.5 UNSAFE 😨
    16:29 <twb> → Overall exposure level for ircd-irc2.service: 9.1 UNSAFE 😨
    16:29 <twb> Hnerp!
    16:35 <pabs> where does that check come from?
    16:35 <twb> pabs: systemd-analyze security
    16:36 <twb> (It's great)
    16:36 <pabs> hmm
    16:36 <pabs> $ systemd-analyze security | grep UNSAFE | wc -l
    16:36 <pabs> 63
    16:36 <pabs> $ systemd-analyze security | grep -v UNSAFE | wc -l
    16:36 <pabs> 14

    17:14 <twb> charybdis (IRC server) default config file on debian has pre-set passwords for people coming from 198.51/24
    17:14 <twb> Meaning that if you just install it, people in that IP range have ops by default
    17:14 <twb> It's also trying to connect (and maybe forward traffic?) to uplink.com by default
    17:41 <dwfreed> twb: the debian stable charybdis default config does not give any external IP address operator access
    17:42 <twb> dwfreed: I'm lookin at 4.1.1 on buster
    17:43 <dwfreed> 4.1.1 is the same thing
    17:44 <twb> dwfreed: I see 	user = "*@198.51.100.0/24";  on line 188
    17:44 <twb> 	password = "letmein";
    17:44 <twb> If that's turned off, I can't see WHERE it is turned off
    17:44 <dwfreed> that is not an oper block
    17:44 <twb> 	class = "opers";
    17:44 <dwfreed> still not an oper block
    17:44 <twb> Hrm, OK
    17:45 <dwfreed> also, that's a documentation network
    17:45 <twb> I am new to actually *configuring* the ircd
    17:45 <dwfreed> CIDR:           198.51.100.0/24
    17:45 <dwfreed> NetName:        TEST-NET-1
    17:45 <twb> dwfreed: oh, I didn't know that
    17:45 <dwfreed> Comment:        Addresses starting with "192.0.2.", "198.51.100.", or "203.0.113." are reserved for use in documentation and sample configurations.  They     should never be used in a live network configuration.  No one has permission to use these addresses on the Internet.
    17:45 <twb> Excellent
    17:45 <dwfreed> the connect blocks use IPs in the 203.0.113.0/24 range
    17:46 <dwfreed> also as an example
    17:46 <dwfreed> 2001:db8::/32 is likewise a documentation network
    17:46 <twb> https://en.wikipedia.org/wiki/Reserved_IP_addresses#IPv4
    17:46 <twb> I guess I should add more blackhole routes
    17:47 <dwfreed> no need, your ISP already has them
    17:47 <twb> I don't trust ISPs to do their job :P
    17:47 <dwfreed> if they don't, their transit providers do
    17:47 <twb> (I have a whole portable /24 so I'm already a bit weird)
    17:48 <twb> Anytime I have something routing between two networks, I blackhole private ranges as a basic best practice paranoia
    17:50 <dwfreed> look at RFC 1918 and the RFCs that update it
    17:51 <dwfreed> or read wikipedia
    17:51 <twb> I have, long long ago

2019-05-20 #systemd::

    11:11 <twb> Does IPAddressAllow= control which addresses the unit can be a server for (listen on), or which addresses the unit can connect to, or both, or what?
    11:12 <twb> "Access will be granted in case its destination/source address matches any entry"
    11:12 <twb> ...makes it sound like as long as *either* end matches, it'll be allowed
    11:13 <twb> Hrm, so I can do something like "IPAddressAllow=10/8 127/8 IPAddressDeny=all" to allow it to reach anything on the LAN
    11:14 <twb> But I can't say something like "this host has ten IPs, allow unit X.service to LISTEN only on 10.1.2.3/32, but ACCEPT connections from anywhere"
    11:54 <twb> Is there an /etc/systemd/system.conf.d/override.conf, or do I have to edit system.conf directly?
    11:57 <twb> Is there something like systemd-cgls but for namespaces instead of cgroups?
    12:07 <deltab> newer versions of pstree can show namespace changes
    12:09 <twb> Hrm, diff -u <(pstree) <(pstree -S)
    12:09 <twb> Shows that e.g. dovecot has a "mnt" namespace, and systemd-nspawn has a whole bunch
    12:12 <twb> When I see this error, what have I done wrong?
    12:12 <twb> root@not-omega:~# machinectl login not-alamo
    12:12 <twb> Failed to get login PTY: Protocol error
    12:14 <twb> The container has started up just fine (via machinectl start not-alamo); the guest has systemd as pid1; not sure what else to check
    12:16 <twb> Hrm, there is *both* "login" and "shell" commands
    12:16 <twb> Same error, though
    12:25 <twb> To answer one of my earlier questions, systemd-system.conf says /etc/systemd/system.conf.d/*.conf is checked
    12:25 <deltab> does the container have dbus? https://github.com/systemd/systemd/issues/685
    12:26 <twb> deltab: good thinking!  Debian's systemd doesn't hard-depend on dbus for horrible reasons
    12:28 <twb> Does SystemCallArchitectures= affect people using qemu CPU emulation + binfmt-misc to run cross-arch chroots?
    12:28 <twb> e.g. an armv7 chroot on x86_64 hardware
    12:29 <twb> https://wiki.debian.org/QemuUserEmulation
    12:30 <twb> deltab: dbus was not installed
    12:32 <twb> "$ systemd-nspawn --machine not-alamo apt install dbus" didn't Just Work; seems it either can't resolve or can't connect to deb.debian.org :-(
    12:35 <twb> Looks like inside the container, nsswitch.conf and resolv.conf are only pointing at systemd-resolved, and systemd-resolved's backcompat listened on 127.0.0.53 is off because I'm running an authoritative nameserver on this host
    12:35 <twb> So installing libnss-resolve inside the container will probably fix it
    12:37 <twb> OK, after installing dbus, "machinectl login" works fine!
    12:38 <twb> "machinectl shell not-alamo" gives "sh: 2: exec: : Permission denied".
    12:38 <twb> Ah, I'm supposed to do something like this: "machinectl shell not-alamo /bin/cat /etc/passwd"
    12:43 <deltab> it seems the message was changed to "Failed to get login PTY: There is no system bus in container" -- do you have a pre-2016 version of systemd?
    12:44 <deltab> looks like IPAddressAllow works at the packet level, so it doesn't know about syscalls
    12:44 <twb> inside the container, yes
    12:45 <twb> host is Debian 10, guest is Debian 9
    12:45 <twb> deltab: so IPAddressAllow is basically a per-unit netfilter *filter table?
    12:46 <deltab> yeah, looks that way
    12:47 <twb> Is it possible to have a full nftables ruleset per-unit?
    12:48 <twb> (for doing fancier things, like, oh, rate limiting or port knocking)
    13:16 <deltab> I imagine so, tied to the network namespace, but I don't know how you'd set it up
    13:23 <twb> I tried using a PowerPC 64 chroot via qemu+binfmt, and SystemCallArchitecture didn't stop me
    13:23 <twb> http://ix.io/1JxB
    13:24 <twb> The use case for that is cross-compiling without having to spin up special any cross-arch packages, or a full VM, but sort of in-between
    13:35 <scientes> tttwyou really need a real VPS
    13:35 <scientes> qemu for powerpc64 is not very good
    13:35 <scientes> at least when it comes to altivec
    13:36 <scientes> twbbbbbbbbbbbb
    13:36 <scientes> twb, 
    13:37 <twb> k.  That was really a thought experiment though
    13:37 <scientes> and clang is a native cross-compiler
    13:37 <scientes> so it is quite easy to cross-compile with clang
    13:38 <Xogium> qemu is no good for aarch64 either
    13:38 <twb> scientes: but then you have to deal with all the build depends and everything
    13:38 <scientes> debian can do it
    13:38 <scientes> but only to a certainly point necessary to bootstrap a new arch
    13:38 <scientes> ARM sponsored that work
    13:38 <twb> you mean like "apt install libfoo-dev:ppc64el" ?
    13:38 <scientes> for their AArch64 bringup
    13:38 <scientes> no, it can bootstrap itsself
    13:39 <scientes> dpkg-buildpackage supports cross-builds
    13:40 <twb> hrm
    13:41 <scientes> systemd probably cross builds with it (or use to) as it is important
    13:41 <twb> does systemd have a list of supported architectures?
    13:41 <twb> You know like how GNOME doesn't support s390 anymore because rust.
    13:42 <scientes> s390 works with llvm
    13:42 <scientes> well actually not sure, ppc64el certainly does
    13:42 <scientes> twb, systemd supports gcc and clang
    13:42 <scientes> and even the 486
    13:42 <twb> righto
    13:43 <scientes> (which is gcc-specific, clang can only build pentium 3+)
    13:43 <twb> insert joke here "it said pentium or better so I bought a sparcstation"
    13:46 <Xogium> too bad systemd can only be built with glibc
    13:47 <Xogium> uclibc-ng used to work, then they broke support for it, then they fixed it, then they broke it for good again
    13:47 <Xogium> and lets just not try with musl
    13:48 <twb> but but but musl has a UTF-8 LANG=C!
    13:49 <scientes> or you can just ignore that variable...
    13:49 <scientes> cause wide character support in libc is so funcky, systemd doesn't use it
    13:50 <Xogium> well, they planned on making that the default for a while and afaik its still the default
    13:50 <Xogium> you have to specify the C local now or its going to default to the utf8 variant
    13:51 <Xogium> which… Noone provides, afaik
    13:52 <scientes> environment variables are a really bad design anyways
    13:52 <scientes> they waste memory bad
    13:52 <twb> Xogium: AFAIK glibc/RH/Debian are working to clean up C vs. C.UTF-8 right now
    13:53 <Xogium> well, they'd better do it :D
    13:54 <Xogium> I mean, we're in 2019 lol
    14:35 !!! irc.freenode.net: deleted (closed)
    > 14:45 /join #systemd
    14:45 <twb> Does "systemctl daemon-reload" cause system.conf to be reread?
    14:45 <twb> I guess I can just put a typo in it and see if I see a warning
    14:46 <twb> Rargh, my qemu test earlier was bogus!
    14:46 <twb> May 20 13:22:30 not-omega systemd[1]: /etc/systemd/system/qemu-versus-systemcallarchitectures.service:4: Unknown lvalue 'SystemCallArchitecture' in section 'Service', ignoring
    14:47 <twb> Ah, it's
    14:47 <twb> Ah, it's plural
    14:54 <twb> OK, SystemCallArchitectures=native is still not stopping qemu-user-binfmt, FYI
    14:55 <twb> Also, daemon-reload appears to have reread system.conf, because now the logs are getting "Consumed 31ms CPU time, no IP traffic." lines
    15:58 <twb> Hey!  journalctl -u foo can't see kernel auditd messages about foo!
    16:21 <twb> If I want to see the read-only vs. read-write vs. hidden of a given unit, do I look in /proc/<pidof daemon>/mountinfo?
    16:21 <twb> That certainly tells me... something
    16:22 <twb> It's hard to tell what's a bind mount, though
    16:23 <twb> http://ix.io/1JxW
    17:34 <twb> These options are pretty confusing - it has ro *and* rw, in separate places
    17:34 <twb> root@not-omega:~# grep -Fw --color -e ro -e rw /proc/$(pidof rsync)/mountinfo
    17:34 <twb> 1364 1281 0:21 / / ro,relatime shared:550 master:1 - zfs omega/ROOT rw,xattr,posixacl
    17:34 <twb> it's read-only in practice, though
    17:39 <twb> If I have a unit that wants to bind to a low port AND doesn't support socket activation, can I tell systemd to run it as User= but still give it CAP_NET_BIND?

2019-05-20 #dovecot::

    19:38 <twb> Anybody know offhand what capabilities and syscalls dovecot needs?
    19:39 <twb> systemd-analyze security dovecot | curl '-sSfFf:1=<-' ix.io  looks like   http://ix.io/1Jyn
    19:39 <twb> (That's systemd having a whinge about anything systemd isn't in charge of locking down)
    19:42 <cmouse> it's impossible to say exhaustively.
    19:42 <twb> No worries
    19:43 <twb> At the moment I'm just e.g. making /etc read-only and then waiting for complaints in the logs
    19:43 <twb> When I get it to a "near enough is good enough" state I'll push it upstream to Debian
    19:44 <cmouse> did you look at the default policy we ship with dovecot?
    19:45 <twb> I'm not sure what you mean by policy
    19:45 <cmouse> sorry i mean service unit
    19:45 <cmouse> https://github.com/dovecot/core/blob/master/dovecot.service.in
    19:45 <twb> I'm starting from whatever Debian ships, which is probably the same as what you ship
    19:46 <cmouse> for apparmor we provide 'apparmor'  plugin
    19:46 <cmouse> to make it less complicated to write policies.
    19:47 <cmouse> mostly it's for allowing to have a "hat" for accessing the user's mails
    19:47 <twb> confirmed, what you linked to is what I was starting from, which systemd considers to be "EXPOSED" (http://ix.io/1Jyn)
    19:48 <cmouse> sure.
    19:48 <cmouse> we could probably tighen things up more if we wanted
    19:48 <twb> It's broadly similar to an apparmor profile/selinux policy.  There's no changehat equivalent, though.
    19:48 <cmouse> yeah
    19:48 <twb> It's more stuff like "hey dovecot is never going to need to load kernel modules, so block that before it even asks"
    19:48 <cmouse> we had to drop e.g. NoNewPrivileges because that breaks things
    19:49 <cmouse> sure
    19:49 <twb> ah, that's good to know
    19:49 <cmouse> it is just difficult to validate which are breaking and which are not
    19:49 <cmouse> since dovecot can be ran in so many ways
    19:49 <cmouse> hm
    19:49 <twb> Yeah, I'm looking to get something that's like 80% locked down and handles 80% of use cases with no fiddling, and people who want more or less can tweak it
    19:49 <cmouse> we have ProtectSystem=full in our service block
    19:50 <twb> =full isn't as full as =strict ;-)
    19:50 <cmouse> i know
    19:53 <twb> the nice thing about this new "systemd-analyze security" thing is it prioritizes them by badness, so you can do the scariest ones first
    20:03 <twb> I think if NoNewPrivileges=yes breaks things, it would be good for the default unit to have a comment like "# NoNewPrivileges=yes broke XXX (ref. issue1234)", so there's a hint for the next person
    20:04 <twb> The hint at the top about dovecot.service.d/service.conf can mention "systemctl edit dovecot" which is new since about 2016
    20:05 <twb> I guess PrivateHome=no is needed for sieve and stuff, even if the actual mailboxes live in /var
    20:07 <twb> Also, this might be obvious to you already: Type=simple means other units (e.g. postfix) cannot wait for dovecot to be "ready" unless dovecot has sd_notify("READY=1") stuff baked into it.
    20:08 <twb> With Type=forking, systemd assumes when the double fork happens is when dovecot's "finished starting" and its dependencies can be spun up
    20:09 <twb> I guess it doesn't really matter, because if postfix can't talk to dovecot's SASL socket, it'll just reject a few messages, it won't actually *crash*...

2019-05-20 #debian-next::

    18:14 <twb> Can I tell adduser to run "zfs create -o quota=1G /home/foo" when it makes a non-system user?
    18:14 <twb> I can't see anything in adduser.conf except for setting the quota.
    18:15 <twb> (zfs create makes a new dataset *and* sets a quota)
    19:21 <twb> Gods, it's perl, I'd forgotten how old this code was
    19:21 [twb runs away]
    19:46 <dwfreed> twb: "If the file /usr/local/sbin/adduser.local exists, it will be executed after the user account has been set up in order to do any local setup.  The arguments passed to adduser.local are: username uid gid home-directory"
    19:46 <dwfreed> a non-system user is any user whose UID is >= 1000
    19:50 <twb> dwfreed: oh thanks, I missed that somehow
    19:52 <dwfreed> I'd seen it before, so I knew what to look for in the manpage
    19:52 <twb> I'll need to poke it a bit to see if that happens before or after /etc/skel copying &c
    19:52 <twb> zfs gets cranky if you try to make a mountpoint where files already exist, &c
    19:54 <dwfreed> adduser.local is the very last thing
    19:54 <dwfreed> so you'd want to wrap adduser in your own script then
    19:54 <dwfreed> twb: ^
    19:55 <twb> Righto
    19:55 <twb> For the initial ~10 users I can just do wahtever.  I was trying to be defensive for when user ~11 is created in a couple of years, and nobody realizes they need to "zfs create" because it's just in some knowledgebase article somewhere

2019-05-20 #apparmor::

    15:59 <twb> If upstream provides an apparmor profile for a binary, but it's a bit broken, can I store my fixes in a separate file?
    15:59 <twb> The immediate case is this:
    16:00 <twb> kernel: audit: type=1400 audit(XXX): apparmor="DENIED" operation="open" profile="/usr/sbin/apt-cacher-ng" name="/etc/ssl/openssl.cnf" pid=8216 comm="apt-cacher-ng" requested_mask="r" denied_mask="r" fsuid=108 ouid=0
    17:09 <jjohansen> twb: potentially. It depends on what you are trying to do, and how the profile is setup
    17:09 <jjohansen> you could of course always just replace the profile, obviously this isn't what you are asking
    17:09 <jjohansen> depending on the reason for the denial
    17:09 <jjohansen> its might be possible to extend the profile with an include.
    17:10 <jjohansen> eg. several profiles have an include of
    17:10 <jjohansen> /etc/apparmor.d/local/...
    17:10 <twb> with that whole local/ thing everyone seems to- that
    17:10 <jjohansen> which is a file you can locally edit
    17:10 <jjohansen> right
    17:11 <twb> Can I tell genprof to write there?
    17:12 <jjohansen> however, local can't override some rules in the main profile, eg. if there is a deny in the main profile there isn't currently a way to override that in local
    17:12 <jjohansen> twb: I don't think genprof supports writing to local yet, I know its planned for
    17:12 <jjohansen> cboltz can speak to it better than I can
    17:13 <jjohansen> twb: when apparmor 3 lands you will also have the option of setting up policy overlay directories
    17:13 <jjohansen> so you could setup, a local overlay say
    17:13 <jjohansen>  /etc/apparmor-local.d/
    17:13 <jjohansen> or what ever you want to call it
    17:14 <jjohansen> and drop a copy of the profile into that
    17:14 <jjohansen> and edit it directly
    17:14 <jjohansen> if a file is in the override dir, it will be used instead of the file underneath
    17:14 <jjohansen> sadly that isn't available to you yet
    17:15 <jjohansen> 2.13 only supports it for caches

[BIG GAP WHERE I GOT SICK OF COPY-PASTE-PRUNING ALL MY IRC LOGS ]

2019-06-25 #dovecot::

    15:59 <twb> cmouse: hey you know how you removed NoNewPrivileges because sendmail_program (may) need setgid?  [https://github.com/dovecot/core/commit/a66e59551]
    16:00 <cmouse> twb: yes?
    16:00 <twb> I have a horrific workaround
    16:00 <twb> BindReadOnlyPaths=/usr/bin/msmtp:/usr/sbin/sendmail
    16:00 <cmouse> that ... can't possibly work
    16:01 <twb> msmtp doesn't need setgid, so you just tell it to send to localhost:25, which is postfix
    16:01 <cmouse> ah.
    16:01 <twb> I'm testing it now
    16:01 <cmouse> you are right, it's horrific.
    16:01 <xrandr> cmouse: how much of the configuration would need to be changed?
    16:01 <cmouse> xrandr: impossible to say without knowing
    16:01 <cmouse> https://wiki.dovecot.org/Upgrading this might help
    16:02 <twb> (It does mean you need to allow AF_INET/6 and IPAddressAllow=localhost, but dovecot at least already needs that much)
    16:03 <cmouse> twb: why not just sent sendmail_path=/usr/bin/msmtp?
    16:03 <xrandr> cmouse: thanks, I'll look into it. For now, it is workign
    16:04 <twb> cmouse: only because that's dovecot-specific and I want to reuse this for e.g. smartd
    16:04 <twb> (and smartd just calls /bin/mail, which calls /usr/sbin/sendmail)
    16:04 <cmouse> twb: btw, i have better idea for you.
    16:04 <twb> Oh, also, because systemd will error out if /bin/msmtp isn't installed
    16:05 <cmouse> try setting 'submission_host=localhost:25' in your config
    16:05 <twb> cmouse: you mean in the postfix main.cf?
    16:05 <cmouse> no, i mean in dovecot config
    16:05 <twb> ah Ok
    16:05 <cmouse> and set sendmail_path=
    16:05 <cmouse> assuming you have 2.3.6 or so
    16:06 <cmouse> ah, no, that has been there for 2.0.10
    16:06 <cmouse> so "should work"
    16:06 <cmouse> q
    16:29 <twb> cmouse: this is working, yay!  http://ix.io/1MJb/ini
    16:29 <cmouse> can you also try the submission_host alternative?
    16:29 <twb> So I can basically use that last paragraph as a "drop in" for all the units I want to have sendmail access
    16:30 <twb> cmouse: sure.  How do I trigger dovecot to try to send an email?
    16:30 <cmouse> twb: vacation?
    16:30 <twb> Right now I don't even have dovecot set up as the LDA; it's basically a fresh debian 10 install
    16:31 <cmouse> twb: sieve vacation is one way to trigger such thing
    16:33 <twb> is there a CLI / TUI sieve client?
    16:33 <cmouse> doveadm sieve
    16:33 <twb> I guess I can just write a sieve file and drop it in by hand
    16:33 [twb RTFM's doveadm sieve]
    16:33 <cmouse> it's slightly safer way to drop it in by hand
    16:36 <twb> Hahaha.  "doveadm sieve list -A" complains about systemd-coredump account (uid=999)
    16:37 <cmouse> twb: yes.
    16:37 <cmouse> twb: there is also min_valid_uid setting
    16:40 <twb> Hrm, I'm very surprised that's not set to 1000 on Debian by default
    16:48 <twb> doveadm also "saw" the user created on-the-fly to execute ntpwait.  Even though ntpwait ends, RemainAfterExit was on, so systemd kept the "temporary" account in nss, where doveadm could see it via getpwent
    16:49 <twb> That took me a little white to debug
    17:11 <twb> OK, I've been stuck for a bit.
    17:11 <twb> root@not-omega:~# doveadm sieve put -u test1 -a vacation-test.sieve < vacation-test.sieve
    17:11 <twb> doveadm(test1): Error: Mailbox INBOX: file_dotlock_create(test1) in directory /var/mail failed: Permission denied (euid=1001(test1) egid=8(mail) missing +w perm: /var/mail, dir owned by 0:0 mode=0755)
    17:11 <twb> That's even with systemd lockdown turned off and dovecot restarted
    17:12 <twb> What dumb thing have I missed?  Maybe that sieve isn't allowed for plain mbox, only maildir/dbox?
    17:13 <twb> Hrm, /home/test1/sieve/vacation-test.sieve.sieve  exists
    17:16 <cmouse> twb: did you enable?
    17:16 <twb> enable what?
    17:16 <cmouse> the sieve script
    17:16 <twb> Isn't that what the -a (tries to) do?
    17:16 <cmouse> well, check with 'doveadm sieve list -u test1'
    17:16 <cmouse> it should tell you if it's active or not
    17:17 <cmouse> also, you need to have 'sieve' plugin loaded for lda/lmtp
    17:17 <twb> vacation-test.sieve ACTIVE
    17:17 <cmouse> twb: also, /var/mail is 0:0
    17:17 <cmouse> twb: dovecot can't create test1 in there then
    17:17 <cmouse> twb: you either need to create and chown yourself, change /var/mail privs, and optionally use mail_privileged_group
    17:18 <twb> There is already a /var/mail/test1 mbox -- http://ix.io/1MJk
    17:18 <cmouse> did you use ProtectSystem=strict?
    17:18 <twb> That's off, and in any case the error is coming from doveadm, which is running unconfined
    17:19 <cmouse> can you show 'doveadm user test1'?
    17:19 <twb> Maybe "dotlocks" need elevated privileges
    17:19 <cmouse> no,.
    17:19 <cmouse> and 'doveconf -n' too
    17:19 <twb> I notice that both mailutils and mutt have setgid binaries for their dotlocks
    17:19 <cmouse> sure.
    17:19 <cmouse> but it's not needed.
    17:20 <cmouse> dotlock = create a .file
    17:20 <twb> http://ix.io/1MJl  doveadm user test1
    17:20 <cmouse> ah.
    17:20 <twb> doveconf -n http://ix.io/1MJm
    17:21 <cmouse> do you also have /home/test1 ?
    17:21 <twb> Yep
    17:21 <cmouse> k
    17:21 <twb> http://ix.io/1MJp
    17:21 <cmouse> btw, you probably want to use %Lu instead of %u
    17:21 <cmouse> aaaaaa... heh =)
    17:21 <twb> That's just whatever Debian gave me; I haven't messed with the dovecot config at all yet.
    17:22 <cmouse> try setting mail_location = mbox:~/mail:INBOX=/var/mail/%u:INDEX=~/mail/.index/
    17:22 <cmouse> or something like that
    17:22 <twb> Will it be simpler to just switch to maildir or dbox?  Because that's part of the end goal anyway
    17:22 <cmouse> also, for the record, i don't think you should use mbox at all
    17:22 <cmouse> i'd use maildir at least.
    17:25 <twb> Is "mail_location = mdbox:/var/mail/%Lu" good enough, or do I need to set an INDEX separately
    17:26 <cmouse> uhm.. why do you think that would work?
    17:26 <cmouse> you probably don't want to use /var/mail for mdbox.
    17:26 <twb> OK.  I definitely want mail and $HOME to be on separate filesystems, at least.
    17:27 <cmouse> k
    17:27 <cmouse> you can use /var/mail
    17:27 <cmouse> but mdbox is going to be a *directory* not a *file*
    17:27 <twb> understood
    17:28 <twb> It's a test server; I'm just moving the old mboxes away and not caring
    17:28 <cmouse> ok
    17:30 <twb> Yeah OK so mail_location alone still needs $HOME access for some stuff; I guess to "protect" dovecot internals from direct user meddling I want to set mail_home as well, like you suggested yesterday
    17:30 <twb> (My other concern there would be EDQUOT, because I have strict quotas on $HOME, but not on mail)
    17:45 <cmouse> btw, you should probably read up on how mdbox works
    17:45 <cmouse> because it needs regular maintenance to work properly.
    17:45 <twb> oh well in that case I'll just continue using maildir and not giving a shit :-)
    17:47 <twb> With "mail_location = maildir:/var/mail/%Lu", doveadm no longer gives me any errors.
    17:47 <cmouse> uh, maildir expects a directory as wel.
    17:47 <cmouse> as well
    17:48 <twb> Yeah I know.
    17:48 <cmouse> you could've tried making /var/mail/.imap folder, chmod 1777
    17:48 <cmouse> just came to mind
    17:49 <twb> Hum, OK
    17:49 <twb> Anyway, after all that, I now have a vacation sieve script installed and active.  Now, how do I get it to fire the sieve script?
    17:49 <cmouse> just deliver mail to user.
    17:50 <cmouse> you can directly execute dovecot-lda
    17:50 <twb> Ah but remember 16:30 <twb> Right now I don't even have dovecot set up as the LDA; it's basically a fresh debian 10 install
    17:50 <twb> Oh OK
    17:50 <cmouse> dovecot-lda -f some@fake.address -d test1 -a test1@fake.address
    17:50 <cmouse> then just write some crap and press ctrl+d
    17:50 <cmouse> on it's own line.
    17:59 <twb> OK, dovecot-lda is delivering the message, but not a vacation message back (I think), probably because my sieve script is broken  http://ix.io/1MJx  http://ix.io/1MJw
    17:59 <twb> I'll try running sievec on it
    17:59 <twb> no errors or output from sievec
    18:00 <twb> I can see /home/test1/.dovecot.sieve but I can't see a compiled version, like I remember from last time I did sieve stuff
    18:03 <cmouse> twb: it's compiled lazily.
    18:03 <twb> Righto

2019-06-25 #postfix::

    15:27 <twb> OK, I have a stupid idea, and I want you to listen to the backstory and then tell me HOW stupid it is.
    15:28 <twb> 1. systemd has a bunch of "drop privileges" features for daemons. e.g. seccomp bpf to make anything that tries to call mount(2) do a core dump instead
    15:29 <twb> 2. a bunch of those implicitly break setgid binaries.  Which is rarely an issue, EXCEPT THAT postdrop is sgid postfix, and lots of things want to use /usr/sbin/sendmail (which uses postdrop).
    15:31 <twb> 3. so screw it, just make /usr/sbin/sendmail be msmtp instead, using localhost as the smarthost, and postfix set to be allow relay for localhost clients.
    15:32 <twb> That way I only need to allow loopback network access, not all the things that would otherwise break postdrop sgid.
    15:32 <twb> The immediate example I have right now is dovecot --- https://github.com/dovecot/core/commit/a66e595515ab579a875a2e9b8116be5da45fb5d6#diff-5bbec0a0006d92d441b5c8fa72690f95
    15:33 <twb> But my other test case is smartd (which sends an email when your disk is dying).
    15:33 <twb> So, how crazy is this plan?
    16:31 <twb> I tested it and it's working, yay! http://ix.io/1MJb/ini

    18:15 <pj> twb: there are a number of good reasons to replace the sendmail binary with msmtp, but I have never had problems with systemd and the postfix sendmail binary, it has always just worked for me.  I would say something is wrong with your install if it doesn't.
    18:16 <twb> pj: I've told systemd to block setgid
    18:17 <pj> oh, in that case the ramifications are on you.
    18:17 <twb> Yeah understood :-)
    18:18 <twb> It's in the same general bucket as "I blocked bad things with SELinux/Apparmor, and now a good thing is also blocked"
    18:18 <pj> there is nothing wrong with wanting a more secure system, but do realize the most secure system is one that can't do anything at all.  I would see if you can put an exception in for postdrop.
    18:18 <blackflow> twb: in my book, if I block something with selinux/apparmor that I think should be blocked and a piece of software breaks, I'd drop that software if possible. No excuse for bad practices.
    18:18 <pj> ...or just use msmtp like you're doing, there are other advantages to that, actually.
    18:18 <twb> pj: I can punch a hole for postdrop sgid, but it's a much bigger hole
    18:19 <pj> you get all the smtp_* features with msmtp, which you don't get with pickup.
    18:20 <pj> blackflow: it's really not a bad practice just because postfix uses a feature of the filesystem that someone arbitrarily decides to block for their own security reasons.  It's not even a security feature that is enabled by default in any distro that I know of.
    18:22 <twb> Actually for daemons that run as root, the whitelist difference isn't too big --- http://ix.io/1MJz/ini (postdrop)  versus  http://ix.io/1MJA/ini (msmtp)
    18:23 <twb> it's bigger for daemons that have all their privileges taken away before systemd even starts them
    18:24 <twb> BTW if you're curious, this command will print all the things that are suid/sgid/sticky: find -O3 / -xdev -type f -executable -perm /6000 -ls
    18:24 <blackflow> running daemons as non-root (with some cap_net_bind_service magick) is what I'd prefer where possible too. I haven't yet sat down to talk with postfix about that tho :)
    18:24 <twb> Mostly they're su/sudo/pkexec and chfn/chpw things
    18:25 <twb> blackflow: mariadb does that by default now, to my great surprise
    18:26 <blackflow> you mean the maintainer of mariadb for your distro configured the service unit to do so? important distinction. something I'd like to see done more in Debian but it won't be as they still wanna support sysv
    18:26 <twb> blackflow: no, I mean upstream
    18:26 <twb> https://mariadb.com/kb/en/library/systemd/
    18:26 <blackflow> twb: in the daemon itself?
    18:26 <blackflow> oh you mean upstream packaged service unit?
    18:27 <twb> blackflow: yes
    18:27 <blackflow> yes, but distros will change that.
    18:29 <twb> blackflow: if you're curious, http://cyber.com.au/~twb/tmp/systemd_241_lockdown.txz  (work in progress)
    18:30 <blackflow> what's that?
    18:30 <twb> That's the "block all the things!" rules I'm dropping on top of various services
    18:31 <twb> (See also "systemd-analyze security")
    18:32 <blackflow> ah, run unpriv, with caps where needed, readonly fs view, strict, rw where minimally needed, private tmp, no access to dev, seccomp lockdown, ... ?
    18:33 <twb> blackflow: yep all that stuff
    18:34 <blackflow> that's what I'm doing where possible, but haven't yet gotten to setup postfix like that.
    18:34 <twb> blackflow: I'd be curious to see your results; I can be found on #emacs most weekdays
