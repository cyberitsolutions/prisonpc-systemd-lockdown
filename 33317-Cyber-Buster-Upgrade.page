GOAL: Alpha and Omega are running Debian 10 Buster.


Checklist
=========

- **TODO** choose hardware

  Omega & Omega-Understudy will remain on the good supermicro servers they're already on.
  They will get MORE RAM.
  They will get small SSDs to make ZFS work better.
  They might have their HDDs replaced.

  For initial experiments (the "learning server"),
  a temporary system has been built from old parts.

  Alpha & Alpha-Understudy hardware remains undecided at this time.
  (Currently on sucky 3U whitebox servers.)

- **TODO** buy more **ECC** RAM for omega & omega-undertudy
- **TODO** buy small SSDs for omega & omega-understudy (for SLOG ZIL, and *maybe* for L2ARC)

- **TODO** get Debian 10 to boot with the rootfs on ZFS
- **TODO** get Debian 10 to boot with /boot also on ZFS (i.e. make a ZFS EFI driver).

  This will let us give the *entire* disk to ZFS, and have just a simple refind-based ESP on a USB key.
  (We won't be able to use stock refind, we'd have to have a ZFS.EFI from somewhere else.)

  This is not comforting:
  https://github.com/zfsonlinux/zfs/commit/7f35d649253c29da8852ae105980a86d8ee7ee2a



Plan
====
* New-Alpha does networking stuff.

  * firewall, DNS, ntpsec, &c
  * nsd3 (authoritative DNS?)
  * fail2ban again?

* New-Omega does everything else.

  * MORE RAM!  (ideally 64GB to 128GB)
  * VT-d & VT-x
  * 1 minimum-size SSD for ZIL SLOG (NVMe would be nice, but AHCI is OK)
  * 1 minimum-size SSD for L2ARC (NVMe would be nice, but AHCI is OK)

  * UEFI and refind, *NOT* BIOS/extlinux/grub

  * Convert omega-understudy and alpha-understudy and cyber-offsite to
    be based on ZFS send/recv.

  * Buster, not Stretch

  * Use dracut instead of initramfs-tools ???

    The benefit is a "more systemd-y" ramdisk.
    The default Debian ramdisk is very old and based on buggy sh scripts.
    The systemd model is that systemd should be IN THE RAMDISK, too.
    The systemd model ALSO goes BACK into the ramdisk during the shutdown process.
    This is particularly useful for systems with ZFS or btrfs rootfs.

  * ZFS on Linux (ZOL) for archive/RAID
  * Samba AD (accounts/LDAP), SMB3.2 file sharing
  * postfix/dovecot/(mailman3?)

    * use dovecot LDA (and sieve); BAN PROCMAIL.

  * strong apparmor/systemd lockdown, NOT containers/VMs
  * icinga3 (replaces nagios), collectd/graphite

  * alloc (mariadb+php7+apache) in a VM, because we don't trust alloc.
    KVM, libvirtd, libvirt-daemon-driver-storage-zfs?

  * rsyslog (logserv) + journald
  * ssh/sftp gateway

  * IRC (eventualy replace with matrix/riot.im)

  * nginx, gitit

  * fail2ban for ALL services (SSH might continue to use the in-kernel IPS as well/instead)
  * letsencrypt for all services
  * gitolite for /srv/vcs

  * configuration management? (ansible / salt)
  * no squid
  * no cups (everyone has to install cups on their laptop)
  * motion (camera)
  * apt mirror (or NBN?)

  * start actually doing IPv6 and nftables, not just IPv4 and iptables

  * discourage/ban people writing /var/spool/cron/crontabs;
    make them write ~/.config/systemd/ timers, so that
    they're version-controlled better.

    Question: do they run when the user isn't logged in?
    e.g. overnight alloc submission job?


How do we boot ZFS?
-------------------

* ZFS wants to be given whole disks, not partitions.
  On Linux, benefits are

  1) ZFS will auto-set the disk's I/O scheduler; and
  2) ZFS will auto-partition replacement disks.

  UPDATE: <rlaager> twb: #1 is not applicable. The initramfs will set noop on the disks in the root pool.

* UEFI (or BIOS) support for /boot on ZFS exist, but do not support all ZFS features.
  Therefore, *either*

  1) the main ZFS pool is limited to a crap feature set; or
  2) /boot lives somewhere else (not the main ZFS pool)

  UPDATE::

    15:34 <rlaager> I think "crap feature set" is a bit harsh. That was _the_ feature set not that long ago.
    15:35 <twb> I haven't looked at exactly what the featureset is

  UPDATE: here is the featureset that zfs_x64.efi from efifs 1.3 (= grub-2.02-143-g51be3372e) supports::

      /*
       * List of pool features that the grub implementation of ZFS supports for
       * read. Note that features that are only required for write do not need
       * to be listed here since grub opens pools in read-only mode.
       */
      #define MAX_SUPPORTED_FEATURE_STRLEN 50
      static const char *spa_feature_names[] = {
        "org.illumos:lz4_compress",
        "com.delphix:hole_birth",
        "com.delphix:embedded_data",
        "com.delphix:extensible_dataset",
        "org.open-zfs:large_blocks",
        NULL
      };

  Comparing that to what rlaager is doing in the HOWTO, the zfs create
  args aren't directly diffable, because in the bpool they're using
  "-d" to turn off most things, then "-o" to turn some things back on.

  rlaager worked it out for me::

    15:53 <rlaager> twb: multi_vdev_crash_dump (which is never used anyway), large_dnode, sha512, skein, and edonr.
    15:53 <rlaager> twb: Oops, sorry, that was bionic, not buster.
    15:56 <rlaager> twb: Buster is the same as Bionic.
                    0.8.0 adds encryption, device_removal, obsolete_counts (part of device_removal, essentially), and bookmark_v2 (part of encryption, essentially)

    15:52 <rlaager> There's also a reasonable debate to be had here about whether we should enable features "just because we can" or whether we should only able features that "matter" on the bpool.
    15:53 <twb> I actually use ext2 for /boot quite often because YAGNI even for the journal and extents :-)


* UEFI ESP is a small (~4MB) FAT32 partition, that must exist *somewhere*.
  It can't reliably be mirrored, so Cyber Best Current Practice is to make it a completely generic refind.img USB key, mounted inside the case.
  Yes, it's a SPOF, but it's a *read-only* device, and if it breaks, it's a simple matter to "cp refind.img /dev/sdz" to make a new USB key.

  Refind doesn't have a ZFS.EFI driver, but it can use grub2's zfs.mod, recompiled for EFI by the efifs project.
  https://efi.akeo.ie/downloads/efifs-1.3/x64/zfs_x64.efi

  rlaager's design would then work like this:

  * ESP (w/ ZFS driver) →
  * bpool (small, feature-limited ZFS pool just for /boot) kernel + ramdisk →
  * rpool (large, full-feature ZFS pool for everything else) rootfs et al.

* Instead of rlaager's design, we propose to store a copy of /boot on the ESP itself.

  Whenever flashrom would run (on initramfs compile / kernel install?)::

      mount -o remount,rw /boot/efi
      rsync --exclude=efi /boot/ /boot/efi/debian/
      mount -o remount,ro /boot/efi

  Also, to preserve the old model of "just dd this image to a new USB
  key", take a backup of the entire USB key *back* to a zvol inside
  the main pool, e.g. ::

      # The --inplace is to avoid COWs on NOP blocks;
      # I think there is a new ZFS feature that can automatically skip NOP writes?
      rsync --inplace /dev/disk/by-id/TOSHIBA-XXXX  /dev/disk/by-uuid/XXXX

  This ensures both /boot and the full ESP disk get ZFS snapshotted and ZFS send-ed to the usual backup places.

  * UPDATE::

        15:41 <rlaager> twb: You might be able to save some steps with this approach:
                1) Always keep your FAT32-formatted zvol mounted at /boot. Mount it with "sync" for extra safety, especially in light of...
                2) rsync the zvol to /dev/disk/by-id/TOSHIBA-xxxx "Whenever flashrom would run"
        15:42 <twb> ah so copy it in the other direction, basically
        15:42 <rlaager> Exactly. Then it should be "safe" to keep it mounted all the time, which
                        is what I assume you were trying to avoid with the flash drive, which causes the dual re-mount steps.
        15:43 <twb> That would do (slightly) more writes to the USB key, but probably fine.
        15:43 <rlaager> Would it write more?
        15:44 <twb> depends if the rsync blocks line up with the FAT and/or erase blocks, I guess

        15:43 <twb> It would also hose any writes by the bootloader or mainboard (or other OSs) to the USB key...
        15:44 <rlaager> But yes, it would overwrite any changes from elsewhere. That is a downside.


  * Why not just put /boot on the normal pool and use grub zfs.mod driver?
    Because then we wouldn't be able to use sexy new ZFS features until GRUB gets support for them, and
    grub isn't updated very often.
    (Currently I'm just *assuming* those features are important enough to be worthwhile.)

  * Why not just use rlaager's bpool and rpool?
    Because then we'd have to handle disk partitioning ourselves and I/O scheduling, instead of letting ZFS manage it.
    Because we hate grub (and update-grub and os-prober) and like refind, and getting the ZFS driver from grub into refind (via efifs project) is an extra hassle.

  * Why not just netboot?
    Because then we can't boot until the PXE server is running.
    Because we also want to use this design *on* the PXE server itself.

  * Why not just mount the ESP as /boot?
    Because then the ONLY copy of /boot is on the USB key, which is a SPOF.

  * Why not use BIOS instead of UEFI?
    Because the grub ZFS driver is just as limited on BIOS as on UEFI, i.e. it wouldn't help.
    Because we would also still need somewhere to store the stage0 and stage1, i.e. a disk whose partitioning is managed outside of ZFS.

  * Why not mirror the ESP so it's redundant?
    Because it's more failure-prone than just reflashing the SPOF USB key.

    At a minimum, we'd need to use md metadata=0.9 (so the md metadata lives at the END of the partition), and
    we'd need to make the FAT filesystem slightly smaller than the partition its on
    (so that if/when e.g. refind or the mainboard UEFI firmware writes to the FAT, it will never overwrite the md metadata).
    And even once that's done, the GPT partition tables would not be md mirrored, so if they ever changed, they'd need to be manually kept in sync.

    Possibly we could work around that by md mirroring the entire USB
    keys, with metadata=1.2, so that the disk layout would be
    "[GPT1][md metadata][FAT][blank][GPT2]".  But UGH.


ZOL 0.8~rc4?
------------

* ZOL 0.8 adds a systemd generator (zfs-mount-generator).
  This makes /etc/fstab obsolete and allows mounts to be done ENTIRELY inside ZFS.
  Without this, ZFS and systemd fight, and you have to jump through hoops to work around it.

* ZOL 0.8 adds native ZFS full-disk encryption.
  We want this for the offsite host.

  If we do it for new omega itself, then

  1) the offsite host *NEVER* needs to know the decrypt key; but
  2) every time omega has a power outage, someone has to provide the decrypt key.

  If we set up alpha the same way, the same applies to alpha.
  If we don't, then I *think* snapshots of alpha sent to the offsite DR will be unencrypted?
  I'm not too sure.

  This is still super experimental, so I'm not really comfortable with it.
  We will probably end up with the shitty situation of missing out on this by <1y, and
  having unencrypted new-alpha and new-omega, and LUKS encrypted offsite DR.

How do I grow the refind ESP enough to put kernels in there?
------------------------------------------------------------
* gparted can grow a FAT32 filesystem
* parted 3.0+ has no "resize", only "resizepart", which explicitly does not touch the filesystem, only the partition.
* gparted is doing the resize by calling libparted's fat_resize(), AFAICT.
* libparted's provides no way to access fat_resize() from a script or CLI?
* no other tools, e.g. dosfsutils, seem to provide a filesystem-only growing tool a la resize2fs for ext4.
* UPDATE: apt install fatresize ???


How do we lay out the datasets in the ZFS pool?
----------------------------------------------------------------------
* Why do we have -o canmount=no "useless" datasets?
  It is just to avoid specifying -o mountpoint=X?

  Mostly, but russm says this also is definitely the recommended way Solaris ZFS worked when ZFS was new.

  Therefore twb convinced to just carry around these "extra" "useless" intermediary datasets.

* Is it sensible to group datasets "logically" instead of "as
  mounted"?  e.g. ::

    OS/root,
    DATA/USER/twb/home (/home/twb),
    DATA/USER/twb/mail (/var/mail/twb),
    DATA/USER/ALL/mail (/var/mail/Lists),
    DATA/USER/ALL/biz (/srv/biz)

  russm asks: what does that GAIN us?

  twb says: we can say things like "What's the growth on twb's data?" and "Do another backup, but ONLY of twb's data".

  russm says: but OTOH it's more confusing (compared to pool structure matching mount structure),

  Therefore twb convinced to make the pool/dataset structure match the mountpoint structure.

  * PS: in addition to pool-structure-matches-mount and
    pool-structure-is-logically-grouped, we also considered
    pool-structure-is-flat, like we do for LVM (because we have to
    for LVM) e.g. ::

        root
        home-twb  (/home/twb)
        var-spool-cron (/var/spool/cron)

  * PS: since this is a singleton server, we CAN just move things from
    their traditional locations in many cases, e.g. /var/mail can move
    to /srv/mail.  Is this sensible, or a waste of time?

    russm says overall waste of time;
    twb doesn't really care.

* Also, should the ZVOLs (VM disks, plus the ESP backup) go at the root of the pool, or what?

  russm says: put them under pool/ZVOLs, because by default zpool/zfs
  commands don't show the zvols, but the "ZVOLs" dataset will still
  show up, basically as a reminder to look at zvols as well.

  We might have called it "VMs" instead of "ZVOLs", but there's on
  (ESP backup) that isn't a VM, so... meh.



References
==========
* login.cyber.com.au:/srv/vcs/flash-kernel-efi.git
* `lshw-omega-understudy.html`_
* https://www.supermicro.com/products/motherboard/Xeon/C600/X9SRi-F.cfm
* https://github.com/zfsonlinux/zfs/wiki/Debian-Buster-Root-on-ZFS  (Using ZFS 0.7)
* https://github.com/zfsonlinux/zfs/wiki/Debian-Buster-Encrypted-Root-on-ZFS  (Using ZFS 0.8~rc4 from experimental)
* https://github.com/pbatard/efifs
* https://efi.akeo.ie/downloads/efifs-1.3/x64/zfs_x64.efi
* https://github.com/hn/debian-buster-zfs-root
* https://raw.githubusercontent.com/hn/debian-buster-zfs-root/master/debian-buster-zfs-root.sh
* http://open-zfs.org/wiki/Feature_Flags
* https://manpages.debian.org/buster/manpages/hier.7.en.html
* https://manpages.debian.org/buster/dracut-core/dracut.8.en.html#SEE_ALSO
* https://manpages.debian.org/buster/zfsutils-linux/zfs.8.en.html
* https://manpages.debian.org/buster/zfsutils-linux/zpool.8.en.html
* https://systemd.io/BOOT_LOADER_SPECIFICATION   # UPDATE: not really helpful.

  I have confirmed the EFIFS 1.3 uses grub-core's zfs.c almost at HEAD.
  This is the commit of grub that efifs is using::

      51be337 2018-04-16 22:36 -0700 NVi ∙ [HEAD] templates: Update grub script template files

  There are negligible differences between that and grub master::

      twb@goll[grub]$ tig HEAD..master -- grub-core/fs/zfs
      ad4bfee 2019-04-08 07:24 +0200 VSe ∙ Change fs functions to add fs_ prefix
      ca0a4f6 2013-11-20 02:28 +0100 VSe ∙ verifiers: File type for fine-grained signature-verification controlling

  There are negligible differences between that and the (earlier) 2.02 stable grub release::

      twb@goll[grub]$ tig 2.02..HEAD -- grub-core/fs/zfs
      fa42786 2017-08-03 15:46 +0100 PBa ∙ zfs: remove size_t typedef and use grub_size_t instead

  2.02 is the version of grub in Debian buster.
  There is a 2.04 release candidate at this time (Apr 2019). ::

      $ tig 2.02..grub-2.04-rc1 -- grub-core/fs/zfs
      ad4bfee 2019-04-08 07:24 +0200 VSe ∙ Change fs functions to add fs_ prefix
      ca0a4f6 2013-11-20 02:28 +0100 VSe ∙ verifiers: File type for fine-grained signature-verification controlling
      fa42786 2017-08-03 15:46 +0100 PBa ∙ zfs: remove size_t typedef and use grub_size_t instead


IRC Logs
========

2019-04-17  #cyber::

    <twb> ron: old Z68 mobo is now running UEFI firmware and can boot refind
    <twb> FTR, the process was:
    <twb> 1. download .exe from gigabyte's website (requires js)
    <twb> 2. from windows, run the .exe, which happens to be a .7z self-extractor (so PROBABLY could use 7zip on linux)
    <twb> 3. from that, get the FLASHEFI.EXE and FOO.U1F files
    <twb> 4. download FREEDOS 1.2 LITE USB .zip, unzip it, get FD12LITE.IMG.
    <twb> 5. sudo cp FD12LITE.IMG /dev/sdz
    <twb> 6. gparted /dev/sdz, grow the FAT16 partition from ~30 MB to ~62 MB
    <twb> 7. sudo mount /dev/sdz1 /mnt; sudo cp FLASHEFI.EXE FOO.U1F /mnt
    <twb> 8. plug the USB key into a **USB2** port, because FreeDOS doesn't have a USB3 (XHCI) driver?
    <twb> 9. boot freedos, choose "english" choose "fuck off"
    <twb> 10. type "FLASHEFI FOO.U1F" to run the flashing program, wait a few minutes while it runs.
    <twb> 11. can't exit the program, so power off, remove power cable, replace power cable, turn on, hooray, you have an UEFI firmware

2019-04-18  #debian-au::

    12:07 <k-man> twb: regarding zfs for rootfs, no, I didn't do that on my laptopt
    12:07 <k-man> as it seemed too difficult at the time
    12:08 <k-man> in fact, I don't store much, if any data on my laptop that isn't backed up elsewhere so at the moment, my laptop isn't backed up - which I guess isn't ideal
    12:08 <twb> heh
    12:08 <k-man> I've been thinking of re-installing buster, and trying to use zfs for rootfs
    12:09 <k-man> there is this blackbox stript to do it. I've not tried it though: https://github.com/hn/debian-buster-zfs-root
    12:11 <twb> k
    12:11 <twb> I'll read that, but I'm planning to do the install via debootstrap, I think
    12:11 <k-man> righto
    12:11 <k-man> is this for yourself or someone else?
    12:11 <twb> this is for my new main server at work
    12:12 <k-man> nice
    12:12 <k-man> is that your choice to use zfs?
    12:13 <twb> yes
    12:13 <twb> AFAICT btrfs has lost
    12:13 <k-man> wow, interesting
    12:14 <k-man> is it a race where there can only be one winner?
    12:14 <twb> Well it seems crazy to have two good filesystems
    12:15 <k-man> yes
    12:15 <k-man> i think zfs had a richer featureset than btrfs - which meant that btrfs was always going to be playing catchup
    12:17 <twb> AFAIK the main missing features in ZFS are:  1. linux-compatible license; 2. per-file nodatacow; 3. EFI driver (i.e. /boot can be inside the ZFS array)
    12:18 <twb> All of which are annoying, but offset by a. built-in bcache equivalent (ZIL, L2ARC); b. fully automatic resilver to spare drive
    12:19 <twb> Oh also ZFS dedup is post-facto and expensive; btrfs dedup is pre-facto and cheap, but requires you to opt into it
    12:19 <k-man> right
    12:19 <twb> But in both cases the end result is mostly "nobody has dedup"
    12:19 <k-man> just about everything I read about ZFS dedup says "don't use it"
    12:19 <k-man> yeah
    12:19 <k-man> i really like ZVols too
    12:20 <k-man> which BTRFS doesn't do afaict
    12:20 <twb> btrfs doesn't need them
    12:20 <twb> regular files work just as well
    12:20 <k-man> isee
    12:20 <twb> both of them suck at quotas
    12:20 <twb> per-user quotas, I mean
    12:21 <twb> the recommended method is "make a separate zfs for each /home/foo and set a quota on that zfs"
    12:22 <twb> -o com.sun:auto-snapshot=false  I'd have never found that one
    12:24 <twb> That script weirdly makes / and /var separate filesystems
    12:24 <twb> And it's written in bash, but only sh is available in the place he claims to be running it
    12:59 <k-man> twb: he mentioned in the readme that /var had to be seperate for some reason
    12:59 <k-man> something to do with the timing of mounting the zfs fs
    12:59 <k-man>  /var has to be mounted from /etc/fstab rather than the ZFS way
    13:00 <twb> k-man: his readme seems to be explainin why he had a workaround to support a separate /var, but not why he wants /var separate in the first place
    13:00 <k-man> oh ok
    13:00 <k-man> doesn't debian do that if you choose the "everything in seperate partitions" option?
    13:01 <twb> dunno, maybe
    13:01 <twb> having a separate /var is stupid IMO
    13:01 <twb> The reason that's A Thing is because originally / and /usr were read-only and shared between 100 computers
    13:01 <twb> and /var was the per-computer part
    13:01 <k-man> twb: ok. I'm not arguing- just speculating that that was his reasoning
    13:01 <k-man> ah ok
    13:02 <twb> But having /usr separate isn't even supported anymore; it was about 60% broken and then systemd forced that to be 100% broken and officially abandoned
    13:02 <k-man> ah

2019-04-18 #zfsonlinux::

    12:56 <twb> I've babysat a couple of Debian + ZOL hosts before, but I'm setting up my own for the first time.  I also want the rootfs to be on ZOL.  As well as the notes in /topic, my main reference is https://raw.githubusercontent.com/hn/debian-buster-zfs-root/master/debian-buster-zfs-root.sh  (from https://github.com/hn/debian-buster-zfs-root).
    12:57 <twb> I notice that's adding *partitions* to the pool, instead of whole disks.  Isn't that a Bad Thing?
    12:57 <rlaager> No, it's fine. I fought the good fight for wholedisk, but it's just not the right fit here.
    12:57 <twb> Because you need an ESP still?
    12:58 <twb> And a /boot, I guess
    13:00 <rlaager> twb: Right, and now we're doing two pools: bpool for /boot and rpool for /
    13:00 <twb> rlaager: who is "we" ?
    13:03 <twb> Is there a ZFS EFI driver yet?  When using BTRFS, my preferred way to handle this is to have whole-disk elements in the pool, and then have a completely static refind ESP on an internal USB key.  Since refind has a btrfs EFI driver, /boot can just be part of the normal btrfs pool.
    13:04 <twb> There's still a SPOF for that USB key, but it's read-only and not device-specific, so it's trivial to replace
    13:06 <twb> I think refind's btrfs EFI driver was stolen from grub2, and I see there's a /usr/lib/grub/x86_64-efi/zfs.mod... not sure how to turn that into an .efi...

    FIXME I had a link from somebody to a ZFS EFI driver from the "efifs" project, but firefox crashed by laptop and I lost it.


    13:01 <gurg> btw rlaager, is this your tutorial since it mentions you?
    13:01 <gurg> https://github.com/zfsonlinux/zfs/wiki/Ubuntu-18.04-Root-on-ZFS
    13:02 <gurg> Because I was wondering if I should open an issue for the hints for step 2.3 "creating the boot pool" because the hint command there for if doing a raidz says rpool instead of bpool
    13:10 <rlaager> gurg: Typo fixed. Thanks!


    14:50 <twb> Hey, did you know your https://github.com/zfsonlinux/zfs.wiki.git has fsck errors
    14:50 <twb> You can't clone it unless you turn off transfer.fsckobjects=true in git
    14:51 <twb> It's almost entirely linear so you might get away with actually fixing it, instead of just having to live with it


    15:26 <twb> The ZOL wiki links to some introductory blog posts https://pthree.org/2012/12/07/zfs-administration-part-iv-the-adjustable-replacement-cache/
    15:26 <zfs-bot> [ Aaron Toponce : ZFS Administration, Part IV- The Adjustable Replacement Cache ] - pthree.org
    15:26 <twb> That says ZIL >1GB isn't really useful.  Is that still true?
    15:27 <twb> And if so, is it silly to get two separate ~100GB SSDs, one for ZIL and one for L2ARC?
    15:28 <twb> I did that last time, but maybe that was just because partitioning disks is lame
    15:29 <jtara> twb it all comes down to your workload
    15:29 <jtara> a SLOG (separate ZIL) is almost always a good idea for a pool that will see significant sync writes, even if it's no faster than the other disks
    15:29 <jtara> l2arc can help, sometimes, sort of...but imo it really is more for salvaging an awful situation than anything else
    15:29 <jtara> memory is better when you don't have to fake it ;)
    15:30 <PMT> yeah but at the time it was built you weren't usually cost-limited for arbitrary amounts of memory, but HW limited because you couldn't stick more in =P
    15:32 <twb> aren't SSDs cheaper than "buy more RAM" still?
    15:32 <jtara> the times i've seen gains with it align are when metadata is badly fragmented from data and you are suffering from it
    15:32 <jtara> yeah but l2arc helps quite a bit less than just more arc
    15:32 <twb> k
    15:33 <jtara> if you have a trashed pool (logbias=throughput, primarycache=metadata to try to cut down on random read iops) then yeah l2arc can help make an awful situation better
    15:33 <jtara> but so much better to just not get into that in the first place
    15:34 <twb> Looks like for comparable capacity, RAM costs is about four times SSDs cost
    15:35 <twb> oops, I can't math
    15:36 <twb> it's more like 32 times
    15:37 <jtara> imo get a pool going well, look at your arc hit rates, then decide if its worth it
    15:37 <twb> That's fair
    15:38 <jtara> i did zfs consulting for database setups for many years, l2arc really was only beneficial a small amount of the time
    15:38 <twb> because (assuming you kept an AHCI slot free) you can always add the l2arc in <1h downtime
    15:38 <PMT> twb: cheaper isn't what I meant, I meant "no money short of a custom Cray thing would buy you more"
    15:39 <jtara> if l2arc makes or breaks your setup, ime something is very weird.  it can help, but it won't rock your world.
    15:39 <twb> PMT: IIUC you're saying that when l2arc was invented, you simply couldn't put more ram in the server.  Now, you can (although it's still a bit more expensive than SSDs)
    15:40 <jtara> if you can't fit enough ram, try to drive up your average recordsize if you can
    15:40 <jtara> that will decrease metadata overhead and help
    15:41 <jtara> old trick from the sun days anyway
    15:41 <twb> I did have one server that was struggling for some reason, and one of the things I did was add ZIL SLOG and L2ARC on NVME SSDs, and the problem went away.  So I guess I (falsely) internalized that adding SSDs for SLOG and L2ARC is a cheap and easy thing you should ALWAYS do.
    15:42 <PMT> twb: you still have limits, but yes, you could put in much less RAM at almost any cost.
    15:42 <twb> PMT: understood
    15:42 <jtara> the big thing a slog will do that surprises most people, is to decouple rmw reads and compression from the point of time of write
    15:42 <jtara> without one, big sync writes get rmw and compression inline
    15:42 <jtara> which can really be a dramatic dofference
    15:42 <twb> rmw?
    15:43 <jtara> read modify write.  like if you write 32k of a 128k record and the rest needs to get read to merge it
    15:43 <twb> I think I understand that
    15:44 <jtara> basically deferring rmw is good because eventually you may get all the pieces and can avoid the read
    15:44 <PMT> twb: ZFS ~never overwrites things in place. So if you modify some or all of a block, the entire new block gets written separately somewhere else, then it may mark the old one as not needed any more. But since that means reading the old one, modifying the contents, and writing it out separately, you call it [...]
    15:44 <twb> I did something like that once to linearize / cohere write spam to RRD files
    15:45 <jtara> anyway, learn and love zpool iostat, especially -r
    15:45 <jtara> it tells you a lot about what's really happening
    15:46 <PMT> sometimes it tells lies.
    15:46 <PMT> Not often, but sometimes.
    15:47 <jtara> it helps to turn off aggregation and to cross reference with blktrace, i do admit
    15:48 <jtara> but for a "wtf is going on with this pool" peek it's hard to beat
    15:49 <twb> I've used it before.  And sar / iostat on pre-ZFS systems
    15:51 <twb> jtara: does having compression "in the loop" like you were talking about, does that matter if it's a realtime compression algorithm like LZ4 / LZO?
    15:52 <jtara> it does, because usually sync write concerns are driven by latency
    15:53 <jtara> so even a pretty fast compression alg can have significant latency impact
    15:53 <twb> Hrm, OK.
    15:53 <jtara> but usually a bigger concern are rmw reads
    15:53 <jtara> when you make a write wait for a read, it's bad
    15:54 <jtara> i almost always recommend deferring them to txg commit time when you can
    15:54 <jtara> which means either a slog or a very high zfs_immediate_write_sz
    15:55 <twb> So is this a reasonable rule-of-thumb?  1) a SLOG ZIL is always useful; 2) an L2ARC is only really useful when you can't get more ARC and/or your system is badly configured
    15:55 <jtara> thats my opinion anyway, there are others out there
    15:56 <jtara> but that's the classic solaris zfs approach and it works well
    15:58 <twb> Is it true that a SLOG ZIL >1GB is a waste of time?
    15:59 <PMT> twb: the rule of thumb is generally X seconds of sync IO for the main pool
    15:59 <jtara> a slog stores sync writes (and all writes in their sync domains) between txg commits
    15:59 <PMT> usually X is 5
    15:59 <jtara> usually you don't need a lot
    15:59 <twb> PMT: so like if I do zpool iostat and I see 100MB of writes per second, my SLOG should be around 5*100MB = 500MB?
    16:00 <PMT> twb: if that's how long you make txg sync length, yeah. You might adjust that for other reasons. But generally it's X seconds of maximum for the pool, not maximum you've seen.
    16:00 <jtara> there's overhead for every write and for every 128k or so block
    16:01 <PMT> Padding it is fairly minimally risky. Undersizing it makes it more useless than you might want.
    16:01 <jtara> just oversize it 10x what you think you'll ever need and don't worry
    16:01 <PMT> If you can do that, sure.
    16:01 <twb> PMT: OK so it would be more like 5 seconds times the sustained write speed of all the HDDs in the pool?
    16:02 <twb> In my immediate case it's easy for me to give it 10GB or 100GB instead of 1GB, I'm just trying to understand where the numbers come from
    16:02 <PMT> twb: generally? if it were raidz vdevs I'd probably suggest something like X seconds times the total throughput of data disks per pool, but the overprovisioning suggestion means you probably shouldn't bother caring about the difference.
    16:02 <twb> OK cool
    16:02 <PMT> twb: your goal is for it to be able to take all the sync IO from a txg without needing to wait on the disks to catch up.
    16:02 <jtara> if you really want to precision size it, take zfs_dirty_data_max and double it
    16:03 <jtara> that should be worst case
    16:03 <jtara> seriously if you run out then you will just slam into a complete wall until the txg gets committed in full
    16:04 <twb> jtara: OK, although it helps to be able to estimate the number BEFORE I put in a purchasing request, let alone set up the pool :-)
    16:04 <jtara> can you even get disks that small though
    16:05 <PMT> Sometimes.
    16:05 <PMT> The other thing is that as disks get smaller they often have lower performance because they need less flash. ;)
    16:05 <twb> So let me use some real numbers here... sustained write for a WD Red 4TB is 150MB/s.  150MB/s * 5s * 4 disks in a RAIDZ1, yields about 3000MB, or 3GB
    16:05 <jtara> ok, say you're writing compressible data, like text
    16:06 <PMT> Are you sure the drives only do 150 MB/s, max?
    16:06 <twb> So if the smallest SSD I can buy anyway is 100GB, I can give 10GB to the SLOG and 90GB to L2ARC just because it's lying around anyway
    16:06 <twb> PMT: https://www.wd.com/content/dam/wdc/website/downloadable_assets/eng/spec_data_sheet/2879-800002.pdf
    16:06 <jtara> so for your raidz to handle 3000mb, your slog might have to handle 9000mb
    16:06 <PMT> twb: caveat, data in the L2ARC takes up space in the ARC. So it's not just a free set of more RAM.
    16:06 <jtara> or even
    16:06 <jtara> over 9000.
    16:06 <PMT> twb: heh, 4TB.
    16:07 <jtara> try to resist the temptation to do something else with a slog device
    16:07 <jtara> but, people do
    16:07 <twb> This is for what you might call a "pet server"; the entire dataset fits into 1TB currently
    16:08 <twb> jtara: OK.  So buy the smallest SSD and let the SLOG have all of it, and it might be massively overprovisioned, but who cares
    16:15 <jtara> my customers always care about predictable latency and smooth degradation and stuff like that...so yeah


    18:42 <twb> rlaager: in https://github.com/zfsonlinux/zfs/wiki/Debian-Buster-Root-on-ZFS §2  you use mdadm and sgdisk to wipe existing config on a disk.  You might want to also use wipefs(1), which removes everything that blkid(1) can see
    18:49 <twb> Ugh, I was wondering why gdisk instead of libparted --- turns out the ZFS support in libparted is Ubuntu-specific


2019-04-23 #zfsonlinux::

    12:31 /join #zfsonlinux
    12:34 <twb> rlaager: re zfs.wiki/Debian-Buster-Root-on-ZFS.md, you said last week that you wanted to give ZFS the whole disk(s), but had to give up in that HOWTO (because grub zfs is feature-limited, and because ESP).
    12:35 <twb> $coworker has a different idea and I want to know how much you (all) hate it
    12:35 <twb> 1. ESP is stock refind on a USB key inside the chassis, because refind Just Works, and then it's a completely static ESP that's easy to replace
    12:35 <twb> 2. except that refind has the same problem as grub, so instead of making /boot separate, leave it on the ZFS, but have an apt post-install hook that copies /boot onto the ESP
    12:36 <twb> If the ESP drive goes tits-up, you need to boot a live system to recover, instead of just making another stock refind USB key, but that's not TOO hard, and it means you can continue to give full disks (instead of partitions) to ZFS.  Which is/was a Super Nice Thing
    12:39 <PMT> twb: i mean, that assumes the existence of an internal USB key, which is a bit of a specialized way to solve this
    12:39 <twb> Granted
    12:39 <twb> That's our (work's) standard way to work around EFI having no concept of mirrored ESP
    12:40 <twb> You make the ESP the same across all systems, so it's more like a network card than a software configuration, and if it breaks you just install a new one
    12:41 <twb> grub zfs.mod limitations won't let me make it EXACTLY the same (at least, if the pool is going to use newer features), so #2 is my workaround for that
    12:42 <PMT> I mean, at that point, why not just netboot refind?
    12:43 <twb> I guess mainly because that makes it dependent on the PXE server already being up.  Also one of the targets for this design *is* the netboot server
    12:44 <PMT> So you have multiple netboot servers and they boot from each other. :V
    12:44 <twb> heh, fair
    12:46 <PMT> pts0: i mean, everything you've never done before is complex, so
    12:46 <twb> Looking at the issue from the other end: Solaris ZFS used to *really* want whole disks, not partitions (AFAIK).  Is that (disks not partitions) something ZOL still cares about?
    12:49 <PMT> twb: Solaris wanted whole disks to play with the write cache settings. ZoL wants whole disks to set the whole disk's IO scheduler to noop or equivalent. If you can't use the whole disk, you could still script doing that.
    12:49 <twb> Ah OK
    12:49 <PMT> (if it's not a whole disk you also need to go fiddle with the partition tables to expand a device, I believe, but vOv)
    12:49 <jasonwc> DeHackEd, Cisco already listed the Optane DIMMS for sale.  $2000 for 128GB and $21k for 512GB.  --> https://www.storagereview.com/cisco_details_intel_optane_dc_persistent_memory_pricing
    12:49 <zfs-bot> [ Cisco Details Intel Optane DC Persistent Memory Pricing | StorageReview.com - Storage Reviews ] - www.storagereview.com
    12:50 <PMT> lmfao
    12:50 <PMT> quite the goddamn dimm
    12:50 <twb> Does ZFS care if the partitions were created with parted --align=optimal (or whatever the gdisk equivalent is)?
    12:51 <PMT> ZFS doesn't, but the drives do. :D
    12:51 <twb> k
    12:51 <jasonwc> they have a review of a Supermicro server with those persistent DIMMs.  "Our first test is the 4K random read test here the persistent memory started at 1,371,386 IOPS at 4.6μs and went on to peak at 13,169,761 IOPS at a latency of only 12.1μs. "
    12:51 <pts0> PMT so it sounds like mdadm with mbr partitions booting to bios boot mode is the way to given all that twb is going through
    12:51 <PMT> That's mostly a joke - using non-physical block size aligned partitions is an invitation to pain and suffering.
    12:51 <PMT> pts0: what?
    12:52 <twb> PMT: yeah understood, I just half-expected you to say "ZFS is so smart it'll automatically fix that for you if you screw up"
    12:52 <pts0> so just ext4 a root and then a boot partition and do a bios as opposed to a uefi boot
    12:52 <PMT> pts0: ...how did you get there from any of this discussion.
    12:52 <pts0> zfs on boot with uefi is crazy complex
    12:52 <twb> PMT: I think they're thinking of a md raid1 ext /boot, and the UEFI->BIOS thing is just a conflation
    12:52 <pts0> what i said is much simpler
    12:53 <pts0> no forget uefi
    12:53 <PMT> twb: i know, i'm just unsure why they think this is helpful.
    12:53 <pts0> it won't even see the stuff right without a bunch of fanagling
    12:53 <jasonwc> pts0, What's so complex about it? It's documenteed by rlaager
    12:53 <twb> pts0: are you suggesting that the non-UEFI (BIOS) ZFS drivers in grub are *better* than the UEFI ZFS drivers in grub?
    12:53 <pts0> no i'm saying no zfs for the boot and root
    12:53 <pts0> and just do a bios boot
    12:54 <PMT> pts0: you appear to be advocating a solution to a problem nobody else thinks is a problem.
    12:54 <jasonwc> You can do BIOS boot with separate boot and root pools
    12:54 <jasonwc> You can also do a BIOS boot with a single root pool if you don't need features Grub doesn't support
    12:54 <pts0> seems to complex to me from everything i've looked at
    12:54 <pts0> 100 damn steps
    12:54 <jasonwc> It's just thoroughly documented.  It's not a hard process.
    12:54 <twb> I don't want pre-UEFI BIOS, and I want / on ZFS.  Having /boot on ZFS is "nice to have" but not critical..
    12:55 <rlaager> pts0: You're free to do whatever you want on your systems, of course, but this works fine and is well documented. It's also the way forward if this ever gets distro integration, which is being considered.
    12:55 <pts0> Can someone give me a link to the documentation
    12:55 <pts0> maybe i saw the wrong thing
    12:55 <twb> pts0: zfs.wiki/Debian-Buster-Root-on-ZFS.md
    12:55 <jasonwc> pts0, What distribution are you planning to use?
    12:55 <twb> pts0: so uh... https://github.com/zfsonlinux/zfs/wiki/Debian-Buster-Root-on-ZFS
    12:55 <pts0> ubuntu
    12:56 <twb> pts0: there is a similar page for Ubuntu LTS on that wiki
    12:56 <jasonwc> pts0, https://github.com/zfsonlinux/zfs/wiki/Ubuntu-18.04-Root-on-ZFS
    12:56 <pts0> yeah...that's the one i saw
    12:56 <pts0> it's above my brain capacity
    12:56 <pts0> ugh
    12:56 <jasonwc> pts0, For one thing, it documents both an encrypted and non-encrypted setup
    12:56 <jasonwc> For another, it includes every step, so you just need to follow the instructions once.
    12:56 <twb> pts0: most of it is standard, e.g. the ESP stuff is the same as if Ubuntu's debian-installer script was doing it.
    12:57 <rlaager> twb: If you put /boot into the ESP, then yes, your alternative proposal works fine, and ZFS can have whole disks for the rpool.
    12:57 <pts0> I just can't understand it
    12:57 <jasonwc> pts0, What don't you understand?
    12:57 <pts0> I wonder if they were to put it in sections--maybe taking out the encrypted part,etc.
    12:57 <twb> rlaager: OK, thanks.  I was mainly hoping for a response of "oh, interesting idea" or "don't do that, because <reason>!"  :-)
    12:57 <pts0> put the encrypted part in another link
    12:57 <pts0> i dunno
    12:58 <pts0> Then it's just not explained well
    12:58 <pts0> run sgdisk with crazy switches
    12:58 <pts0> well i've never run sgdisk before
    12:58 <pts0> i've used fdisk
    12:58 <rlaager> twb: This also works if you have separate disks generally. My tentative plan for my personal server is to replace everything with the following: 1) A mirrored pair of SSDs. They will be partitioned for BBP (because this is BIOS only hardware but could be ESP), bpool, and a "special" (allocation classes) partition for the rpool. 2) The spinning disks will be a raidz3 with wholedisk.
    12:58 <pts0> and another one so I dont' even know what it's doing
    12:58 <twb> pts0: fdisk -> gdisk is just MBR -> GPT
    12:58 <jasonwc> pts0, I mean, you can manually partition with fdisk
    12:59 <rlaager> twb: The main reason to NOT do it in a server environment is the lack of mirroring, which you've acknowledged.
    12:59 <pts0> all day long
    12:59 <pts0> but what's with all the switches
    12:59 <jasonwc> pts0, In fact, I"ve done it.  You can also use the commands provided and it will work.
    12:59 <pts0> then they don't talk about how to get the disk uuids or any explanation for the sizing and zfs switches
    12:59 <pts0> just do all this craziness
    12:59 <twb> rlaager: OK, I understood all that except for the special allocation classes thing
    12:59 <pts0> ok, my main disk dies
    12:59 <pts0> then what
    13:00 <pts0> i have to go into the bios and repoint it to another disk or something
    13:00 <rlaager> pts0: If you have concrete suggestions, I'm open to adding more explanations. It does make it longer, but people seem to like the explanations generally, as they can skip them if they don't care.
    13:00 <pts0> the format arch uses always helps me
    13:00 <pts0> i don't know why
    13:00 <jasonwc> I suppose it would be nice if there were expandable sections so you could just follow the one you wnated
    13:00 <pts0> yeah
    13:00 <pts0> yes
    13:00 <rlaager> twb: It's a new in 0.8.0 feature that puts the metadata on separate devices, in this case, SSDs. So the hope is I'd get a useful mix of capacity from raidz3 and metadata performance from SSDs.
    13:00 <twb> rlaager: ah, OK.  I have seen a similar thing in btrfs, I think
    13:01 <jasonwc> but not sure if the Wiki even supports that
    13:01 <rlaager> I'm not sure either. Maybe I could look at readthedocs.io or whatever the kids are using these days.
    13:01 <pts0> rlaager for starters make two separate links for encrypted vs non-encryped
    13:01 <pts0> to clear up the cluttered appearance...but other than that...truth is i dont' know what the hell im doing so
    13:01 <pts0> this is going to be a tough one
    13:02 <jasonwc> pts0, Try following the instructions in a VM.  I bet you'll find it is easier than you think.
    13:02 <jasonwc> pts0, and if you break the VM, no big deal
    13:02 <rlaager> Another option would be to have the .md output from some script. Then it could write out BIOS vs UEFI; encrypted vs. not, etc. And we wouldn't care how many different outputs there were, because it's automated. Then just have the users click some links to get the right version.
    13:02 <rlaager> But another way to do this is to write this into the OS installer, which is hopefully happening.
    13:02 <pts0> so what happens if my main disk dies
    13:02 <pts0> do i have to go into the bios
    13:03 <rlaager> pts0: "Hints: ... ls -la /dev/disk/by-id will list the aliases."
    13:03 <jasonwc> pts0, You can install Grub to all disks. Presumably, your BIOS allows you to sort boot order, so it should just boot from the second listed disk
    13:03 <twb> One of my hats is documentation manager / tech writer, and I can definite vouch for reST/sphinx/readthedocs stack over markdown.  The expanding/collapsing block thing would either be part of the CSS/js theme, or an extension.
    13:04 <pts0> ok, updates are ALWAYS changing grub crap
    13:04 <pts0> is an update going to break it
    13:04 <twb> (sphinx is what linux kernel and python communities use these days)
    13:04 <rlaager> pts0: As a general tip... I suggest following the instructions once, exactly as written. Once you see how things work and have a working system to poke at, it will make a lot more sense.
    13:04 <pts0> ****do you want to overwrite custom grub with package maintaners...that one
    13:04 <rlaager> It's fine. Obviously say "no" to that prompt.
    13:05 <pts0> then i miss something the package maintainer wanted
    13:05 <rlaager> But to be fair, you do need to have some idea what you're doing, and be willing to take some risk. This is not the common-case scenario, as it's not a distro default. It's not distro supported. etc. etc.
    13:05 <pts0> i always hate that one
    13:05 <pts0> sgdisk     -n2:1M:+512M   -t2:EF00 /dev/disk/by-id/scsi-SATA_disk1
    13:05 <pts0> so what are all the switches
    13:06 <twb> pts0: per the manpage, it creates a new partition, number 2, from 1M to 1+512MB, on the disk with serial number "SATA_disk1"
    13:06 <twb> pts0: oh and sets its GPT UUID (i.e. "partition type") to "EF00", which is presumably the magic number for the ESP
    13:06 <pts0> ok so thats not the uuids i see when i run blkid
    13:07 *** ChanServ MODE +v mahrens
    13:07 <twb> pts0: again per the gdisk manpage, the 4-digit UUID is a shorthand for the "real" UUID
    13:07 <pts0> yeah see i have no idea what the hell that means
    13:08 <jasonwc> pts0, I think you're overthinking this.  Try it in a VM to get a feel for how things work.  If you have a decent understanding of ZFS, you should be fine.
    13:08 <twb> pts0: that's OK.  The HOWTOs on the ZOL wiki assume you already have some experience with these tools.
    13:08 <pts0> I'll go throught it and make a bunch of notes when I have time and try to install it so you guys know what an idiot thinks about so if you want to idiot proof it you can
    13:09 <twb> pts0: you just need some practice, I think.  As jasonwc says, you can do this with a toy VM to avoid risking your "real" setups
    13:10 <pts0> Honestly the complexity just makes me want to use the hardware raid...then you have people saying it's perfectly fine
    13:11 <pts0> anyway, i appreciate all the help
    13:11 <PMT> I mean, if you dislike things you don't understand, you're free to do whatever you want.
    13:12 <pts0> why do you say that
    13:12 <pts0> i know that
    13:12 <pts0> i'm not trying to be a jerk or anything
    13:13 <pts0> I'm tempted to learn this but I haven't been feeling well lately
    13:13 <twb> hardware raid is a huge pain, because you have to buy a spare rescue card in case the main card dies (because different models aren't compatible); you can't SMART to the drives to find out when they're dying; you have to run proprietary software in a RHEL chroot to manage the array, &c &c
    13:13 <jasonwc> pts0, I found it overwhelming at first too, but I wouldn't go back to using anything but ZFS for my root fs.  It's saved me so many times when something broke and I was able to fix it with a quick rollback, live, without restoring from backups
    13:14 <pts0> twb but you'd have to buy a spare hba too right
    13:14 <pts0> the thing that scares me is zfs just dying due to hardware raid
    13:14 <pts0> some people really stress that
    13:14 <PMT> that...doesn't make sense.
    13:15 <twb> pts0: if you're doing RAIDZ, then you're using passthrough HBAs
    13:15 <twb> pts0: which are 1. cheap; and 2. fungible.
    13:15 <twb> pts0: I know that if I put the disks into *any* computer with enough SAS slots, I can recover the data.
    13:16 <PMT> If you hide the multiple disks from ZFS, then yes, it can't recover from corruption, because it doesn't have any sort of redundancy even if there's a HW raid under it, but that's not really a ZFS-ism, it's just a fact.
    13:16 <twb> (When I was talking about buying a "rescue card" for hardware raid, "card" = "HBA")
    13:16 <pts0> check out my posts on reddit to see what I was looking at:https://www.reddit.com/user/5tzr/posts/
    13:16 <zfs-bot> [REDDITOR] 5tzr | Link: 6 | Comment: 35
    13:16 <zfs-bot> [ 5tzr (u/5tzr) - Reddit ] - www.reddit.com
    13:16 <PMT> no
    13:17 <pts0> PMT you're free to do whatever you want.
    13:18 <jasonwc> pts0, I think the first post from fengshui is on point
    13:18 <jasonwc> https://www.reddit.com/r/zfs/comments/bg4t3w/any_thoughtsopinionscorrections_on_this_article/
    13:18 <zfs-bot> [REDDIT] Any thoughts/opinions/corrections on this article? (https://mangolassi.it/topic/12047/zfs-is-perfectly-safe-on-hardware-raid/9) to r/zfs | 0 points (33.0%) | 4 comments | Posted by 5tzr | Created at 2019-04-22 - 17:27:15UTC
    13:18 <zfs-bot> [ Any thoughts/opinions/corrections on this article? : zfs ] - www.reddit.com
    13:20 <pts0> I'm not sure if I should trust that Perc card even in passthrough mode
    13:21 <pts0> maybe I should get another card?
    13:21 <pts0> One guy said you could flash the full perc but not the mini
    13:21 <pts0> I have the mini
    13:21 <jasonwc> Ah
    13:21 <jasonwc> I mean, you can get a 9211-8i pre-flashed with IT mode for $50 on Ebay
    13:21 <jasonwc> I've got 6 of them and they work great
    13:22 <pts0> That's what you would recommend for a Dell R720?
    13:22 <pts0> Do you have a link?
    13:22 <twb> perc 5 HBA can't do passthrough IIRC
    13:22 <twb> It can only do single-disk RAID0s, and you lost all your SMART
    13:23 <pts0> perc h310 mini
    13:23 <pts0> that's the model I have
    13:23 <twb> Last time I was on a server that came with a RAID HBA, I just paid $20 to replace it with a passthrough HBA
    13:24 <pts0> What HBA would you recommend for a R720?
    13:24 <twb> pts0: whatever the first-party one is
    13:24 <pts0> I don't know what that is
    13:24 <jasonwc> pts0, I mean, there are a ton of them on Ebay.  Here's one from a seller I've actually purchased from in the past that I trust
    13:24 <jasonwc> pts0, https://www.ebay.com/itm/LSI-SAS-9211-8i-8-port-6Gb-s-Internal-IT-MODE-ZFS-JBOD-HBA-IR-MODE-RAID/352612690487?hash=item52195aaa37:m:mbEDowx1AV5jIlfu6fiaMHg
    13:24 <zfs-bot> [ LSI SAS 9211-8i 8-port 6Gb/s Internal (IT-MODE) ZFS JBOD HBA / (IR-MODE) RAID | eBay ] - www.ebay.com
    13:24 <twb> pts0: so call your dell salesdroid and ask
    13:25 <pts0> I don't know what you mean by first party
    13:25 <twb> jasonwc: what's "IT mode" mean, there?
    13:25 <twb> pts0: "first-party" means Dell sells it to you and guarantees it'll work in their server
    13:25 <jasonwc> pts0, Each SAS port will give you 4x 6 Gbit bandwidth, so it'll give you full bandwidth for 8 drives, but if you're using HDDs and have a backplane with a SAS expander, you can use a single HBA for 24 or more drives
    13:25 <jasonwc> twb, IT mode means it's just a pure HBA, no RAID features, just pass-through
    13:25 <jasonwc> twb, full SMART access etc.
    13:25 <twb> jasonwc: ah cool
    13:26 <twb> jasonwc: that's not a shibboleth I knew, but I'll remember it for next time
    13:26 <jasonwc> twb, These LSI cards are sold under a bunch of names, and you can flash different firmware.  IR is RAID IIRC, IT is what you want for ZFS
    13:26 <pts0> and this will work with the Dell disk case?
    13:26 <jasonwc> twb, The 9211-8i are pervasive but there are newer and faster variants with SAS 12 Gb support
    13:27 <jasonwc> pts0, Dell disk case?
    13:27 <jasonwc> You mean Dell Chasis?
    13:27 <jasonwc> I think a lot of people use 9211-8i's from various manufacturers with the IT firmware.  Unless Dell is doing something to block non-Dell hardware, I don't see why it wouldn't work.
    13:28 <pts0> Hell I don't know
    13:28 <jasonwc> You can ask on Reddit.  Lots of people use these cards.
    13:28 <pts0> I thought you had to cable something to it
    13:28 <jasonwc> They're just dumb HBAs
    13:28 <jasonwc> yeah, you cable it to the backplane
    13:28 <pts0> the backplane where all the disks plug into?
    13:28 <jasonwc> with a direct attach backplane, you'll have one SAS port for every 4 drive bays
    13:28 <jasonwc> With a SAS expander, you may have one or two ports for all drives
    13:29 <jasonwc> pts0, Yes, drives attach to the backplane, and then on the back of the backplane there will be SAS ports that go to the HBA
    13:29 <jasonwc> pts0, How many drives are you going to be using?
    13:30 <pts0> well i was thinking 8, 2 mirrored for the boot/root stuff
    13:30 <pts0> but is that not what the guide says to do?
    13:31 <jasonwc> pts0, This what you have? https://www.youtube.com/watch?v=a5toVeaLqRA
    13:31 <zfs-bot> [ PowerEdge R720: Hard Drive Backplane - YouTube ] - www.youtube.com
    13:31 <pts0> i think that's the xd with a bunch of drives
    13:31 <pts0> mine just has 8
    13:31 <pts0> R720xd vs R720
    13:32 <pts0> I just have the R720
    13:32 /dim pts0
    13:32 *** DIM pts0 Ashenzari Henzell Sizzell Sequell Lantell Rotatell Eksell Cheibriados Gretell Kramell BTS zfs-bot nalpre EMIYA ladybird ldybrd ladybug ladybot ladyblurb mrscribe travis-ci Brainstorm Not-ba84 lubotu2 kenaan Svadilfari bbot2 feepbot curlbot zfs firebot pg_docbot pg_docbot crosbot zwiebelbot sd-bot sd-bot0 sd-bot1 sd-bot2 sd-bot3 sd-bot4 sd-bot5 sd-bot6 sd-bot7 32NAAB16X CIA- CIA-0 CIA-1 CIA-2 CIA-3 ChanServ Energon lpaste FBI GumbyPAN Jesdisciple KGB KGB-0 KGB-1 KGB-2 KGB-3 MentorSeeker birny checkbot cyber darcscommitbot darcswikibot dpkg dselect evalbot evil_twin factorbot fajita friendlyToaster fsbot fsbot` fsbot`` infobob ghcbot gitbot gitinfo GitHub157 GitHub106 greybot hgbot hpaste iwakura judd knoba lambdabot lamebot lisppaste lopbot pasteban perlbot pursuivant rudybot rudybot_ samba-bot sarahbot shadowpaste shbot shebang specbot tpg trungl-bot twiggz ubottu ubotu ubot2 ubot93 upstartbot uvirtbot uvirtbot~ uvirtbot` vpnHelper wikibugs IZBot Selvvir Not-63f6 logerritbot loircbot Nugs Not-c51b wrt Anaphaxeton Apic Buglouse average theos edgar-rft ManateeLazyCat MrSassyPants Roxyhart0 TheEvilPhoenix ^law^ a7a ams bazhang coldhead consolers coolack djszapi djszapi_ eagles0513875 eagles051387_ eagles0513875|2 eagles0513875| fix jaimef akkad joeoshawa jordanb jordanb_ kaushal notbrien tanasinn wolferz yates BlueOcean blass rockxloose rockxloose_ p0a rob_debian jeromelanteri epony RaycatRakittra Aethaeryn kenzo kenzo215 njsg Muimi Muimi1 iqubic Juesto bazhang Megabyte Shinigami-Sama sam_yan biffhero reisio Branes Calinou Captain_Crow CcSsNET John[Lisbeth] dorothyw johnnymacs Cheiron nunag rotorua tokeroa Devastator Error404NotFound Grum str1ngs LimCore Mgamerz WorstHumanEver Pikkachu RoyK RoyK^ antono atomx aurelien buu clint clint- cloneMX cluelessperson cluelessphone dominicdinada mpourhadi mpourhadi_ drake1 aeth fxiny haylo haylo_ heviarti heviarti_ icepick caffe j4jackj dvorakbot lich linocisco maroloccio maroloccio2 omeddragon pizzasauce rgr robottinosino ruben32 seanjohn silentwhisper xk05 |xk05| xk051 xorred thunk
    13:32 <pts0> So you think that card will work?
    13:33 <pts0> I can buy a more expensive one if it's better
    13:33 <jasonwc> you're right about the mini card
    13:33 <jasonwc> "If you're using a mini card don't attempt flash it as you'll brick the card instead configure the disks as non raid to pass them through. The mini card looks like this."
    13:33 <jasonwc> https://www.serverhome.nl/media/catalog/product/cache/1/image/9df78eab33525d08d6e5fb8d27136e95/p/e/perc-h310-mini-mono.jpg
    13:33 <pts0> this midget porn collection is very important to me
    13:33 <pts0> bad joke
    13:34 <jasonwc> " If you're using a non mini card, I'd suggest you flash it to a pure HBA using 9211-8i firmware."
    13:34 <jasonwc> Apparently the non-mini card is just a 9211-8i
    13:34 <pts0> yes i have the mini
    13:34 <jasonwc> So I don't see why buying one online wouldn't work
    13:34 <pts0> someone told me exactly what you just posted
    13:34 <jasonwc> if it makes you feel better, you can get the dell one
    13:35 <jasonwc> https://www.ebay.com/itm/LSI-9211-8i-P20-IT-Mode-for-ZFS-FreeNAS-unRAID-Dell-H310-6Gbps-SAS-HBA/253955813684?epid=19006955695&hash=item3b20f23134:g:kKkAAOSwMjpb11RL
    13:35 <zfs-bot> [ LSI 9211-8i P20 IT Mode for ZFS FreeNAS unRAID Dell H310 6Gbps SAS HBA | eBay ] - www.ebay.com
    13:35 <jasonwc> So this is just a Dell H310 flashed with the IT firmware
    13:35 <jasonwc> given that the firmware is the same, it's silly to pay more for it
    13:35 <jasonwc> You're better off to asking on Reddit if anyone had problems using a 9211-8i that was not sourced from Dell.  I suspect it'll work just fine.
    13:36 <pts0> I wonder if I do Dell firmware updates if it will brick it
    13:36 <jasonwc> What firmware updates would you need to do?
    13:36 <pts0> Dell has that automated firmware updates through the lifecycle controller, etc
    13:36 <jasonwc> Other than update the BIOS on my motherboard
    13:36 <pts0> and with an iso
    13:37 <pts0> oh it updates disk firmware too
    13:37 <jasonwc> Wouldn't that assume the disks came from Dell?
    13:37 <pts0> yeah good point
    13:37 <pts0> hell im going to try it
    13:37 <pts0> screw it
    13:37 <jasonwc> I've had good experience with UnixSurplus
    13:38 <jasonwc> They are based in CA and I've bought a few servers from them
    13:40 <pts0> Does that guide create a separate pool/mirror for the boot/root stuff or is it all one big raid6 or something
    13:41 <jasonwc> pts0, The guide can be modified to do whatever you want.  I personally use a mirror of SSDs for my root pool.  Mirrors provide better IOPS and are more efficient than raidz2 for storing small blocks (8 and 16K).  With raidz/2/3, you get the random IOPS of a single disk, but sequential reads and writes scale with the # of disks - 1/2/3 disks based on your parity selection.  Mirrors will give you the write IOPS of a single
    13:41 <jasonwc> disk and the read IOPS of the # of disks in the mirror.
    13:42 <jasonwc> pts0, Are you using just SSDs or will this system also have HDDs?
    13:44 <pts0> It will just have HDDs
    13:44 <pts0> Man I just can't understand what that guide is doing
    13:44 <pts0> I want my boot/root disks to be mirrored
    13:44 <jasonwc> I don't think you want a single raidz2 in that case.  You'll get the IOPS of a single disk and that's not great.
    13:45 <pts0> It seems like that woudl be the default
    13:45 <jasonwc> pts0, The guide can be used to create a mirror, raidz/2/3, or a singleton
    13:45 <pts0> but how
    13:45 <pts0> you have to boot off one disk
    13:45 <pts0> and if that one dies, you have to boot of the other
    13:45 <pts0> unless you just don't want to boot
    13:46 <jasonwc> pts0, so, the guide shows how to create a pool using a single disk
    13:47 <pts0> ok so then what
    13:47 <jasonwc> pts0, To simplify, zpool create -o options rpool mirror Disk1 Disk2
    13:47 <pts0> that disk dies and I'm screwed
    13:48 <jasonwc> pts0, for raidz, zpool create -o options rpool raidz Disk1 Disk2 Disk3
    13:48 <jasonwc> pts0, The guide actually covers this: "If you are creating a mirror or raidz topology, create the pool using zpool create ... rpool mirror /dev/disk/by-id/scsi-SATA_disk1-part4 /dev/disk/by-id/scsi-SATA_disk2-part4 (or replace mirror with raidz, raidz2, or raidz3 and list the partitions from additional disks). "
    13:48 <pts0> I would bet good money it wouldn't jsut boot off that second drive then
    13:48 <jasonwc> pts0, The guide assumes some knowledge of ZFS
    13:49 <pts0> if i pulled the first out
    13:49 <jasonwc> pts0, It would if you install Grub to both drives, which the guide instructs you to do
    13:49 <jasonwc> pts0, I've pulled a drive and it boots
    13:50 <pts0> So it starts the partitions off at an offset or something so you can put grub on it
    13:56 !!! irc.freenode.net: connection broken by remote peer (closed)
    13:56 !!! irc.freenode.net: connection broken by remote peer (closed)
    > The other advantage of all first-party hardware is that 14:01 /join
      #zfsonlinux
    14:01 *** services. 328 #zfsonlinux http://zfsonlinux.org
    14:01 <pts0> sgdisk     -n2:1M:+512M    ok from 1M to 512m but then this just overwrites it sgdisk     -n3:0:+512M
    14:02 <pts0> the third one overwrites the second one
    14:02 <pts0> and it overwrites the first one because it starts at 0
    14:03 <pts0> there's just nothing in the gude about it...ah well
    14:04 <pts0> maybe i'll be smarter in the future...anyhow thanks everyone
    14:04 <pts0> take care
    14:04 <jasonwc> no, the third is +512M...
    14:04 <jasonwc> so it doesn't overwrite it
    14:04 <jasonwc> first is 24K-1M, second 1M-512M, third starts at 512M
    14:04 <jasonwc> pts0, I would recommend using ZFS for your data pool to get familiar with it, and you can revisit using ZFS for a boot pool when you feel more comfortable
    14:07 <jtara> i'd agree
    14:07 <pts0> unfortunately i don't think i can
    14:07 <pts0> i can either enable hardware raid or not
    14:07 <jtara> it's best used on a small scale first if you're still learning it
    14:07 <pts0> why are there three ts
    14:07 <pts0> t1, t2, t3
    14:08 <jasonwc> pts0, You can use two disks for a mdadm mirror for your boot, and give the remaining disks to ZFS for a raidz/2 pool
    14:08 <pts0> n1, n2 and n3 are partitions but the ts are types right
    14:08 <jasonwc> pts0, mdadm should be very simple
    14:08 <pts0> lol
    14:08 <pts0> yeah i looked at that guide too
    14:08 <pts0> there is a script that supposedly makes it simple
    14:08 <jasonwc> pts0, And if you're not booting off of ZFS, you don't have to worry about kernel compatibility.
    14:09 <pts0> i think, truth is, you just have to understand all this to do a non-standard install
    14:09 <jasonwc> You can setup a RAID1 mirror using mdadm with either the Debian or Ubuntu installer
    14:09 <pts0> ok, yeah, i'll try that
    14:09 <pts0> last question though why are there three ts
    14:09 <pts0> t1, t2, t3
    14:09 <jasonwc> so, use 2 disks for your / with mdadm and LVM
    14:09 <pts0> if it's just a partition type
    14:09 <jasonwc> it'll do that for you
    14:10 <jasonwc> then I would probably do a raidz2 with the 6 disks unless you need IOPS in which case you can do 3 mirrors striped
    14:10 <pts0> truth is im' just learning
    14:10 <pts0> jsut a cheap box to play with
    14:10 <pts0> seriously though what are the three ts
    14:11 <pts0> in the guide in that partitioning section
    14:11 <pts0> t1, t2 and t3...n1, n2 and n3 are the partition numbers
    14:11 <jtara> if you're just learning, leave hw raid off, maybe even plan on two separate pools
    14:11 <jtara> so you can back up everything to one if you need to blow away the other, or something
    14:11 <jtara> plus some things are best appreciated with a couple different pools
    14:12 <jtara> personally i would only do sw raid for a boot drive, but that's probably more of a solaris perspective than linux
    14:12 <jasonwc> jtara, Why? mdadm should be well tested.  Raid5/6 has the write hole but RAID10 should be fine.
    14:14 <jtara> i'd rather have a sw raid setup because i don't trust boot disks sitting behind proprietary controllers, for the most part
    14:14 <jtara> and i feel that sw raid gives better indications of predictive failure for root mirroring
    14:14 <jasonwc> Yeah, I meant why would you trust a hardware RAID controller over a hardware RAID solution for RADI10?
    14:14 <jtara> plus zfs makes it really easy on solaris to do
    14:14 <jasonwc> "personally I would only do sw raid for a boot drive"
    14:14 <jtara> i didn't say i would, i don't think
    14:15 <jasonwc> Or do you just mean you would use ZFS for anything else?
    14:15 <jtara> i would use sw raid either with a mirror on a zpool or with something non-zfs i guess
    14:15 <jtara> back in the bad old days of sunos we used something very mdadm-ish that worked reasonably
    14:15 <pts0> -n3:0:+512M
    14:16 <pts0> does that mean to start where the last partition ended
    14:16 <jtara> oh i think i understand the misunderstanding
    14:16 <jtara> i mean, "for a boot drive, i would only use sw raid, not hw raid"
    14:16 <jasonwc> ah, got it
    14:16 <jtara> english is such a stupid language lol
    14:16 <jasonwc> pts0, Yeah, it starts at +512M
    14:16 <jtara> doesn't even have proper lambdas
    14:17 <jasonwc> Yeah, I interpreted it as I would only use SW raid for this one situation, and hardware RAID in all others.
    14:17 <jtara> ahh
    14:17 <pts0> i thought it meant to start where the last one ended and go 512M more
    14:18 <pts0> i'll play and see what it does
    14:18 <pts0> thanks guys for putting up with me
    14:18 <jasonwc> pts0, Since you seem uncomfortable with sgdisk, you can also achieve the same results with fdisk, which is definitely easier to use
    14:18 <jasonwc> pts0, But for a howto, it's better to use instructions that minimize the opportunity for error
    14:19 <jasonwc> You don't need to become an expert on sgdisk to use ZFS.
    14:19 <jtara> just don't be in a hurry to put mission critical stuff that requires all kind of stuff on it
    14:19 <jtara> play with it, destroy some pools, create some more, get a feel for it
    14:19 <jasonwc> yeah, I created several VM root pools before I did one on my real system
    14:19 <pts0> i see what the ns and ts match up now
    14:20 <jasonwc> And I had used ZFS for a non-root pool for a few years before using ZFS on root
    14:20 <pts0> it's specifying the partition number in both
    14:20 <pts0> so t4 to change the type of partition 4
    14:20 <jtara> we still don't use zfs for root here, but i'm trying to get it widely adopted for containers
    14:20 <pts0> i have nothing important to screw up so
    14:20 <pts0> meh
    14:20 <jtara> considering how many pools i'm jamming into a single physical it works pretty well
    14:20 <twb> Strictly, sgdisk is GPT sfdisk.  gdisk is the GPT fdisk.  The "s" is the non-interactive version, IIRC.
    14:21 <twb> I personally prefer libparted over g/fdisk, but the feature sets aren't identical.
    14:21 <jtara> pts0: that's an awesome place to be in!
    14:21 <pts0> well i'm also sickly, ugly and old so
    14:22 <jasonwc> jtara, Why have so many pools on a single disk?
    14:22 <jtara> our original use case had one pool per local container, as well as one "backup" pool for each container on remote block storage
    14:23 <jtara> going forward we may have multiple containers per pool but this has been the cleanest way to give them isolation so far
    14:23 <pts0> twb so sgdisk is fdisk
    14:24 <jtara> it lets each container either manage its own pool or delegates that to a backup pod
    14:24 <twb> fdisk and sfdisk are MBR; gdisk and sgdisk are GPT.  GPT is required for UEFI.  GPT is required for ≥4TiB disks.
    14:25 <jtara> plus it's mostly ssd so we're not nearly so iop constrained as long as each pool io plays nice
    14:26 <pts0> sfdisk is not fdisk?
    14:26 <jasonwc> On an unrelated topic, what's the best way for a consumer to obtain enterprise SSDs?
    14:27 <fling> Does not zfs support badblocks?
    14:27 <jasonwc> I only see them offered by 3rd party sellers that are surely not authorized retailers.  Do you have to buy in bulk?
    14:28 <pts0> the ones with s in front of them are for non-interactive?
    14:30 /join #7z
    14:30 /join #7zip
    14:48 <PMT> fling: I don't believe there's any mechanism, no
    14:50 <twb> jasonwc: what do you mean by "enterprise" SSDs?
    14:50 <twb> Different FTL?
    14:50 <fling> enterprise bytes!
    14:51 <jtara> different duty cycles, more overprovisioning, often pre burned in
    14:51 <twb> Fair enough
    14:51 <fling> jtara: what is pre burned?
    14:52 <twb> I didn't know they were A Thing, presumably because I'm not enterprisey enough to even get them offered to me :-)
    14:52 <jtara> normally ssd performance degrades over time as blocks fill
    14:53 <jtara> burning them in basically gets you to steady state performance instead of facing a steeply falling curve
    14:53 <jtara> in a lot of places consistent performance is more important than brief maximum performance basically
    14:53 <jtara> and big overprovisioning really diminishes erase cost
    14:55 <jtara> the other thing is...if you can run with big blocks on zfs and not raidz them, you make really sequential writes
    14:55 <jtara> so you can greatly minimize ssd erase cost
    14:56 <PMT> I mean, I imagine you could probably implement a badblocks equivalent assuming the region isn't in use, by permanently leaking it. :V
    14:57 <twb> Is badblocks even A Thing still?  Doesn't the HDD firmware have the equivalent internally, and when that runs out, you should throw the disk away
    14:57 <PMT> twb: in theory.
    15:32 <twb> http://ix.io/1GWp/rst  are my notes about my plan for booting w/ ZFS
    15:34 <rlaager> twb: #1 is not applicable. The initramfs will set noop on the disks in the root pool.
    15:34 <rlaager> I think "crap feature set" is a bit harsh. That was _the_ feature set not that long ago.
    15:34 <twb> OK :-)
    15:35 <twb> I haven't looked at exactly what the featureset is
    15:35 <rlaager> If you're going to use a bpool, what's the point in using refind instead of GRUB? Because you can't mirror the ESP? What type of motherboards are you using?
    15:36 <rlaager> Oh, nevermind, I misread.
    15:41 <rlaager> twb: You might be able to save some steps with this approach: 1) Always keep your FAT32-formatted zvol mounted at /boot. Mount it with "sync" for extra safety, especially in light of... 2) rsync the zvol to /dev/disk/by-id/TOSHIBA-xxxx "Whenever flashrom would run"
    15:42 <twb> ah so copy it in the other direction, basically
    15:42 <rlaager> Exactly. Then it should be "safe" to keep it mounted all the time, which is what I assume you were trying to avoid with the flash drive, which causes the dual re-mount steps.
    15:43 <twb> That would do (slightly) more writes to the USB key, but probably fine.  It would also hose any writes by the bootloader or mainboard (or other OSs) to the USB key...
    15:43 <rlaager> Would it write more?
    15:44 <rlaager> But yes, it would overwrite any changes from elsewhere. That is a downside.
    15:44 <twb> depends if the rsync blocks line up with the FAT and/or erase blocks, I guess
    15:46 <rlaager> I'm still not sure why you can't have multiple ESPs.
    15:46 <twb> You can have multiple disconnected ESPs, but you can't have them all be mirrored together without fighting the UEFI standard (AFAICT)
    15:47 <twb> If you have multiple ESPs, then things that aren't Linux that write to them will write to *one* of them, maybe the wrong one
    15:47 <rlaager> Correct. Do your systems actually write to them?
    15:47 <twb> I don't know :-)
    15:47 <twb> UEFI conformant devices are allowed to
    15:48 <twb> and IIUC refind supports it as an alternative to writing to the efi boot vars, because they're on a PROM with a very limited number of writes, and it's soldered onto the mainboard
    15:48 <rlaager> Also, to answer your question about NOP writes... if you set the checksum to one of the cryptographically secure choices, then ZFS will avoid writing blocks to disk that are the same as the existing blocks. That's the nop-write thing. That's pretty small potatoes here, though.
    15:49 <twb> Looking at your HOWTO, I must be reading it wrong, because it looks like you're turning on most features on the bpool, not the rpool?  e.g. feature@async_destroy=enabled
    15:49 <rlaager> I'm reasonably confident that my Supermicro boards do not have any features that write to the EFI partition and do not care about EFI variables. They're basically doing the "scan the disks" approach from BIOS booting, but with EFI. So for a setup like that, dd'ing them is pretty safe. Your systems may vary.
    15:50 <rlaager> You have different goals than me, so you have reached a different result. I think your approach is sound and well-thought-out.
    15:50 <rlaager> twb: The bpool has features disabled with -d, then individual features turned back on. The rpool takes the default, which is all features enabled.
    15:50 <twb> Ah, so *that's* what the -d does
    15:51 <twb> So let's see what features are actually different between bpool and rpool on a buster system...
    15:51 <rlaager> The bpool enables those features which GRUB supports, _plus_ all "read-only compatible" features, because GRUB is only reading the pool. That may change if we can get GRUB to write to the pool, even in a limited way, for the grubenv file.
    15:51 <lundman> Didn't I see a commit recently that finally lets us feature=disabled too
    15:52 <rlaager> There's also a reasonable debate to be had here about whether we should enable features "just because we can" or whether we should only able features that "matter" on the bpool.
    15:52 <rlaager> lundman: I think that exists, but I'm not 100% sure. I could use that, but this approach is safer if someone backports a newer ZFS or something.
    15:53 <rlaager> twb: multi_vdev_crash_dump (which is never used anyway), large_dnode, sha512, skein, and edonr.
    15:53 <lundman> yeps - just fresh in my mind as I cherry-picked it over last week
    15:53 <twb> I actually use ext2 for /boot quite often because YAGNI even for the journal and extents :-)
    15:53 <rlaager> twb: Oops, sorry, that was bionic, not buster.
    15:56 <twb> SHA-2 sounds important given that SHA-1 is orange since 2004 http://valaurora.org/hash.html  (damn, that link is dead today...)
    15:56 <rlaager> twb: Buster is the same as Bionic. 0.8.0 adds encryption, device_removal, obsolete_counts (part of device_removal, essentially), and bookmark_v2 (part of encryption, essentially)
    15:57 <rlaager> Neither is sha1. The sha512 feature is to allow you to use checksum=sha512 (which is SHA512/256) rather than checksum=sha256. But the default is checksum=fletcher4 anyway.
    15:57 <twb> Ah cool
    15:58 <twb> Is 0.8 likely to be "ready" in the next six months?
    15:58 <twb> at-rest FDE (without the LUKS hassle) is a "nice to have" for me
    15:58 <rlaager> From a practical standpoint, assuming you don't need device_removal, the main feature difference is large_dnode and/or encryption, with the latter being the killer difference.
    15:59 <rlaager> Yes, 0.8.0 will probably be released soon.
    15:59 <twb> Will it be possible to in-place upgrade from no encryption to encryption?
    15:59 <twb> Like turn it on and then resilver the pool?
    15:59 <FireSnake> turn it on for new data
    16:00 <FireSnake> for new datasets
    16:00 <FireSnake> not for existing
    16:00 <rlaager> Yes, in the sense that you can enable it on your pool on new _datasets_. But if you have sensitive data in the pool already, you may not want to do that, as that would leave sensitive data on the disk. You might want to wipe the disks.
    16:00 <twb> That makes sense
    16:01 <rlaager> Were you looking at Debian then?
    16:01 <twb> So even if I had e.g. /var/mail and made a new encrypted zfs and copied all the files from old-var-mail zfs to new-var-mail zfs, it wouldn't do any kinda secure erase of the old-var-mail disk blocks
    16:02 <twb> So remanence would still screw me unless I manually nwipe'd the drives or similar
    16:02 <rlaager> twb: You might want to compile the 0.8.0rc4 package from git. Here's the not-secret-but-not-linked-from-anywhere experimental HOWTO for that: https://github.com/zfsonlinux/zfs/wiki/Debian-Buster-Encrypted-Root-on-ZFS
    16:02 <twb> rlaager: oh!  I saw that page but my brain assumed it was the LUKS method still.
    16:04 <twb> Another dumb question: if I zfs send from a system without ZFS encryption, to a system *with* ZFS encryption, will the receiving host have FDE on the received snapshots?
    16:05 <rlaager> That depends on the properties inherited and such. It should be straightforward to achieve that result. Unless you need to receive the root dataset; that might be an issue. I haven't tested this, though.
    16:05 <twb> Right now my historical infrastructure looks like:   prod server → rsync → ZFS archive server → rsync → offsite DR server with FDE
    16:06 <twb> And my plan is to have end up with prod server w/ ZFS snaps → zfs send → offsite DR server with FDE
    16:06 <jtara> that can work well, zfs send/receive can work much better for some workloads
    16:06 <jtara> but for some its not that important
    16:07 <rlaager> twb: Right, so if I were you, I'd probably start by replacing the offsite DR server with ZFS encryption. That would result in: prod server → rsync → ZFS archive server → zfs send → offsite DR server with FDE
    16:07 <rlaager> That's a minimal change. Assuming that works, then convert to the end game.
    16:08 <twb> that's fair, although right now the budget prefers me to upgrade the prod server first
    16:09 <rlaager> I proposed a software change. Unless you're talking about labor budgets, there's no spending involved. ;)
    16:09 <twb> as long as the offsite host will have FDE, I don't *really* care
    16:09 <twb> rlaager: well, I gotta have enough capacity to have the old and new setups running side-by-side briefly, or be confident enough to do it in-place
    16:10 <rlaager> Sure, there's nothing wrong with doing it the other direction.
    16:10 <jtara> do you want to apply the incrementals on the remote end
    16:10 <jtara> or just store periodic full and incremental sends
    16:10 <jtara> both work, just different approaches
    16:11 <twb> The end goal is to have all the archive snapshots on the offsite host.  Right now, the offsite host only has the latest state
    16:11 <rlaager> If bandwidth is limited, it may actually be better to start with the prod server. This could avoid the need to send two full copies during the migration process.
    16:11 <twb> Right now the bandwidth is about 1Mbit/s, but hopefully that'll go to about 10Mbit/s in a couple of months
    16:12 <twb> But for initial syncs I can just give someone a USB HDD
    16:12 <rlaager> Yeah, do the prod server first. Get your zfs send | recv worked out on site, between prod and ZFS archive server. Once that's golden, then it's just a matter of converting the DR server to ZFS and doing one big send | recv.
    16:12 <twb> The offsite host is only DR ("the building burns down") anyway, so it's not a major concern
    16:13 <rlaager> For your initial sync, you can do send | recv to a pool on a USB drive, then have someone drive it to the DR server, then send | recv from USB to the DR pool. Thereafter, just do incrementals.
    16:14 <twb> yeah or even just zfs send >/mnt/usb-hdd/zfs-send.img
    16:14 <rlaager> Sure, that works too.
    16:21 <twb> What does device_removal get me?  If a disk is dying, I want to replace it while the system is running.
    16:29 <twb> FTR, https://valerieaurora.org/hash.html  was the URL I was trying for earlier.
    16:29 <zfs-bot> [ Lifetimes of cryptographic hash functions ] - valerieaurora.org
    16:34 <rlaager> device_removal allows you to remove a top-level vdev, subject to some limitations. For example, if you accidentally `zpool add` instead of `zpool attach`. Or if you want to reduce something from N striped mirrors to N-1.
    16:36 <twb> But not relevant for "disk WDC-xxx is dead and I need to replace it with new blank disk WDX-yyy" ?
    16:37 <rlaager> Correct. That's just a regular replacement that's worked since "forever".
    16:37 <twb> Yay


    18:13 <twb> https://github.com/zfsonlinux/zfs/wiki/Debian-Buster-Encrypted-Root-on-ZFS says- oh never mind.  It's written for the future, after ftpmasters move 0.8~rc from NEW to experimental.
    18:15 <sphalerite> jtara: not so much a use case as "what happens if I accidentally move the disks around badly, and how afraid should I be of it?"
    18:15 [twb tries to remember why CCM or GCM is better than plain old CBC or CTR]
    18:16 <lundman> position based ivset generation?
    18:16 <twb> Ah, they appear to do the authentication AND encryption as mode, rather than chaining CBC/OTR and a separate MAC
    18:17 <twb> Originally they were separate, which meant that everyone screwed up combining them
    18:17 <twb> And most of my textbooks are from that era :-)
    18:19 <twb> GCM was first published in 2005; CCM in 2003.
    18:21 <twb> Ohhh does 0.8 add zfs-mount-generator for systemd?
    18:21 <twb> Because that's something I've been worrying about
    18:21 <twb> (the lack of it)
    18:23 <jtara> sphalerite: basically if you totally remove a disk and then import a pool it will come up as degraded
    18:23 <jtara> and you'll need to take steps to really break the mirror so to speak
    18:25 <jtara> my security guys are really happy about tightly coupled authenticated encryption, i think that's going to be a deciding factor
    18:26 <twb> Yeah it makes sense
    18:27 <twb> I'm just 15 years behind best *current* practice :-)
    18:27 <jtara> yes well :)
    18:28 <jtara> the thing a lot of places have issues with is that pool metadata itself is not encrypted
    18:28 <jtara> volume names, properties, on-disk zfs structure metadata
    18:28 <twb> Not at all, not ever?
    18:29 <jtara> the whole point is you want a pool to be able to manage data it doesn't have the key for
    18:29 <jtara> so all the payload data itself is secure
    18:29 <twb> IIRC ecryptfs had that problem
    18:29 <jtara> well none of the structural metadata contains anything revealing
    18:29 <jtara> it's just "this chunk is linked to this node"
    18:30 <twb> Oh hang on.  When you say pool metadata, you don't mean dirents
    18:30 <jtara> right
    18:30 <twb> OK you just mean like the equivalent of LVM and md metadata -- "I'm a RAIDn array with three disks named X, Y and Z" sort of metadata
    18:31 <jtara> well, kind of
    18:31 <jtara> basically the metadata block for each leaf data block has things like its checksum
    18:31 <jtara> and where the data block itself is stored
    18:31 <twb> Are the names and snapshot times of the zvols/zfses encrypted?
    18:31 <jtara> so you might know that there is a file or volume X, which has at position Y a chunk of data Z
    18:31 <jtara> but you don't know what X is or what the data inside Z says
    18:32 <twb> Or can the attacker see e.g. there is a snapshot called illegal-porn@2007-01-01
    18:32 <jtara> snapshot and volume names are in the clear
    18:32 <jtara> so that as you apply deltas you can manipulate them
    18:32 <twb> Oh, another dumb question then
    18:32 <jtara> like, you can apply an incremental zfs send of encrypted data, then delete an old snap to do an incremental roll
    18:33 <jtara> and you never need the encryption keys to do so
    18:33 <twb> If I zfs send encrypted data to an offsite backup, does and offsite host just holds it, never "uses" it, does the offsite host *ever* have to decrypt it?
    18:33 <jtara> honestly if you have people who work with you who manage to hide useful information in pool or dataset names, more power to you, i can't get mine to :p
    18:33 <jtara> it never has to decrypt it
    18:34 <twb> that's awesome
    18:34 <jtara> you can zfs receive into that pool without the key, and zfs send out of it without the key
    18:34 <jtara> yeah
    18:35 <jtara> and you can rekey things without reencryption
    18:35 <jtara> hopefully i can take advantage of that to give my backups a specific key per time period, while keeping all the source pools with totally random keys

    18:52 <twb> rlaager: BTW, you can do "apt-get build-dep" on a directory, rather than having to copy things from debian/control Build-Depends by hand
    18:52 <twb> e.g. git clone .../zfs.git && sudo apt-get build-dep ./zfs
    19:14 <twb> 18:21 <twb> Ohhh does 0.8 add zfs-mount-generator for systemd?
    19:14 <twb> ...I can see in git that it does.
    19:32 <twb> Boy the ZOL codebase does a lot of $(MAKE) -C
    19:33 [twb waves the RMCH paper around grumpily]

    19:41 <twb> X: zfsutils-linux: missing-systemd-timer-for-cron-script etc/cron.d/zfsutils-linux
    19:41 <twb> 24 0 8-14 * * root [ $(date +\%w) -eq 0 ] && [ -x /usr/lib/zfs-linux/scrub ] && /usr/lib/zfs-linux/scrub  # Scrub the second Sunday of every month.  
    19:41 <twb> That looks pretty straightforward...
    19:42 <twb> zfs-scrub.timer: [Timer] OnCalendar=Sun/2 *-*-*
    19:43 <twb> zfs-scrub.service: [Service] Type=oneshot ExecStart=/usr/lib/zfs-linux/scrub
    19:43 <fling> Lalufu: not even in master yet?
    19:43 <Lalufu> I couldn't say
    19:43 <Lalufu> it might be
    19:43 [fling checking]
    19:44 <fling> Is not trim eating data anymore?
    19:44 <twb> Ah my math might be a little wrong, I think mine would run "once per fortnight, on sundays" rather than "once per month, on the second sunday of that month"
    19:45 <twb> OnCalendar=Sun *-*-8..14 00:24:00
    19:48 <twb> looks good to me
    19:48 <twb> "NEXT: Sun 2019-05-12 00:24:00 AEST  LEFT: 2 weeks 4 days left "

    19:54 <twb> Wow.  I discovered arc_summary, and looking at a prod ZFS pool, I can see L1ARC "Actual Hit Ratio" is 82%, where L2ARC "Hit Ratio" is only 5%
    19:54 <twb> Meaning (I think) that the L2ARC really isn't helping much

2019-04-23 ##eros-chat::

    18:24 <twb> MTecknology: so are you moving things from NEW queue still
    18:24 <twb> MTecknology: because as it happens zfs 0.8 is in there right now, wanting to move from NEW to experimental
    18:24 <twb> MTecknology: and that version fixes systemd integration AND adds full-disk encryption
    18:25 <MTecknology> that sounds like an unpleasant review
    18:25 <twb> unpleasant for you maybe but convenient for me when it's done ;P
    18:25 <twb> AIUI it's only going into experimental ANYWAY so it's not like it should be a big deal, except maybe licensing?
    18:26 <twb> I don't know where the ftpmasters keep their notes about NEW queue, though
    18:30 <MTecknology> It's in a postgresql db that's kind of unpleasant to attach to.
    18:30 <MTecknology> I don't see any notes on it
    18:47 <twb> righto
    18:48 <twb> Do you know the URL to dget stuff from NEW queue?  It's kinda hidden because they don't want people playing with it
    18:53 <MTecknology> I don't see anything available. That should probably be a new addition
    18:54 <twb> don't worry the upstream instructures are building it from a git on salsa anyway, which is near enough
    18:56 <MTecknology> Have you ever used pbuilder or sbuild?
    19:11 <twb> MTecknology: not for a decade

2019-04-24 #flashrom::

    18:43 <twb> I have a dumb question.
                I have a boring Debian x86_64 system, with a boring FAT32 ESP, and
                I want to copy /boot into the ESP anytime /boot changes.
                Somebody told me "flashrom does basically that, for more complicated hardware,
                  where the number of erase cycles is really small.
                  So it should already have OS integration to know WHEN to reflash the kernel and ramdisk."
    18:43 <twb> But I'm looking at the flashrom package and all I see is the binary itself,
                no hooks into apt or update-initramfs or anything.
    18:44 <twb> Am I looking wrong, or was my friend too optimistic?
    18:54 <twb> Ah sorry to bother you.  It seems I was confusing flashrom with flash-kernel
    19:14 <Hellsenberg> twb: I think you did :P

2019-04-24 #debian-arm::

    18:48 <twb> Hey, suppose I have a dumb system, where just dropping a new kernel and ramdisk into /boot isn't enough, but some extra flashing process has to happen afterward.  What part of Debian makes that actually happen?  I thought flashrom had a update-initramfs hook or dpkg trigger, but I don't see it.
    18:50 <hrw> twb: flash-kernel?
    18:51 [twb starts poking around the installation-guide-armhf and-]
    18:51 <twb> ah, thanks
    18:51 <hrw> flashrom is a tool to update bios chips and other flashable firmware chips
    18:54 <twb> My use case is pretty silly.  I'm actually on x86-64 UEFI, and for stupid reasons I want to just copy /boot into the ESP anytime /boot changes.  Rather than guessing /etc/kernel/postinst.d/ or something, I wanted to see see how other people were already solving this, and steal their answers
    18:55 <hrw> twb: you know that you do not need to have kernels in ESP?
    18:56 <twb> I know, but then I have to move /boot out of my main ZFS pool
    18:56 [pabs3 would just run rsync from an apt hook]
    18:59 <twb> pabs3: yeah that would be the 90% right solution.  That wouldn't cover e.g. tweaking /etc/initramfs-tools/conf.d/ and then doing an "update-initramfs -u"
    19:00 <pabs3> ack, I would only ever tweak that via apt
    19:00 <hrw> twb: maybe you need to tweak grub to read zfs?
    19:00 <twb> hrw: grub2.02/efifs1.3 can't read ZFS pools with some newer features enabled - notably ZFS-native encryption
    19:01 <twb> http://ix.io/1H3C if you want the gory details
    19:10 <hrw> twb: make non-encrypted pool for /boot maybe?
    19:10 <twb> hrw: at which point, it needs to live somewhere
    19:13 <hrw> true
    19:23 <twb> Looks like flash-kernel adds /etc/initramfs/post-update.d/ *and*
                                             /etc/kernel/{postinst,postrm}.d/, and also
          a dpkg trigger for ITSELF, and also
          an initramfs-tools hook to (I think) resolve root=UUID=X in advance,
          in case udev/libblkid isn't available at boot time.

2019-04-24 #zfsonlinux::

    19:50 <twb> OK help me out here.  I have a pool of disks and I want to take down the pool and wipe the metadata, prior to creating a new pool
    19:51 <twb> umount -a -t zfs and zpool export -a are refusing to get rid of some stuff, even though lsof says nothing is actually open
    19:51 <twb> so I went "sod it" and tried to just do "wipefs -a /dev/sda", and even THAT fails: wipefs: error: /dev/sda: probing initialization failed: Device or resource busy
    19:56 <twb> Unrelated: for a small SATA/AHCI server, is it better to use /dev/disk/by-id/wwn-XXX or /dev/disk/by-id/ata-XXX ?  The FAQ isn't very clear.

2019-04-24 #parted::

    21:32 <twb> gparted can resize the *filesystem* in a FAT32 ESP GPT partition.
    21:32 <twb> AFAICT, parted can't actually do that.
    21:33 <twb> But gparted appears to just be asking libparted to do it, so... WTF?  Is filesystem resizing limited to people without visual impairment?
    21:33 <twb> (parted used to have "resize", but current versions only seem to have "resizepart", which explicitly doesn't affect the filesystem, only the partition)
    21:35 <twb> libparted definitely has libparted/fs/r/fat/resize.c:fat_resize()
    21:35 <twb> So how do I access that functionality, without a GUI?
    21:49 <twb> A-ha!  This isn't supported by any of the normal fat packages (e.g. dosfstools), but there is a package that does *just* this, using libparted for the actual work.  http://sf.net/projects/fatresize
    21:49 <twb> Sorry I lost my temper

2019-04-24 #emacs::

    <RANTING ABOUT #PARTED>
    21:46 <npostavs> twb: https://manned.org/fatresize.1 maybe?
    21:47 <twb> npostavs: ah thanks
    21:47 <twb> That exists in Debian
    21:47 <twb> I was looking under dosfstools and similar
    21:50 <mkletzan> And it looks like fatresize is in portage as well.  Stupid me for trying to eat more healthy and get some exercise...
    21:51 <twb> It looks at a glance like fatresize was basically someone who was in the same position as me, but a couple of years ago
    21:51 <twb> And unlike me, they actually cared enough to solve it for other people
    21:52 <mkletzan> otoh the efi partition is usually small enough to copy the data elsewhere, reformat and copy back
    21:53 <twb> mkletzan: that requires me to get things like the fat size correct, and it's easy to fuck up such things
    21:53 <twb> e.g. mkfs.vfat on a 4MB disk will be FAT12 or 16 by default, and then SOME firmwares will ignore it
    21:53 <mkletzan> `-F 32` ?
    21:54 <twb> yeah, but you have to notice that and remember to use it
    21:54 <twb> "grow the thing that's already there and works" is (hopefully) harder to fuck up
    21:54 <mkletzan> agreed
    21:54 <twb> hahaha
    21:54 <twb> Note ==== You can't resize FAT32 partition lesser than 512Mb because Windows(R) doesn't work properly with small FAT32 file system. Use FAT16.
    21:55 <twb> I think they mean MiB not Mbit

2019-04-26 #systemd::

    15:18 <twb> Hey, you know how having systemd in the ramdisk is generally a Good Thing?
    15:19 <twb> And debian and arch don't do that by default, and for arch it's just a simple config option.
    15:19 <twb> On debian, is "just use dracut (not initramfs-tools)" the best way to get a systemdized ramdisk?
    15:21 <Xogium> twb: yeah that's what I do here. Except I generate the initramfs as a standard cpio archive that I embed into the kernel, via buildroot. My initramfs contains systemd as init, busybox for all the shells and utilities, and whatever deps from util-linux is required. 4 mb in size compressed in xz
    15:21 <Xogium> I'm just not sure why systemd doesn't seem to recognize that it is inside an initramfs
    15:22 <twb> Xogium: are you doing that via dracut, or are you doing some kind of hand-rolled business?
    15:22 <Xogium> I do have /etc/initrd-released inside, or forcing initrd.target on kernel cmdline would have failed spectacularly
    15:23 <Xogium> twb: no dracut, no mkinitcpio. But buildroot is able to create a cpio archive suitable for use as an initramfs with whatever packages you selected in your rootfs
    15:23 <twb> My immediate use case is I'm rolling a modern Debian Buster system with multi-disk ZFS for root (with a zfs-mount-generator), and I'm hoping dracut will be more future-proof-y.
    15:24 <twb> #systemd used to get people with confused systems because init was plain sh and / was multi-disk btrfs, which made me afraid
    15:24 <twb> I mean /bin/init in the initrd was a sh script
    15:24 <Xogium> heh I can understand
    15:25 <grawity> Arch's sh-based initramfs just runs `btrfs device scan`
    15:25 <grawity> not sure what archzfs stuff does, but it also *seems* to work fine with my pool, so I haven't really tried systemd initramfs on the fileserver yet
    15:26 <grawity> (it doesn't help that the initial 242 release completely broke systemd-initramfs for everyone)
    15:26 <Xogium> here /bin/init points to systemd, but it is very minimal, contains only one or two other things, not even networkd, since I don't actually need it, having a serial console whenever I want
    15:26 <Xogium> grawity: did it ?
    15:26 <grawity> so say the people affected by it
    15:26 <twb> Xogium: yeah I accept that it works for you, but you're being a bit more "unique snowflake" than I have in mind for my current system
    15:27 <grawity> Xogium: https://git.archlinux.org/svntogit/packages.git/commit/trunk?h=packages/systemd&id=aa6c53bf7c18ef733da2bcf3e33324dda151b39f
    15:27 <Xogium> I'm on systemd 242.0-1 and I had no problem, though I've seen people complain a LOT
    15:28 <grawity> my dev system went from 241-git to 242-git and I haven't actually booted it to Linux for two weeks anyway, so I haven't experienced this personally
    15:28 <Xogium> twb: hehe well I'm simply trying to use this new systemd.volatile=overlay for my tiny system here
    15:29 <twb> I'm not familiar with that
    15:29 <twb> I haven't really kept track since v219
    15:29 <Xogium> and I do need an initramfs for this, so I had to figure how to do it in buildroot
    15:30 <Xogium> new stuff in 242, mounts the root device read-only with a writable tmpfs on top using overlayfs, that way anything you modify there is lost after a reboot, perfet for what I plan here, making the system go back to sane default
    15:30 <Xogium> *perfect even
    15:31 <twb> Looks like I'll be on v241, at least initially.
    15:32 <Xogium> grawity: haha I made sure to reboot right after the update, just to see what it'd do, and I didn't have such problem lucky for me I imagine
    15:33 <Xogium> that being said I think someone had an even weirder issue, this is the second time I see a machine getting stuck on 'loading initial ramdisk...'
    15:37 <Xogium> anyway, for what I'm doing here, clearly using rd.systemd.unit= on cmdline is a workaround, but I'm still not understanding why it treats the initramfs as a normal rootfs without this, I also have /etc/initrd-release into it
    15:38 <Xogium> might be coming from the fact I have a fully fledged systemd in the filesystem, and not just a couple binaries and services with targets such as multi-user.target for example, I have no idea
    15:39 <Xogium> I've been trying to do the best of both worlds with buildroot in this case
    15:43 <twb> Xogium: without that, how does systemd (running in the initrd) know whether the initrd is the "final system", or if it's destined to mount /root and switch_root?
    15:44 <twb> Because you might just be packing the final rootfs into the initrd and then stopping, e.g. for a smartbulb firmware
    15:46 <Xogium> I'm not sure that's what I've been trying to find out, how does it determine to be in an initramfs at all… boucman_work found that it apparently just check for /etc/initrd-release, so I'm guessing that I have a bunch of stuff that I'd need to take out of my initramfs or something, because it doesn't react at all how I expect it to
    15:52 <Xogium> hmm
    15:53 <Xogium> okay so by examining the archlinux initramfs they just pack a lot less systemd stuff than me
    15:54 <Xogium> but they've made the default target be initrd.target in the first place, so I could also just do the same and it would work, my default target is multi-user.target because that's how buildroot makes it by default
    15:55 <twb> bootup(7) has some info about that
    15:58 <Xogium> yeah, the charts are a bit difficut to parse with my screen reader, but I think I got the basic idea of it
    15:58 <Xogium> *difficult
    15:59 <twb> Xogium: ah ouch.  I can manually convert it to graphviz-format if it helps
    15:59 [twb is sighted]
    16:00 <Xogium> hehe I've never used graph, so I'm not at all sure how helpful that would actually be
    16:00 <twb> Ruh Roh, in Debian, intel-microcode might not work with dracut
    16:01 <Xogium> well that's annoying
    16:01 <twb> amd-microcode, by comparison, explicitly supports either initramfs-tools or dracut or tiny-initramfs
    16:01 <twb> (tiny-initramfs is similar design to RH ash - bare minimum all in one C binary as /init)

2019-04-26 #debian-kernel::

    15:31 <twb> Hey, dumb question.  Does/will buster have something like ksplice, so I can get kernel bugfixes without a full reboot?  ISTR there was mainline replacement for ksplice in the news recently...
    15:34 <pabs> there isn't a Debian service producing patches yet, Ubuntu has one though
    15:34 <pabs> I haven't heard of anyone working on one either
    15:35 <twb> Oh.  I assumed that happened automagically by just having the finished kernels in /boot :P
    15:36 <twb> It looks like the low-level functionality is in mainline 4.0, shared between kpatch (RH) and kgraft (SUSE)
    15:37 <pabs> I got the impression it needs a service, not sure tho https://linux-audit.com/livepatch-linux-kernel-updates-without-rebooting/
    15:38 <twb> FTR the high-level goal is to bypass the "file a Change Request and wait 6 weeks for approval" step.  The actual reboot itself wouldn't be a big deal.
    15:38 <pabs> shame ksplice didn't get merged, this new feature looks pretty useful: https://blogs.oracle.com/linux/using-ksplice-to-detect-exploit-attempts
    15:39 <twb> pabs: AFAICT oracle bought ksplice and shut it down
    15:39 <twb> pabs: which was when RH et al spun up their own replacements
    15:40 <pabs> seems like it is still available, gratis for Ubuntu/Fedora https://ksplice.oracle.com/
    15:42 <twb> "Support for RHEL changed to a free 30-day trial for RHEL customers as an incentive to migrate to Oracle^[5]^[6] and use of the Oracle Unbreakable Enterprise Kernel (UEK) became mandatory for Ksplice on production systems.^[7]"
    15:42 <twb> So yeah, it's available on desktops but they clearly want to lock in all the serevrs
    15:42 <pabs> apparently one can build ones own patches, the tools aren't in Debian tho http://chrisarges.net/2015/09/21/livepatch-on-ubuntu.html
    15:43 <pabs> (linked from https://blog.dustinkirkland.com/2016/10/canonical-livepatch.html)
    15:44 <pabs> woops, kpatch is in Debian
    15:45 <twb> Can I do that from pre-built debian kernels, or would I have to roll my own kernels from the .DSCs?
    15:53 <pabs> kpatch-build apparently relies on building the source twice with extra options. kgraft seems to use debug info from the old/new kernels
    15:54 <twb> cool
    15:54 <twb> but kgraft isn't in debian yet
    15:55 <twb> So probably this won't be "Just Work" until debian 11
    15:55 <pabs> (also not sure if that is outdated info)
    15:55 <pabs> I think this is the first time I've seen anyone in/around Debian show interest in the livepatching stuff
    15:57 <twb> I'm just sick of $customer's admin people dicking me around
    15:57 <twb> I don't care enough to do a lot of work, but if you'd said "oh yeah just apt install XXX" then I'd be super happy :-)
    15:58 <twb> kexec used to be that simple before systemd
    15:58 <twb> so all my servers with 5 minute POSTs got nicer overnight
    16:05 <pabs> hmm, can't find the code to kgraft userspace
    16:12 <pabs> aha https://git.kernel.org/pub/scm/linux/kernel/git/jirislaby/kgraft.git/
    16:13 <pabs> oh that is the old kernel stuff
    16:17 <pabs> Gentoo's solution: https://wiki.gentoo.org/wiki/Elivepatch https://linuxplumbersconf.org/event/2/contributions/258/attachments/55/62/Elivepatch_Kernel_Summit_20182.pdf

2019-04-26 #zfsonlinux::

    12:40 <twb> Why does rlaager's Debian HOWTO use normalization=formD?  Didn't Apple used to force NFKD in HFS+, and realized it was a bad idea, and stopped doing it in APFS?
    12:41 <twb> (Specifically, the badness happens when you have paths inside a file, e.g. in a .diff or .tar, created on a system with a different normal form.)
    12:41 [twb checks the zfs manpages]
    12:49 <CompanionCube> iirc the way apple normalizes things does indeed suck
    12:49 <twb> CompanionCube: so basically the way ZFS does it, doesn't have the same problem?
    12:51 <WrongDevice> hello
    12:51 <WrongDevice> This pool uses the following feature(s) not supported by this system: 	org.zfsonlinux:userobj_accounting 	org.zfsonlinux:project_quota
    12:51 <WrongDevice> i can disable those ?
    12:51 <WrongDevice> without recreate the zpool ?
    12:52 <WrongDevice> im trying to mount my zpool created with ZOL 0.8.0-rc4
    12:52 <CompanionCube> on?
    12:52 <WrongDevice> i want to mount it on mojave
    12:53 <WrongDevice> using openzfs osx 1.9.0
    12:53 <WrongDevice> last version
    12:53 <WrongDevice> cannot be disabled those features ?
    12:53 <WrongDevice> removed
    12:53 <WrongDevice> i cannot write
    12:54 <WrongDevice> and i moved from btrfs just to write my files from mac to linux
    12:54 *** ChanServ MODE +v behlendorf
    12:54 <twb> WrongDevice: I think you mean "can I disable those features, so I can use the pool from MacOS?"
    12:54 <WrongDevice> thats why i moved to zfs inthe first place
    12:54 <WrongDevice> yes
    12:54 <WrongDevice> yes
    12:55 <WrongDevice> no
    12:55 <WrongDevice> ok yes
    12:55 <CompanionCube> you can disable those features...at pool creation time
    12:55 <WrongDevice> im confused
    12:55 <twb> I don't know if you can disable them from an existing pool.  You can definitely make a new pool with those features never turned on.
    12:55 <WrongDevice> omg
    12:55 <Selavi> I don't think so. enabling features changes the on-disk format. a ZFS without those features won't know what to do with the extra info, and you don't want to write to a pool with unknown features as it could corrupt things
    12:55 <WrongDevice> no
    12:56 <WrongDevice> ok
    12:57 <Selavi> those are both read-only compatible, so you can read from it at least
    12:57 <WrongDevice> and whats the way to create a pool
    12:57 <twb> Could WrongDevice do something like "zpool set -o featureX=disabled pool", then resilver?
    12:57 <WrongDevice> no
    12:57 <CompanionCube> twb: nope.
    12:57 <CompanionCube> WrongDevice: if you don't know that...how did you make the pool in the first place?
    12:57 <WrongDevice> zpool create -f zroot /dev/disk/by-id/id-to-partition-partx
    12:57 <WrongDevice> that
    12:57 <WrongDevice> ^
    12:58 <WrongDevice> command
    12:58 <CompanionCube> OK then
    12:58 <WrongDevice> what i need to add to that command disabling the two features i dont want
    12:59 <WrongDevice> ?
    12:59 <twb> WrongDevice: -o xxx=disabled, I think, for each of the "bad" features
    13:00 <WrongDevice> ok
    13:01 <Selavi> do you have to use the full flag name with the project prefix, or just simple name? I assume simple?
    13:01 <twb> I *think* there is also an option called something like "grub" or "portable" which tries to turn off bad things
    13:01 <CompanionCube> nope
    13:02 <WrongDevice> what i need to do for future reference to make a pool compatible with freebsd too
    13:02 <WrongDevice> and the truebsd is using zol now , all the zol features are compatible with trueos zol ?
    13:03 <twb> There's a table in the wiki somewhere of all the features and what systems support which ones
    13:04 <WrongDevice> where ?
    13:04 <twb> http://www.open-zfs.org/wiki/Feature_Flags
    13:04 <zfs-bot> [ Feature Flags - OpenZFS ] - www.open-zfs.org
    13:04 <twb> Sorry I'm a bit slow today, I'm not in my real office
    13:04 <jasonwc> twb, There is discussion about making such a flag.  It does not exist at this time.
    13:05 <twb> jasonwc: ah thanks.  I misremembered that it existed but didn't work very well :-)
    13:05 <jasonwc> WrongDevice, First off, do you need write access to this pool? If not, you can simply mount it read-only on OS X since those flags are read-only compatible.
    13:06 <twb> Is userobj_accounting similar to ext4's usrjquota?  i.e. per-UID EDQUOT block and inode caps?
    13:06 <jasonwc> WrongDevice, Relatively few features are not read-only compatible.
    13:06 <WrongDevice> yes i want to write
    13:07 <jasonwc> twb, man zpool-features says " This feature allows administrators to account the object usage information by user and group."
    13:08 <twb> I want to limit $HOME for human users to 1GB each, but I planned to do that by just giving each one a separate dataset with a cap (i.e. zfs create -o quota=1G pool/home/alice)
    13:09 <jasonwc> WrongDevice, Based on the feature flag chart, for OS X, you'll want to disable encryption, resilver_defer, allocation_class, large_dnode, project_quota, and userobj_accounting
    13:09 <lundman> depends what you run? 1.9.0 only project_quota and userobj_accounting
    13:10 <twb> lundman: 12:53 <WrongDevice> using openzfs osx 1.9.0
    13:10 <lundman> i only read 5 lines above, more is too heavy!
    13:10 <CompanionCube> but question was 'what flags for freebsd' though?
    13:10 <jasonwc> WrongDevice, The other ones are likely enabled but not active so they aren't causing issues.  Encryption won't be active unless you create an encrypted dataset.  resilver_defer is only active during a deferred resilver. Allocation classes would require adding a special vdev.  Large_dnode would require setting dnode size to something other than the default of legacy
    13:10 <CompanionCube> though the answer to that will be changing in the medium-term :p
    13:11 <jasonwc> WrongDevice, but you're getting hit by userobj_accounting and project_quota because they become active as soon as they are enabled and can't be disabled
    13:11 <jasonwc> the others can be disabled if not active
    13:11 <WrongDevice> lundman: hello
    13:11 <lundman> o/
    13:11 <WrongDevice> yeah 1.9.0
    13:11 <lundman> I only cut that this morning
    13:12 <jasonwc> lundman, The feature chart should be updated then.  It shows resilver_defer as unsupported on OS X master
    13:13 <WrongDevice> ok recreating now
    13:13 <lundman> it should
    13:13 <WrongDevice> i have to reboot to a liveusb
    13:13 <twb> WrongDevice: good luck! :-)
    13:13 <WrongDevice> yeah
    13:14 <WrongDevice>  thanks
    13:14 <lundman> good thing I added feature=disabled too
    13:15 <WrongDevice> i will no need to do disable anything else for use freebsd , i mean freebsd is too broken for my laptop atm , and too damm old graphic stack , this hackintosh works 100 times better than freebsd
    13:16 <WrongDevice> lundman: nice so this means i can create the pool here now and use it for linux
    13:16 <WrongDevice> maybe not good idea
    13:16 <WrongDevice> ...
    13:18 <jasonwc> WrongDevice, From looking at the feature chart, assuming you don't change defaults, if you disable project_quota and userobj_accounting, you should be fine on FreeBSD 12 as well
    13:18 <WrongDevice> lundman: one thing how i create a hostid spl hostid on mac ? or show i set a spl hostid like i set it on the linux kernel command line ?
    13:19 <WrongDevice> jasonwc: good to know thank u
    13:19 <WrongDevice> the table is not easy to read for me
    13:19 <WrongDevice> but i understand what i was looking for to solve my little issue with osx
    13:19 <lundman> it generates a hostid if not set. So either set it with sysctl, or just let it set for you
    13:20 <WrongDevice> lundman: i need to set the . same spl hostid for all my oses
    13:20 <lundman> because?
    13:20 <WrongDevice> i know how to do this before but i forgot
    13:20 <WrongDevice> to not have to import -f and export all the time
    13:21 <WrongDevice> im using  clover
    13:21 <lundman> just import -f on boot, no need to export
    13:21 <WrongDevice> any way to set it in clover ?
    13:21 <twb> WrongDevice: probably /etc/hostid doesn't match in your live system and real system
    13:21 <twb> WrongDevice: just copy /etc/hostid from one system to the other, so they have the same value
    13:21 <WrongDevice> i have two linux using the same hostid
    13:22 <WrongDevice> no is for osx
    13:22 <WrongDevice> that only maybe work on linux
    13:22 <WrongDevice> i dont know
    13:22 <twb> WrongDevice: ah, I misunderstood the question, sorry
    13:23 <CompanionCube> heh, you can see the results of https://github.com/zfsonlinux/zfs/pull/8641 on one of the feature tables :p
    13:23 <zfs-bot> [GitHub] [zfsonlinux/zfs #8641] rlaager: zpool-features(5) and other man page fixes | ### Motivation and Context This fixes some issues with zpool-features(5) and zfs-module-parameters(5)...
    13:24 <WrongDevice> lundman: ok how is set with sysctl ?
    13:25 <lundman> sysctl kern.hostid=
    13:25 <WrongDevice> where i can see my current spl.hostid
    13:25 <lundman> afaik
    13:25 <WrongDevice> ok
    13:25 <twb> and sysctl -a should print all the things it knows about, so "sysctl -a | grep hostid" should tell you something useful.
    13:26 <WrongDevice> hmm this number is bigger than the ones on linux
    13:26 <WrongDevice> on linux are jsut 8 chars
    13:26 <lundman> sysctl -x kern.hostid
    13:26 <twb> WrongDevice: AFAIK hostid is always a 32-bit integer
    13:26 <twb> WrongDevice: so the biggest value should be 4294967296
    13:27 <lundman> sysctl kern.hostid=0xde816bec
    13:27 <WrongDevice> nice
    13:28 <WrongDevice> yeah now i just need to use this one from osx on all the others linuxes
    13:28 <WrongDevice> thank u
    13:28 <twb> https://bugs.debian.org/595790  has a discussion of the "backstory" of hostid
    13:28 <zfs-bot> [ #595790 - The value from gethostid() should be more unique and not change when the host IP changes - Debian Bug report logs ] - bugs.debian.org
    13:29 <lundman> Fowler/Noll/Vo FNV-1a hash of your ioplatformuuidstr
    13:29 <lundman> if you want the details :)
    13:29 <lundman> (on osx)
    13:30 <twb> Is ZFS ever likely to get Zstd or LZMA2?  I'm thinking mainly of /var/log/ (write-once read-never).
    13:32 <jasonwc> twb, There's already a working PR for zstd on ZoL
    13:32 <twb> Cool
    13:33 <jasonwc> twb, https://github.com/zfsonlinux/zfs/pull/8044
    13:33 <zfs-bot> [GitHub] [zfsonlinux/zfs #8044] BrainSlayer: Support zstd compression (port of Allan Judes patch from FreeBSD) | This Patch adds zstd compression support zo ZFS ...
    13:33 <jasonwc> I was told it works fine aside from requiring more memory than other compressors
    13:33 <jasonwc> should be faster than gzip with better compression but much slower than lz4
    13:33 <twb> What abuot blake2? :-)
    13:33 <CompanionCube> originated from a freebsd diff, no? Still not merged in either, though.
    13:34 <jasonwc> yeah
    13:34 <jasonwc> Allan Jude wrote the FreeBSD one
    13:35 <jasonwc> I believe there is an issue if compressed ARC is disabled, but the option to disable compression for ARC may be removed
    13:41 <CompanionCube> twb: btw about the normalization thing, google finds good stuff like https://zfs-discuss.opensolaris.narkive.com/3NqQVG0H/utf8only-and-normalization-properties
    13:41 <zfs-bot> [ utf8only and normalization properties ] - zfs-discuss.opensolaris.narkive.com
    13:43 <twb> CompanionCube: thanks
    13:44 <CompanionCube> the difference is basically *when* the normalization happens
    13:46 <twb> Hrm, OK.  I assumed utf8only (no normalization=) meant that it'd allow non-normalized (i.e. mixed-normalization) codepoint sequences in file namse
    13:47 <CompanionCube> no normalization is nornalization=none though
    13:48 <twb> So surely that means when = never
    13:49 <twb> (unless the userland application chooses to normalize)
    13:49 <CompanionCube> (btw i meant 'diference' vs apple's method)
    13:50 <twb> ah
    13:50 <twb> That makse more sense :-)
    14:15 <rlaager> twb: normalization in ZFS preserves the bytes of the filename, in whatever form they are in. It only affects the indexing. So it won't break tarballs with a different normal form. However, it implies utf8only, which will be a problem if you unpack a tarball with non-UTF8 filenames.
    14:15 <twb> What does "indexing" mean, there?
    14:17 <rlaager> Sorry, the index of directory entries. So if you try to open("file_in_NFC") or open("file_in_NFD"), they open the same file.
    14:18 <twb> Whereas if there was no normalization, those would be different byte sequences and therefore different files?
    14:19 <rlaager> Correct.
    14:22 <twb> Righto
    14:24 <CompanionCube> and also isn't it nfd in particular because the other choices are either identical or slightly slower?
    14:29 <twb> Is the SMB and SID stuff in zfs manpages at all relevant to a linux samba AD/SMB server?  AIUI the answer is "no, that all happens inside samba and user xattrs, and is invisible to ZFS"
    14:58 <twb> rlaager: in your HOWTO, you try to keep the "OS" separate from the user/OS data.  But e.g. rpool/home and rpool/var/mail are still in separate trees.  Wouldn't it make more sense to have only 2 or 3 top levels, like rpool/OS/debian and rpool/USER/home rpool/USER/mail rpool/USER/www ?
    14:59 <rlaager> Yes, that's a perfectly reasonable approach.
    15:00 <twb> Any reason you did't already do that in the howto?
    15:00 <javashin> lundman, hi
    15:01 <rlaager> twb: I'm not 100% sure yet, and I'd like to avoid making changes that later get undone.
    15:01 <javashin> on osx there is a zfs-import service that try to import everything at boot ? if yes how i disable that ?
    15:02 <PMT> IMO rpool/.../USER/{www,mail,...} makes sense for a multi-user env, but at that point a lot of people might have .../USER on a different pool.
    15:02 <lundman> o/
    15:02 <twb> rlaager: righto
    15:02 <lundman> javashin: there is a launchctl script that run on boot
    15:02 <lundman> which executes the zpool-import-all.sh scipt
    15:02 <lundman> just comment out "zpool import" line in it
    15:02 <twb> Another thing I didn't understand: why are you making zfs's for intermediary paths, e.g. you have a "real" / and a "real" /var/mail, but you also make a /var that doesn't seem to do anything
    15:03 <javashin> what is the location of that script ?
    15:03 <javashin> and thanks by the way
    15:03 <lundman> scripts/zpool-autoimport.sh
    15:03 <lundman> but you probably meant once it is installed
    15:04 <javashin> yeah
    15:04 <lundman> one sec
    15:04 <javashin> no prob
    15:04 <lundman> the plist is in /Library/LaunchDaemons/org.openzfsonosx.zpool-import-all.plist   ... and
    15:05 <lundman> it executes /usr/local/libexec/zfs/launchd.d/zpool-import-all.sh
    15:05 <javashin> nice
    15:08 <rlaager> twb: The /var "container" is just to keep a 1:1 relationship between the dataset hierarchy and the filesystem hierarchy, such that inheritance works, avoiding the need to set mountpoints on rpool/var/*.
    15:10 <twb> But it's not 1:1 already bcause you have /root under /home in the pool, but not in the mountpoints
    15:11 <rlaager> Sure. That feels like it should be in /home. ;)
    15:14 <twb> So if I skip those empty mounts, all I have to do is set -o mountpoints on the descendants, and I'm A-OK?
    16:24 <twb> Can a zfs dataset have separate "soft" and "hard" byte limits?
    16:25 <twb> Right now on ext4, my users can go 20% over their soft quota for up to 1 week, which gives them "wiggle room", e.g. to do a "git repack" when they're over quota
    16:33 <twb> Is moving a directory tree between two datasets on the same pool and expensive operation?
    16:37 <CompanionCube> well, cross dataset 'mv' is actually 'cp+rm' if that's what you mean
    16:53 <twb> yeah I was just thinking that
    16:53 <twb> there's no way for the userland to do a move that just changes a couple of top-level dirents
    16:54 <twb> The main use case I'm thinking of is "oh I really want to move /var/foo out of /var and into its own dataset"

2019-04-29 #mailman::

    13:34 <twb> Does mailman typically store its mail under /var/mail/<listname>, or what?
    13:34 <twb> (It's been 10 years since I did any mailman and I have literally no memory of it)
    13:37 <twb> (What I have currently is just each LDAP group gets a mailbox that dovecot exposes as a separate shared namespace, because mailman 2 insisted on installing an httpd on my mail server.  IIUC mailman 3 lets me have a mail-only mailman and never install the passwords-and-CGI web UI)

2019-04-29 #zfsonlinux::

    13:16 <twb> In the olden days under solaris, you could only recv a replication snapshot to the root of a pool.  Is that still the case?
    13:16 <PMT> No.
    13:17 <twb> Cool.
    13:17 <PMT> And when was that the case?
    13:17 <twb> $coworker's memory from opensolaris-era ZFS
    13:17 <twb> My use case is basically I have 1 big server and 1 little server onsite, and I want to replicate all of <little server>'s pool onto <big server>, and then later replicate all of <big server> (including the copy of <little server>) to an offsite DR backup
    13:19 <PMT> -R will work on non-root datasets. I don't actually recall when that was the case, though I'm sure commit logs from illumos do.
    13:23 <twb> So I can do like ssh little zfs send -I @yesterday -R pool@today | ssh big zfs recv pool/little-backup/
    13:23 <twb> Maybe the "problem" was when recv gets -F...
    13:29 <PMT> twb: it'd be just -I, -R is implicit with capital I
    13:29 <twb> okey dokey
    13:29 <PMT> No, wait, I'm wrong, -I just means all snapshots between the two, -RI would be a replication of [...]
    13:29 <twb> It's been a while since I've done this stuff and the manpage is a bit overwhelming
    13:30 <PMT> Sorry, not enough caffeine, presumably.
    13:39 <PMT> (It's okay, I'm reasonably confident someone would have called out that I was objectively Wrong if I hadn't noticed. Accuracy by eventual consensus. :) )
    13:40 <twb> Let me just check who's likely to be awake... http://ix.io/1HvO
    13:41 <twb> Bad time of day for help from europeans or americans

2019-04-30 #refind::

    10:47 <twb> I notice that https://sourceforge.net/projects/refind/files/0.11.4/refind-flashdrive-0.11.4.zip/download  uses FAT12 instead of FAT32.  Is that intentional, or is that just an artefact of running "mkfs.vfat" without an -F32 on a small partition?
    11:45 <Hello71> perhaps it is more portable
    11:47 <Hello71> is it actually causing a problem for you
    11:50 <twb> fatresize and gparted (both using libparted) seem to be getting confused
    11:50 <twb> It's not clear to me how much this is the 12-bit-ness
    11:51 <twb> It's also not clear to me if UEFI requires FAT32, or FAT12/FAT16.  The Wikipedia wording makes it sound like it has to be FAT32 on internal hard disks, but has to be FAT12/FAT16 on USB hard disks.  Which is bloody stupid, but that's par for the course for UEFI specs.

2019-04-30 #dracut::

    12:50 <twb> If this is the dev channel, is there a user channel?
    12:52 <twb> I'm migrating from initramfs-tools to dracut on Debian 10 Buster, purely so I have systemd in the ramdisk, purely so my root-on-ZFS mount generators work nicer.  I want to ask some dumb questions like: does intel-microcode Just Work with dracut on Debian 10?  What's the equivalent of break=bottom?
    12:59 <twb> (By break=bottom I mean that it should mount the rootfs, but just before switch_root, drop me into a busybox ash rescue shell.  Exiting the shell would resume the boot process.)
    12:59 <twb> Also, on Debian 10, does dracut switch *back* to the initrd during shutdown, so that systemd's journal crap won't prevent my ZFS pool from being cleanly torn down?
    13:02 <twb> "rd.shell" seems to only give a shell iff the rootfs fails to mount; I haven't found one that ALWAYS gives you a shell
    13:34 <twb> Hrm, mount -t overlay support is Debian-specific
    14:54 <twb> I can see early_microcode defaults to yes, then it looks for AMD and/or Intel microcode in (by default) /lib/firmware/updates:/lib/firmware:/lib/firmware/$(uname -r)
    17:32 <twb> Why is dracut including less in the ramdisk?  bash I can sort of understand, because no busybox, but... less?
    17:32 <twb> And ls, which is just gross.  If RHEL is still using ls to enumerate files in sysconfig, I shall be very cross.
    17:35 <twb> In the dracut root /shutdown is declared as a #!/bin/sh, but it includes bashisms, so presumably it's nontrivial to just swap in busybox for all the GNU stuff and save 5MB
    18:15 <Mrfai> twb: I'm the Debian maintainer of dracut. I will look into your questions later today.
    18:15 <twb> Thanks
    18:15 <twb> Feel free to ignore any of my comments that were too grumpy :-)
    18:55 <twb> Should I worry that dracut is saying things like "dracut: zfsexpandknowledge: pool omega has device /dev/disk/by-id/ata-MB0500EBZQA_Z1M0FBG7-part1 (which resolves to /dev/sdb1)"
    18:55 <twb> Because those /dev/sdb1 resolutions aren't static and might not be the same next boot
    18:56 <twb> Also the root= looks pretty wrong: dracut: Stored kernel commandline: root=/dev/block/ rootfstype=zfs rootflags=rw,relatime,xattr,posixacl
    18:57 <twb> I'm in a live environment trying to set up the ramdisk for the first regular boot later on; it might be auto-guessing wrong because it's third-part "zfs-dracut", or it might be guessing wrong because I'm in a live environment
    19:21 <twb> Aha, this is good!  I saw debbugs comments that dracut triggers weren't A Thing yet, but it seems like in buster, they are.  Installing busybox caused dracut to rebuild.
    19:24 <twb> Does dracut understand $SOURCE_DATE_EPOCH (https://reproducible-builds.org) ?  Setting it doesn't seem to affect the timestamps as reported by lsinitrd.
    19:25 <twb> I think that must be a bug in lsinitrd, because the actual checksum doesn't change if I rerun dracut with SOURCE_DATE_EPOCH set
    19:26 <twb> except.. changing SOURCE_DATE_EPOCH doesn't then change the checksum, so... >confused<
    19:27 <twb> oh, oh, I see.  dracut without arguments writes to the wrong file (initramfs-4.19.0-4-amd64.img not initrd.img-4.19.0-4-amd64)
    19:28 <twb> Yeah, looking at initramfs-4.19.0-4-amd64.img, it clearly isn't doing the Right Thing with SOURCE_DATE_EPOCH
    19:32 <twb> apt install amd64-microcode intel-microcode did NOT trigger the dracut trigger :-(
    19:39 <twb> Even using datefudge @$SOURCE_DATE_EPOCH doesn't Just Work, because the mtimes are initialized by the kernel
    19:40 <twb> it would need a patch like this (from initramfs-tools's mkinitramfs script): http://ix.io/1HDd/bash http://ix.io/1HDe/bash
    20:44 <twb> is dracut smart enough to see add_dracutmodules+=" busybox " omit_dracutmodules+=" bash dash " and then later see another module depending on bash, and go "hey, this won't work!"
    20:44 <twb> Or will it just build it and break the next boot?
    20:55 <Mrfai> twb: the main dracut script dracut.sh is a bash script, so you always need bash in the initrd.
    20:56 <twb> dracut.sh *builds* the ramdisk
    20:56 <twb> it shouldn't affect what runs *inside* the ramdisk, which should be mostly systemd
    20:56 <twb> (right?)
    20:56 <Mrfai> In dracut there's no break=bottom. See dracut.cmdline for the vaild options: rd.break={cmdline|pre-udev|pre-trigger|initqueue|pre-mount|mount|pre-pivot|cleanup}
    20:56 <twb> ah rd.break=pre-pivot should be close to break=bottom
    21:00 <Mrfai> yes, you are right. No bash scripts are executed inside the initrd.
    21:02 <twb> Mrfai: /etc/shutdown inside the ramdisk contains bashisms, though, and dracut seems to be bin/sh -> bash inside my ramdisk.  I'm not sure if that's just traditional RH laziness.
    21:03 <Mrfai> Do you mean dpkg triggers for dracut? Currently there's one trigger. which IMO is not perfect. But it's not easy do detect when to rebuild the initrd, after some packages on the host have changed.
    21:04 <twb> Mrfai: the one that makes me especially cranky is (AFAICT) a dracut rebuild isn't triggered by installing dracut-config-generic
    21:05 <twb> Which seems like the most important case-  "I'm about to transplant my disks to a new computer, so make the ramdisk portable"
    21:06 <Mrfai> I'm not a trigger expert. But I would like to see better trigger support in dracut. Do you know how to write such a trigger?
    21:06 <twb> I know vaguely
    21:07 <twb> AIUI basically you say "dear dpkg, if any file under /etc/foo changes, tell me".  And what dpkg actually does is run your postinst with some options
    21:08 <twb> I have a config package that removes "weekly" and "yearly" words from /etc/logrotate.d files, so I register my own trigger for that tree.  In my case, it's idempotent, so I don't actually bother to check the args properly
    21:08 <twb> In your case I think you'd register a trigger for something like /usr/lib/dracut/modules.d/, but I'm definitely not an expert
    21:09 <Mrfai> For dracut a trigger that watches /etc/dracut.conf.d and /etc/dracut.conf should be the most important.
    21:10 <twb> I think /etc/dracut.conf itself would be moot, because only you yourself manage that (apart, possibly, from diversions)
    21:10 <twb> but I agree re /etc/dracut.conf.d/
    21:12 <twb> It's about 2 hours past my bedtime, so I have to go.  I should be back tomorrow, though.  If I forget and you want to talk packaging, I'm always in #debian-au on OFTC.
    21:12 <twb> Thanks for your help so far

2019-04-30 #debian-au::

    13:36 <twb> pabs: hey, you seem to know all the debian ML discussions.  Do you know whether there's a general plan to switch Debian to systemd and/or dracut in the ramdisk?
    13:43 <pabs> dracut BoFs have happened several times at recent DebConfs IIRC. not sure when/if that will happen though
    13:45 <twb> The driver for me is that, basically, upstream systemd doesn't support systems with non-systemd ramdisks
    13:45 <twb> Which mostly manifests in obscure edge cases, such as:
    13:45 <twb> 1. multi-disk btrfs doesn't work properly with systemd is not the ramdisk; systemd tries to mount ALL the nodes, not ANY of the nodes
    13:46 <twb> 2. journald keeps /var/log/journal open, which prevents the root (and its underlying disks) from being torn down cleanly during shutdown, unless the OS switch_root's *BACK* into the ramdisk during shutdown
    13:46 <twb> These were problems I encountered as at systemd v215 in Jessie, so *might* be moot nowadays
    13:53 <rjsalts> DKIM works too for being able to validate From: header is legit
    13:54 <pabs> twb: not sure about those issues, I suggest testing buster (in a VM perhaps) and then bringing it up on debian-devel
    13:56 <twb> Well, right now my attitude is basically "fuck it, let's try dracut"
    13:56 <twb> I don't normally use it because 1. it pulls in 50MB of perl Depends and 100MB of cryptsetup/luks/mdadm/dmraid/lvm2 Recommends; and 2. it doesn't do boot=live OOTB
    13:56 <twb> Although I just discovered a Debian-specific patch which does 50% of boot=live as "rootovl"
    13:57 <rjsalts> I don't think unstable is using dracut
    13:58 <rjsalts> but I don't think I have problems with root unmounting cleanly on shutdown
    13:58 <rjsalts> not sure on the btrfs, as I'm using xfs as my fs of choice
    14:02 <twb> rjsalts: I'm not suggesting dracut is the default _yet_
    14:02 <twb> I'm asking if that's a goal debian has
    14:05 <pabs> I believe the dracut maintainer wants it to be the default yes
    14:05 <pabs> not sure about the kernel team
    14:12 <rjsalts> I was getting suggestions to move to plymouth last time I ran apt update on unstable, being dragged in by recommends

    14:36 <twb> I hate that dracut is one big bash script, just like initramfs-tools
    14:36 <twb> Like, surely by now you can use python or perl
    14:38 <twb> It's full of shit like if ! [[ ${KERNEL_INSTALL_MACHINE_ID-x} ]]; then exit 0; fi
    14:38 <twb> Like... do you even?
    14:39 <twb> [[ $dracut_rescue_image != "yes" ]] && exit 0   implies they're running it with error handling disabled
    14:39 <twb> So if anything goes wrong, you just ignore it, instead of stopping and asking the user to fix you
    14:39 <twb> lulz they're adding the exit codes together, so it'll wrap around
    14:42 <twb> lulz dracut.css isn't a CSS file
    14:42 <twb> dracut.asc appears to be asciidoc, rather than a GPG signature
    14:43 <twb> all the bash scripts in dracut are setting LANG=C instead of LC_ALL=C, so if e.g. I have LANG=en_AU.UTF-8 LC_CTYPE=fr_FR.UTF-8, it won't remove the latter
    14:45 <twb> Why the fuck does dracut have its own logfile
    14:57 <twb> https://bugs.debian.org/753752  grrrr
    15:08 <twb> # This is kinda legacy -- eventually it should go away.
    15:08 <twb> case $dracutmodules in ""|auto) dracutmodules="all" ;; esac
    15:08 <twb> grrrrrr
    15:08 <twb> (FTR, initramfs-tools is just as bad)
    15:08 <twb> literally the only reason I haven't just replaced initramfs-tools in debian is I don't want to have an argument with all the people RELYING on its bugs
    15:17 <rjsalts> initramfs-tools is debian native, yes?
    15:17 <twb> yes
    15:17 <twb> It's really really old
    15:17 <twb> busybox was also written by debian for initramfs
    15:18 <rjsalts> yeah, I think I remember the transition to it in etch
    15:19 <rjsalts> from some even more kludgy collection of shell scripts
    15:20 <twb> as a trivial example, in initramfs-tools cryptsetup scripts, you get 3 goes to get your passphrase right.  After that, it just hangs.
    15:21 <twb> like, enough that ctrl+alt+del even won't help
    15:21 <rjsalts> why 3?
    15:21 <twb> because that's how many were coded into the script
    15:21 <rjsalts> not a while loop?
    15:22 <twb> correct
    15:22 <twb> presumably because they didn't want to trigger an infinite busy-loop if /dev/tty was wonky
    15:23 <rjsalts> couldn't you put a test for that inside the while with a continue or whatever?
    15:24 <rjsalts> can you run systemd itself in the initramfs?
    15:24 <twb> rjsalts: initramfs-tools doesn't support systemd init
    15:24 <twb> rjsalts: dracut ONLY supports that, which is why it's what I'm trying to use today
    15:25 <rjsalts> is a simpler system than dracut that worked with systemd possible?
    15:26 <twb> it's theoretically possible
    15:26 <twb> I don't know if anyone has done it.
    15:26 <twb> tiny-initramfs exists but it is using its own C program a la RH's old pre-systemd "nash" ramdisk, where e.g. the NFS syscalls and baked into the stand-alone /init program
    15:27 <twb> Actually, I know someone in #systemd who is doing a non-dracut systemd ramdisk, but they're doing very Unique Snowflake stuff
    15:27 <rjsalts> busybox does that on initramfs-tools initrds?
    15:27 <twb> rjsalts: not to the same extent
    15:28 <twb> busybox uses klibc-utils for nfsmount and ipconfig
    15:28 <twb> And when busybox ash is told to e.g. run "ping", it'll look for /sbin/ping, and run that as a separate process, even if it's a symlink back to busybox.
    15:28 <twb> That's configurable in the busybox compile-time config, but Debian has told it to behave that way
    15:28 <twb> CONFIG_PREFER_APPLETS or something
    15:29 <twb> CONFIG_FEATURE_PREFER_APPLETS=y
    15:32 <twb> The main things needed for systemd in the ramdisk is for default.target to be a symlink to initrd.target (not multi-user.target), and for /etc/machine-id and friends to be copied in
    15:32 <twb> The main things needed for systemd in the ramdisk is for default.target to be a symlink to initrd.target (not multi-user.target), and for /etc/machine-id and friends to be copied in
    16:29 <twb> rjsalts: oh btw re plymouth, at least some systems are incapable of correctly prompting for recovery from a fsck problem, unless plymouth is in the ramdisk
    16:30 <twb> rjsalts: even if it's only plymouth's pseudo-text version
    16:30 <twb> I *think* I saw that on Ubuntu, but I'm not too sure.
    16:44 <themill> «scrub /dev/sda3» nom nom
    17:30 <twb> dracut includes bash in the ramdisk
    17:30 <twb> SAVAGES
    17:30 <twb> And *not* busybox at all, which is bullshit, because it's super handy for recovery
    17:30 <twb>   lrwxrwxrwx 1 root root    4 2019-04-29 19:26 loginctl -> true
    17:30 <twb> lulz
    17:33 <twb> This is dumb, they're including less and GNU cp, mv, ls, sed, &c
    17:33 <twb> 6MB instead of 1MB

    19:01 <twb> haha origin is <NULL> on my bodge-arse repos
    19:29 <twb> dracut doesn't understand SOURCE_DATE_EPOCH
    19:29 <twb> so e.g. a Debian Live ISO built using dracut instead of initramfs-tools will always be irreproducible

2019-04-30 #zfsonlinux::

    15:25 <twb> So when I screw up making a pool, I want to tear down the existing pool before making a new one.
    15:25 <twb> "zpool export omega" and "zpool export -f omega" both fail with "umount: /mnt: target is busy.  cannot unmount '/mnt': umount failed"
    15:25 <twb> lsof says it's not in use.
    15:26 <twb> "zfs umount -a" exits happily.  "umount -a -t zfs" gives the same error as "zpool export"
    15:31 <twb> Also when I try "wipefs -a /dev/sda", I get "wipefs: error: /dev/sda: probing initialization failed: Device or resource busy"
    16:10 <twb> OK, after a reboot, the pool is auto-imported, but I can "zpool export" it at least
    16:10 <twb> And then I can wipefs the underlying drives

    18:41 <twb> Oh boy, I forgot how slow dpkg is when you have to wait for an actual disk
    18:42 <twb> (I usually use dpkg with --force-unsafe-io, on tmpfses, so the fsync spam is nopped away)
    18:46 <nils_> yeah it's very slow especially with ZFS or on spinning rust, I was thinking of maybe doing a snapshot before and after and then disabling fsync() with force-unsafe-io.
    18:47 <twb> The case that was screwing me before (on ext4) was collecting spamming randomized writes to one LV, and dpkg installing something to another LV on the same disks
    18:48 <twb> The syncs at ~200ms intervals prevents the kernel from linearizing the RRD writes for more than that, so the entire OS crashed to a halt and had to be hard rebooted
    18:49 <twb> I "solved" that by telling collectd to do its own RRD buffering in RAM for up to 30min, and to stagged the individual flushes to disk for good measure
    18:50 <twb> Which just meant that if you look at the pretty performance graphs, they're up to 30 minutes behind real-time
    19:01 <rlaager> twb: You might want to set sync=disabled while running dpkg.
    19:01 <twb> is that per dataset, or per pool?  /me checks manpage
    19:03 <twb> I mean, to be fair, dpkg is doing it out of entirely reasonable paranoia.  dpkg doesn't know when the proprietary HDD controller firmware is going to wig out
    19:04 <rlaager> The history on this is long and complicated. See at least this, and everything it links to, including the references to the whole ext4 file renaming thing: https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=605384
    19:04 <zfs-bot> [ #605384 - d-i should use dpkg --force-unsafe-io to optimize installation time - Debian Bug report logs ] - bugs.debian.org
    19:05 <rlaager> The sync= property is per dataset, but of course is inherited like everything else, so you could set it on the pool and just let it inherit down through everything.
    19:06 <rlaager> I haven't tested an install both ways, so I'm not sure how much it matters. I'm assuming that debootstrap does NOT use dpkg --force-unsafe-io, but I don't actually know. I could use eatmydata in the HOWTO, but that name is scary, and it's another dependency.
    19:07 <rlaager> I'd like to set sync=disabled in the HOWTO, but I'm afraid someone might miss the later step to turn it off, and then their data is at risk.
    19:07 <twb> eatmydata is a huge pain and force-unsafe-io is a better replacement.
    19:07 <rlaager> AFAIK, force-unsafe-io only addresses dpkg's syncs, not those from maintainer scripts and things they call, though.
    19:08 <twb> debootstrap is just a crappy bash script; it will default to calling dpkg if you run it on a system with dpkg, but I don't know if you can pass options to it.  Obviously you need some of the rootfs to be present before you crate  /etc/dpkg/dpkg.cfg.d/10-racing-stripe
    19:08 <twb> rlaager: correct, but dpkg sends a minimum of 2 syncs per directory (it used to be 2 per file, but it changed several times to work around bugs I found in dpkg-on-btrfs, and I lost track)
    19:09 <rlaager> Anyway, hopefully one day this will be in the installer, which can safely use sync=disabled for speed without the risk of a human forgetting to undo it. :)
    19:10 <twb> --extractor=ar is probably enough to bypass the syncs when using debootstrap
    19:10 <twb> rlaager: BTW what does devices=off do in your howto?  I read the manpage but I didn't understand it
    19:11 <rlaager> twb: devices=off == nodev
    19:11 <twb> oh, I see
    19:11 <twb> So really nodev,nosuid,noexec should be set on basically everything except like /usr
    19:12 <rlaager> It's the one "security" type thing I left after I removed all the nosuid/noexec stuff. These days, nobody really needs device nodes.
    19:12 <rlaager> If you dig through the history, you'll find more nosuid/noexec. I removed those because they break random things, like Postfix (which chroots into /var so needs at least exec).
    19:12 <twb> I wish Kees Cook would just make linux default to all three so you had to mount the rootfs with -o exec
    19:13 <rlaager> Yeah, you're clearly the target audience for the old approach. :) Dig through the history of that wiki page (a git checkout is easier than the web interface for that).
    19:13 <twb> no worries
    19:13 <pink_mist> 11:11 <twb> So really nodev,nosuid,noexec should be set on basically everything except like /usr <-- uhm, I would like the stuff in /bin and /sbin to continue working
    19:13 <twb> pink_mist: handwave
    19:14 <pink_mist> also I would like to be able to compile and run code from my home dir
    19:14 <twb> I do get people whinging occasionally because they can't run scripts from /home
    19:14 <rlaager> To be clear, I was setting the restrictions only on /var, then unsetting on /var children as needed.
    19:15 <rlaager> Well, or something like that. You could exec from /home.
    19:15 <twb> OTOH most of my end users are detainees (convicted or remanded into custody)
    19:17 <twb> noexec doesn't actually stop you using things like bash, python, tcc.  It just stops ./a.out.  It's about as hard to break out of as chroot(2).

    19:33 <twb> rlaager: have you tried zfs-dracut on Debian?
    19:47 <jtara> rlaager: remember that even if sync=disabled, you still have a guarantee of data durability at 5 second intervals (by default) and stronger ordering guarantees than sync=standard
    19:47 <jtara> though
    19:47 <twb> plus obviously a COW
    19:47 <jtara> i would ask why dpkg is issuing sync writes
    19:48 <jtara> twb that doesn't change at all though
    19:48 <jtara> you always cow.  cow cow uber alles.
    19:48 <twb> It's basically paranoia because dpkg doesn't have any confidence that stuff is actually hitting a disk.
    19:49 <twb> Because like, for all they know the rootfs is a fuse driver on top of iSCSI on top of IP-over-pigeon
    19:49 <twb> You can argue that spamming syncs doesn't help, but I can totally understand how they got there
    19:50 <jtara> a lot of programs abuse fsync, they really do
    19:50 <jtara> but it shouldn't be that damaging if you avoid indirect sync and rmw
    19:50 <twb> Obviously what you really want is for dpkg to unpack to a purely functional filesystem like zypperfs :P
    19:50 <jtara> well that would be nice too :)
    19:51 <jtara> and a kitten
    19:53 <jtara> anyway it's one reason why i recommend a slog so much, they help an unbelievable amount when you have apps spamming fsync
    19:53 <jtara> otherwise if you have async writes to a file, and then an fsync to seal it off...all those async writes get committed to the pool as indirect sync
    19:53 <jtara> and that really toads the wet sprocket
    19:54 <twb> Is that tanker jargon?

    20:05 <twb> jtara: re "why does dpkg even sync?" Message-ID: <19699.43281.82329.482744@chiark.greenend.org.uk> is a pretty good snark (from rlaager's earlier link)
    20:08 <jtara> ah thanks
    20:09 <jtara> i'm of the opinion that everything in the universe should do writeahead logging, but reality hasn't caught up :p
    20:11 <twb> I use sqlite WAL and NFSv4 occasionally bricks it
    20:11 <twb> Although obviously that's NFSv4's fault
    20:12 <jtara> yeah i could see that
    20:12 <jtara> i'm on this massive kerberized nfs rollout right now
    20:12 <twb> only sec=sys here
    20:12 <jtara> credential rotation and management ends up being a colossal hassle
    20:12 <jtara> we deal with PCI, PII and every other TLA i can think of so
    20:13 <twb> I have given up on non-AD krb as too much hassle
    20:13 <jtara> yeah i'm letting our AD team manage that end of things
    20:13 <jtara> they just give us an OU to manage under
    20:13 <twb> cool
    20:14 <twb> I did a samba 4.0.0 AD rollout that was.. exciting
    20:14 <twb> I had to hire one of the samba core devs to help me
    20:15 <jtara> my sympathies
    20:16 <jtara> i'm stuffing gssproxy into a container and ducting auth stuff to it through the pachinko machine that is gss
    20:17 <jtara> so that we can make it generally available on kube/docker
    20:18 <twb> http://cyber.com.au/~twb/tmp/docker.pdf
    20:18 <jtara> tbh the messy part is when you have the same keytab on a few hundred hosts and you need to handle automatic rotation cleanly
    20:19 <jtara> without interruption
    20:19 <twb> Did you know pachinko machines were originally based on a device used to demonstrate the normal distribution to stupid politicians
    20:19 <jtara> hah, that's great
    20:20 <twb> https://www.youtube.com/watch?v=AUSKTk9ENzg  (sorry, I can't find the wikipedia link easily)
