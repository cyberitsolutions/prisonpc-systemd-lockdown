GOAL: Alpha and Omega are running Debian 10 Buster.


Checklist
=========

- **TODO** choose hardware

  Omega & Omega-Understudy will remain on the good supermicro servers they're already on.
  They will get MORE RAM.
  They will get small SSDs to make ZFS work better.
  They might have their HDDs replaced.

  For initial experiments (the "learning server"),
  a temporary system has been built from old parts.

  Alpha & Alpha-Understudy hardware remains undecided at this time.
  (Currently on sucky 3U whitebox servers.)

- **TODO** buy more **ECC** RAM for omega & omega-undertudy
- **TODO** buy small SSDs for omega & omega-understudy (for SLOG ZIL, and *maybe* for L2ARC)

- **TODO** get Debian 10 to boot with the rootfs on ZFS
- **TODO** get Debian 10 to boot with /boot also on ZFS (i.e. make a ZFS EFI driver).

  This will let us give the *entire* disk to ZFS, and have just a simple refind-based ESP on a USB key.
  (We won't be able to use stock refind, we'd have to have a ZFS.EFI from somewhere else.)

  This is not comforting:
  https://github.com/zfsonlinux/zfs/commit/7f35d649253c29da8852ae105980a86d8ee7ee2a



Plan
====
* New-Alpha does networking stuff.

  * firewall, DNS, ntpsec, &c
  * nsd3 (authoritative DNS?)
  * fail2ban again?

* New-Omega does everything else.

  * MORE RAM!  (ideally 64GB to 128GB)
  * VT-d & VT-x
  * 1 minimum-size SSD for ZIL SLOG (NVMe would be nice, but AHCI is OK)
  * 1 minimum-size SSD for L2ARC (NVMe would be nice, but AHCI is OK)

  * UEFI and refind, *NOT* BIOS/extlinux/grub

  * Convert omega-understudy and alpha-understudy and cyber-offsite to
    be based on ZFS send/recv.

  * Buster, not Stretch

  * Use dracut instead of initramfs-tools ???

    The benefit is a "more systemd-y" ramdisk.
    The default Debian ramdisk is very old and based on buggy sh scripts.
    The systemd model is that systemd should be IN THE RAMDISK, too.
    The systemd model ALSO goes BACK into the ramdisk during the shutdown process.
    This is particularly useful for systems with ZFS or btrfs rootfs.

  * ZFS on Linux (ZOL) for archive/RAID
  * Samba AD (accounts/LDAP), SMB3.2 file sharing
  * postfix/dovecot/(mailman3?)
  * strong apparmor/systemd lockdown, NOT containers/VMs
  * icinga3 (replaces nagios), collectd/graphite

  * alloc (mariadb+php7+apache) in a VM, because we don't trust alloc.
    KVM, libvirtd, libvirt-daemon-driver-storage-zfs?

  * rsyslog (logserv) + journald
  * ssh/sftp gateway

  * IRC (eventualy replace with matrix/riot.im)

  * nginx, gitit

  * fail2ban for ALL services (SSH might continue to use the in-kernel IPS as well/instead)
  * letsencrypt for all services
  * gitolite for /srv/vcs

  * configuration management? (ansible / salt)
  * no squid
  * no cups (everyone has to install cups on their laptop)
  * motion (camera)
  * apt mirror (or NBN?)

  * start actually doing IPv6 and nftables, not just IPv4 and iptables


How do we boot ZFS?
-------------------

* ZFS wants to be given whole disks, not partitions.
  On Linux, benefits are

  1) ZFS will auto-set the disk's I/O scheduler; and
  2) ZFS will auto-partition replacement disks.

  UPDATE: <rlaager> twb: #1 is not applicable. The initramfs will set noop on the disks in the root pool.

* UEFI (or BIOS) support for /boot on ZFS exist, but do not support all ZFS features.
  Therefore, *either*

  1) the main ZFS pool is limited to a crap feature set; or
  2) /boot lives somewhere else (not the main ZFS pool)

  UPDATE::

    15:34 <rlaager> I think "crap feature set" is a bit harsh. That was _the_ feature set not that long ago.
    15:35 <twb> I haven't looked at exactly what the featureset is

  UPDATE: here is the featureset that zfs_x64.efi from efifs 1.3 (= grub-2.02-143-g51be3372e) supports::

      /*
       * List of pool features that the grub implementation of ZFS supports for
       * read. Note that features that are only required for write do not need
       * to be listed here since grub opens pools in read-only mode.
       */
      #define MAX_SUPPORTED_FEATURE_STRLEN 50
      static const char *spa_feature_names[] = {
        "org.illumos:lz4_compress",
        "com.delphix:hole_birth",
        "com.delphix:embedded_data",
        "com.delphix:extensible_dataset",
        "org.open-zfs:large_blocks",
        NULL
      };

  Comparing that to what rlaager is doing in the HOWTO, the zfs create
  args aren't directly diffable, because in the bpool they're using
  "-d" to turn off most things, then "-o" to turn some things back on.

  rlaager worked it out for me::

    15:53 <rlaager> twb: multi_vdev_crash_dump (which is never used anyway), large_dnode, sha512, skein, and edonr.
    15:53 <rlaager> twb: Oops, sorry, that was bionic, not buster.
    15:56 <rlaager> twb: Buster is the same as Bionic.
                    0.8.0 adds encryption, device_removal, obsolete_counts (part of device_removal, essentially), and bookmark_v2 (part of encryption, essentially)

    15:52 <rlaager> There's also a reasonable debate to be had here about whether we should enable features "just because we can" or whether we should only able features that "matter" on the bpool.
    15:53 <twb> I actually use ext2 for /boot quite often because YAGNI even for the journal and extents :-)


* UEFI ESP is a small (~4MB) FAT32 partition, that must exist *somewhere*.
  It can't reliably be mirrored, so Cyber Best Current Practice is to make it a completely generic refind.img USB key, mounted inside the case.
  Yes, it's a SPOF, but it's a *read-only* device, and if it breaks, it's a simple matter to "cp refind.img /dev/sdz" to make a new USB key.

  Refind doesn't have a ZFS.EFI driver, but it can use grub2's zfs.mod, recompiled for EFI by the efifs project.
  https://efi.akeo.ie/downloads/efifs-1.3/x64/zfs_x64.efi

  rlaager's design would then work like this:

  * ESP (w/ ZFS driver) →
  * bpool (small, feature-limited ZFS pool just for /boot) kernel + ramdisk →
  * rpool (large, full-feature ZFS pool for everything else) rootfs et al.

* Instead of rlaager's design, we propose to store a copy of /boot on the ESP itself.

  Whenever flashrom would run (on initramfs compile / kernel install?)::

      mount -o remount,rw /boot/efi
      rsync --exclude=efi /boot/ /boot/efi/debian/
      mount -o remount,ro /boot/efi

  Also, to preserve the old model of "just dd this image to a new USB
  key", take a backup of the entire USB key *back* to a zvol inside
  the main pool, e.g. ::

      # The --inplace is to avoid COWs on NOP blocks;
      # I think there is a new ZFS feature that can automatically skip NOP writes?
      rsync --inplace /dev/disk/by-id/TOSHIBA-XXXX  /dev/disk/by-uuid/XXXX

  This ensures both /boot and the full ESP disk get ZFS snapshotted and ZFS send-ed to the usual backup places.

  * UPDATE::

        15:41 <rlaager> twb: You might be able to save some steps with this approach:
                1) Always keep your FAT32-formatted zvol mounted at /boot. Mount it with "sync" for extra safety, especially in light of...
                2) rsync the zvol to /dev/disk/by-id/TOSHIBA-xxxx "Whenever flashrom would run"
        15:42 <twb> ah so copy it in the other direction, basically
        15:42 <rlaager> Exactly. Then it should be "safe" to keep it mounted all the time, which
                        is what I assume you were trying to avoid with the flash drive, which causes the dual re-mount steps.
        15:43 <twb> That would do (slightly) more writes to the USB key, but probably fine.
        15:43 <rlaager> Would it write more?
        15:44 <twb> depends if the rsync blocks line up with the FAT and/or erase blocks, I guess

        15:43 <twb> It would also hose any writes by the bootloader or mainboard (or other OSs) to the USB key...
        15:44 <rlaager> But yes, it would overwrite any changes from elsewhere. That is a downside.


  * Why not just put /boot on the normal pool and use grub zfs.mod driver?
    Because then we wouldn't be able to use sexy new ZFS features until GRUB gets support for them, and
    grub isn't updated very often.
    (Currently I'm just *assuming* those features are important enough to be worthwhile.)

  * Why not just use rlaager's bpool and rpool?
    Because then we'd have to handle disk partitioning ourselves and I/O scheduling, instead of letting ZFS manage it.
    Because we hate grub (and update-grub and os-prober) and like refind, and getting the ZFS driver from grub into refind (via efifs project) is an extra hassle.

  * Why not just netboot?
    Because then we can't boot until the PXE server is running.
    Because we also want to use this design *on* the PXE server itself.

  * Why not just mount the ESP as /boot?
    Because then the ONLY copy of /boot is on the USB key, which is a SPOF.

  * Why not use BIOS instead of UEFI?
    Because the grub ZFS driver is just as limited on BIOS as on UEFI, i.e. it wouldn't help.
    Because we would also still need somewhere to store the stage0 and stage1, i.e. a disk whose partitioning is managed outside of ZFS.

  * Why not mirror the ESP so it's redundant?
    Because it's more failure-prone than just reflashing the SPOF USB key.

    At a minimum, we'd need to use md metadata=0.9 (so the md metadata lives at the END of the partition), and
    we'd need to make the FAT filesystem slightly smaller than the partition its on
    (so that if/when e.g. refind or the mainboard UEFI firmware writes to the FAT, it will never overwrite the md metadata).
    And even once that's done, the GPT partition tables would not be md mirrored, so if they ever changed, they'd need to be manually kept in sync.

    Possibly we could work around that by md mirroring the entire USB
    keys, with metadata=1.2, so that the disk layout would be
    "[GPT1][md metadata][FAT][blank][GPT2]".  But UGH.


ZOL 0.8~rc4?
------------

* ZOL 0.8 adds a systemd generator (zfs-mount-generator).
  This makes /etc/fstab obsolete and allows mounts to be done ENTIRELY inside ZFS.
  Without this, ZFS and systemd fight, and you have to jump through hoops to work around it.

* ZOL 0.8 adds native ZFS full-disk encryption.
  We want this for the offsite host.

  If we do it for new omega itself, then

  1) the offsite host *NEVER* needs to know the decrypt key; but
  2) every time omega has a power outage, someone has to provide the decrypt key.

  If we set up alpha the same way, the same applies to alpha.
  If we don't, then I *think* snapshots of alpha sent to the offsite DR will be unencrypted?
  I'm not too sure.

  This is still super experimental, so I'm not really comfortable with it.
  We will probably end up with the shitty situation of missing out on this by <1y, and
  having unencrypted new-alpha and new-omega, and LUKS encrypted offsite DR.

How do I grow the refind ESP enough to put kernels in there?
------------------------------------------------------------
* gparted can grow a FAT32 filesystem
* parted 3.0+ has no "resize", only "resizepart", which explicitly does not touch the filesystem, only the partition.
* gparted is doing the resize by calling libparted's fat_resize(), AFAICT.
* libparted's provides no way to access fat_resize() from a script or CLI?
* no other tools, e.g. dosfsutils, seem to provide a filesystem-only growing tool a la resize2fs for ext4.
* UPDATE: apt install fatresize ???



References
==========
* `lshw-omega-understudy.html`_
* https://www.supermicro.com/products/motherboard/Xeon/C600/X9SRi-F.cfm
* https://github.com/zfsonlinux/zfs/wiki/Debian-Buster-Root-on-ZFS  (Using ZFS 0.7)
* https://github.com/zfsonlinux/zfs/wiki/Debian-Buster-Encrypted-Root-on-ZFS  (Using ZFS 0.8~rc4 from experimental)
* https://github.com/pbatard/efifs
* https://efi.akeo.ie/downloads/efifs-1.3/x64/zfs_x64.efi
* https://github.com/hn/debian-buster-zfs-root
* https://raw.githubusercontent.com/hn/debian-buster-zfs-root/master/debian-buster-zfs-root.sh
* http://open-zfs.org/wiki/Feature_Flags

  I have confirmed the EFIFS 1.3 uses grub-core's zfs.c almost at HEAD.
  This is the commit of grub that efifs is using::

      51be337 2018-04-16 22:36 -0700 NVi ∙ [HEAD] templates: Update grub script template files

  There are negligible differences between that and grub master::

      twb@goll[grub]$ tig HEAD..master -- grub-core/fs/zfs
      ad4bfee 2019-04-08 07:24 +0200 VSe ∙ Change fs functions to add fs_ prefix
      ca0a4f6 2013-11-20 02:28 +0100 VSe ∙ verifiers: File type for fine-grained signature-verification controlling

  There are negligible differences between that and the (earlier) 2.02 stable grub release::

      twb@goll[grub]$ tig 2.02..HEAD -- grub-core/fs/zfs
      fa42786 2017-08-03 15:46 +0100 PBa ∙ zfs: remove size_t typedef and use grub_size_t instead

  2.02 is the version of grub in Debian buster.
  There is a 2.04 release candidate at this time (Apr 2019). ::

      $ tig 2.02..grub-2.04-rc1 -- grub-core/fs/zfs
      ad4bfee 2019-04-08 07:24 +0200 VSe ∙ Change fs functions to add fs_ prefix
      ca0a4f6 2013-11-20 02:28 +0100 VSe ∙ verifiers: File type for fine-grained signature-verification controlling
      fa42786 2017-08-03 15:46 +0100 PBa ∙ zfs: remove size_t typedef and use grub_size_t instead


IRC Logs
========

2019-04-17  #cyber::

    <twb> ron: old Z68 mobo is now running UEFI firmware and can boot refind
    <twb> FTR, the process was:
    <twb> 1. download .exe from gigabyte's website (requires js)
    <twb> 2. from windows, run the .exe, which happens to be a .7z self-extractor (so PROBABLY could use 7zip on linux)
    <twb> 3. from that, get the FLASHEFI.EXE and FOO.U1F files
    <twb> 4. download FREEDOS 1.2 LITE USB .zip, unzip it, get FD12LITE.IMG.
    <twb> 5. sudo cp FD12LITE.IMG /dev/sdz
    <twb> 6. gparted /dev/sdz, grow the FAT16 partition from ~30 MB to ~62 MB
    <twb> 7. sudo mount /dev/sdz1 /mnt; sudo cp FLASHEFI.EXE FOO.U1F /mnt
    <twb> 8. plug the USB key into a **USB2** port, because FreeDOS doesn't have a USB3 (XHCI) driver?
    <twb> 9. boot freedos, choose "english" choose "fuck off"
    <twb> 10. type "FLASHEFI FOO.U1F" to run the flashing program, wait a few minutes while it runs.
    <twb> 11. can't exit the program, so power off, remove power cable, replace power cable, turn on, hooray, you have an UEFI firmware

2019-04-18  #debian-au::

    12:07 <k-man> twb: regarding zfs for rootfs, no, I didn't do that on my laptopt
    12:07 <k-man> as it seemed too difficult at the time
    12:08 <k-man> in fact, I don't store much, if any data on my laptop that isn't backed up elsewhere so at the moment, my laptop isn't backed up - which I guess isn't ideal
    12:08 <twb> heh
    12:08 <k-man> I've been thinking of re-installing buster, and trying to use zfs for rootfs
    12:09 <k-man> there is this blackbox stript to do it. I've not tried it though: https://github.com/hn/debian-buster-zfs-root
    12:11 <twb> k
    12:11 <twb> I'll read that, but I'm planning to do the install via debootstrap, I think
    12:11 <k-man> righto
    12:11 <k-man> is this for yourself or someone else?
    12:11 <twb> this is for my new main server at work
    12:12 <k-man> nice
    12:12 <k-man> is that your choice to use zfs?
    12:13 <twb> yes
    12:13 <twb> AFAICT btrfs has lost
    12:13 <k-man> wow, interesting
    12:14 <k-man> is it a race where there can only be one winner?
    12:14 <twb> Well it seems crazy to have two good filesystems
    12:15 <k-man> yes
    12:15 <k-man> i think zfs had a richer featureset than btrfs - which meant that btrfs was always going to be playing catchup
    12:17 <twb> AFAIK the main missing features in ZFS are:  1. linux-compatible license; 2. per-file nodatacow; 3. EFI driver (i.e. /boot can be inside the ZFS array)
    12:18 <twb> All of which are annoying, but offset by a. built-in bcache equivalent (ZIL, L2ARC); b. fully automatic resilver to spare drive
    12:19 <twb> Oh also ZFS dedup is post-facto and expensive; btrfs dedup is pre-facto and cheap, but requires you to opt into it
    12:19 <k-man> right
    12:19 <twb> But in both cases the end result is mostly "nobody has dedup"
    12:19 <k-man> just about everything I read about ZFS dedup says "don't use it"
    12:19 <k-man> yeah
    12:19 <k-man> i really like ZVols too
    12:20 <k-man> which BTRFS doesn't do afaict
    12:20 <twb> btrfs doesn't need them
    12:20 <twb> regular files work just as well
    12:20 <k-man> isee
    12:20 <twb> both of them suck at quotas
    12:20 <twb> per-user quotas, I mean
    12:21 <twb> the recommended method is "make a separate zfs for each /home/foo and set a quota on that zfs"
    12:22 <twb> -o com.sun:auto-snapshot=false  I'd have never found that one
    12:24 <twb> That script weirdly makes / and /var separate filesystems
    12:24 <twb> And it's written in bash, but only sh is available in the place he claims to be running it
    12:59 <k-man> twb: he mentioned in the readme that /var had to be seperate for some reason
    12:59 <k-man> something to do with the timing of mounting the zfs fs
    12:59 <k-man>  /var has to be mounted from /etc/fstab rather than the ZFS way
    13:00 <twb> k-man: his readme seems to be explainin why he had a workaround to support a separate /var, but not why he wants /var separate in the first place
    13:00 <k-man> oh ok
    13:00 <k-man> doesn't debian do that if you choose the "everything in seperate partitions" option?
    13:01 <twb> dunno, maybe
    13:01 <twb> having a separate /var is stupid IMO
    13:01 <twb> The reason that's A Thing is because originally / and /usr were read-only and shared between 100 computers
    13:01 <twb> and /var was the per-computer part
    13:01 <k-man> twb: ok. I'm not arguing- just speculating that that was his reasoning
    13:01 <k-man> ah ok
    13:02 <twb> But having /usr separate isn't even supported anymore; it was about 60% broken and then systemd forced that to be 100% broken and officially abandoned
    13:02 <k-man> ah

2019-04-18 #zfsonlinux::

    12:56 <twb> I've babysat a couple of Debian + ZOL hosts before, but I'm setting up my own for the first time.  I also want the rootfs to be on ZOL.  As well as the notes in /topic, my main reference is https://raw.githubusercontent.com/hn/debian-buster-zfs-root/master/debian-buster-zfs-root.sh  (from https://github.com/hn/debian-buster-zfs-root).
    12:57 <twb> I notice that's adding *partitions* to the pool, instead of whole disks.  Isn't that a Bad Thing?
    12:57 <rlaager> No, it's fine. I fought the good fight for wholedisk, but it's just not the right fit here.
    12:57 <twb> Because you need an ESP still?
    12:58 <twb> And a /boot, I guess
    13:00 <rlaager> twb: Right, and now we're doing two pools: bpool for /boot and rpool for /
    13:00 <twb> rlaager: who is "we" ?
    13:03 <twb> Is there a ZFS EFI driver yet?  When using BTRFS, my preferred way to handle this is to have whole-disk elements in the pool, and then have a completely static refind ESP on an internal USB key.  Since refind has a btrfs EFI driver, /boot can just be part of the normal btrfs pool.
    13:04 <twb> There's still a SPOF for that USB key, but it's read-only and not device-specific, so it's trivial to replace
    13:06 <twb> I think refind's btrfs EFI driver was stolen from grub2, and I see there's a /usr/lib/grub/x86_64-efi/zfs.mod... not sure how to turn that into an .efi...

    FIXME I had a link from somebody to a ZFS EFI driver from the "efifs" project, but firefox crashed by laptop and I lost it.


    13:01 <gurg> btw rlaager, is this your tutorial since it mentions you?
    13:01 <gurg> https://github.com/zfsonlinux/zfs/wiki/Ubuntu-18.04-Root-on-ZFS
    13:02 <gurg> Because I was wondering if I should open an issue for the hints for step 2.3 "creating the boot pool" because the hint command there for if doing a raidz says rpool instead of bpool
    13:10 <rlaager> gurg: Typo fixed. Thanks!


    14:50 <twb> Hey, did you know your https://github.com/zfsonlinux/zfs.wiki.git has fsck errors
    14:50 <twb> You can't clone it unless you turn off transfer.fsckobjects=true in git
    14:51 <twb> It's almost entirely linear so you might get away with actually fixing it, instead of just having to live with it


    15:26 <twb> The ZOL wiki links to some introductory blog posts https://pthree.org/2012/12/07/zfs-administration-part-iv-the-adjustable-replacement-cache/
    15:26 <zfs-bot> [ Aaron Toponce : ZFS Administration, Part IV- The Adjustable Replacement Cache ] - pthree.org
    15:26 <twb> That says ZIL >1GB isn't really useful.  Is that still true?
    15:27 <twb> And if so, is it silly to get two separate ~100GB SSDs, one for ZIL and one for L2ARC?
    15:28 <twb> I did that last time, but maybe that was just because partitioning disks is lame
    15:29 <jtara> twb it all comes down to your workload
    15:29 <jtara> a SLOG (separate ZIL) is almost always a good idea for a pool that will see significant sync writes, even if it's no faster than the other disks
    15:29 <jtara> l2arc can help, sometimes, sort of...but imo it really is more for salvaging an awful situation than anything else
    15:29 <jtara> memory is better when you don't have to fake it ;)
    15:30 <PMT> yeah but at the time it was built you weren't usually cost-limited for arbitrary amounts of memory, but HW limited because you couldn't stick more in =P
    15:32 <twb> aren't SSDs cheaper than "buy more RAM" still?
    15:32 <jtara> the times i've seen gains with it align are when metadata is badly fragmented from data and you are suffering from it
    15:32 <jtara> yeah but l2arc helps quite a bit less than just more arc
    15:32 <twb> k
    15:33 <jtara> if you have a trashed pool (logbias=throughput, primarycache=metadata to try to cut down on random read iops) then yeah l2arc can help make an awful situation better
    15:33 <jtara> but so much better to just not get into that in the first place
    15:34 <twb> Looks like for comparable capacity, RAM costs is about four times SSDs cost
    15:35 <twb> oops, I can't math
    15:36 <twb> it's more like 32 times
    15:37 <jtara> imo get a pool going well, look at your arc hit rates, then decide if its worth it
    15:37 <twb> That's fair
    15:38 <jtara> i did zfs consulting for database setups for many years, l2arc really was only beneficial a small amount of the time
    15:38 <twb> because (assuming you kept an AHCI slot free) you can always add the l2arc in <1h downtime
    15:38 <PMT> twb: cheaper isn't what I meant, I meant "no money short of a custom Cray thing would buy you more"
    15:39 <jtara> if l2arc makes or breaks your setup, ime something is very weird.  it can help, but it won't rock your world.
    15:39 <twb> PMT: IIUC you're saying that when l2arc was invented, you simply couldn't put more ram in the server.  Now, you can (although it's still a bit more expensive than SSDs)
    15:40 <jtara> if you can't fit enough ram, try to drive up your average recordsize if you can
    15:40 <jtara> that will decrease metadata overhead and help
    15:41 <jtara> old trick from the sun days anyway
    15:41 <twb> I did have one server that was struggling for some reason, and one of the things I did was add ZIL SLOG and L2ARC on NVME SSDs, and the problem went away.  So I guess I (falsely) internalized that adding SSDs for SLOG and L2ARC is a cheap and easy thing you should ALWAYS do.
    15:42 <PMT> twb: you still have limits, but yes, you could put in much less RAM at almost any cost.
    15:42 <twb> PMT: understood
    15:42 <jtara> the big thing a slog will do that surprises most people, is to decouple rmw reads and compression from the point of time of write
    15:42 <jtara> without one, big sync writes get rmw and compression inline
    15:42 <jtara> which can really be a dramatic dofference
    15:42 <twb> rmw?
    15:43 <jtara> read modify write.  like if you write 32k of a 128k record and the rest needs to get read to merge it
    15:43 <twb> I think I understand that
    15:44 <jtara> basically deferring rmw is good because eventually you may get all the pieces and can avoid the read
    15:44 <PMT> twb: ZFS ~never overwrites things in place. So if you modify some or all of a block, the entire new block gets written separately somewhere else, then it may mark the old one as not needed any more. But since that means reading the old one, modifying the contents, and writing it out separately, you call it [...]
    15:44 <twb> I did something like that once to linearize / cohere write spam to RRD files
    15:45 <jtara> anyway, learn and love zpool iostat, especially -r
    15:45 <jtara> it tells you a lot about what's really happening
    15:46 <PMT> sometimes it tells lies.
    15:46 <PMT> Not often, but sometimes.
    15:47 <jtara> it helps to turn off aggregation and to cross reference with blktrace, i do admit
    15:48 <jtara> but for a "wtf is going on with this pool" peek it's hard to beat
    15:49 <twb> I've used it before.  And sar / iostat on pre-ZFS systems
    15:51 <twb> jtara: does having compression "in the loop" like you were talking about, does that matter if it's a realtime compression algorithm like LZ4 / LZO?
    15:52 <jtara> it does, because usually sync write concerns are driven by latency
    15:53 <jtara> so even a pretty fast compression alg can have significant latency impact
    15:53 <twb> Hrm, OK.
    15:53 <jtara> but usually a bigger concern are rmw reads
    15:53 <jtara> when you make a write wait for a read, it's bad
    15:54 <jtara> i almost always recommend deferring them to txg commit time when you can
    15:54 <jtara> which means either a slog or a very high zfs_immediate_write_sz
    15:55 <twb> So is this a reasonable rule-of-thumb?  1) a SLOG ZIL is always useful; 2) an L2ARC is only really useful when you can't get more ARC and/or your system is badly configured
    15:55 <jtara> thats my opinion anyway, there are others out there
    15:56 <jtara> but that's the classic solaris zfs approach and it works well
    15:58 <twb> Is it true that a SLOG ZIL >1GB is a waste of time?
    15:59 <PMT> twb: the rule of thumb is generally X seconds of sync IO for the main pool
    15:59 <jtara> a slog stores sync writes (and all writes in their sync domains) between txg commits
    15:59 <PMT> usually X is 5
    15:59 <jtara> usually you don't need a lot
    15:59 <twb> PMT: so like if I do zpool iostat and I see 100MB of writes per second, my SLOG should be around 5*100MB = 500MB?
    16:00 <PMT> twb: if that's how long you make txg sync length, yeah. You might adjust that for other reasons. But generally it's X seconds of maximum for the pool, not maximum you've seen.
    16:00 <jtara> there's overhead for every write and for every 128k or so block
    16:01 <PMT> Padding it is fairly minimally risky. Undersizing it makes it more useless than you might want.
    16:01 <jtara> just oversize it 10x what you think you'll ever need and don't worry
    16:01 <PMT> If you can do that, sure.
    16:01 <twb> PMT: OK so it would be more like 5 seconds times the sustained write speed of all the HDDs in the pool?
    16:02 <twb> In my immediate case it's easy for me to give it 10GB or 100GB instead of 1GB, I'm just trying to understand where the numbers come from
    16:02 <PMT> twb: generally? if it were raidz vdevs I'd probably suggest something like X seconds times the total throughput of data disks per pool, but the overprovisioning suggestion means you probably shouldn't bother caring about the difference.
    16:02 <twb> OK cool
    16:02 <PMT> twb: your goal is for it to be able to take all the sync IO from a txg without needing to wait on the disks to catch up.
    16:02 <jtara> if you really want to precision size it, take zfs_dirty_data_max and double it
    16:03 <jtara> that should be worst case
    16:03 <jtara> seriously if you run out then you will just slam into a complete wall until the txg gets committed in full
    16:04 <twb> jtara: OK, although it helps to be able to estimate the number BEFORE I put in a purchasing request, let alone set up the pool :-)
    16:04 <jtara> can you even get disks that small though
    16:05 <PMT> Sometimes.
    16:05 <PMT> The other thing is that as disks get smaller they often have lower performance because they need less flash. ;)
    16:05 <twb> So let me use some real numbers here... sustained write for a WD Red 4TB is 150MB/s.  150MB/s * 5s * 4 disks in a RAIDZ1, yields about 3000MB, or 3GB
    16:05 <jtara> ok, say you're writing compressible data, like text
    16:06 <PMT> Are you sure the drives only do 150 MB/s, max?
    16:06 <twb> So if the smallest SSD I can buy anyway is 100GB, I can give 10GB to the SLOG and 90GB to L2ARC just because it's lying around anyway
    16:06 <twb> PMT: https://www.wd.com/content/dam/wdc/website/downloadable_assets/eng/spec_data_sheet/2879-800002.pdf
    16:06 <jtara> so for your raidz to handle 3000mb, your slog might have to handle 9000mb
    16:06 <PMT> twb: caveat, data in the L2ARC takes up space in the ARC. So it's not just a free set of more RAM.
    16:06 <jtara> or even
    16:06 <jtara> over 9000.
    16:06 <PMT> twb: heh, 4TB.
    16:07 <jtara> try to resist the temptation to do something else with a slog device
    16:07 <jtara> but, people do
    16:07 <twb> This is for what you might call a "pet server"; the entire dataset fits into 1TB currently
    16:08 <twb> jtara: OK.  So buy the smallest SSD and let the SLOG have all of it, and it might be massively overprovisioned, but who cares
    16:15 <jtara> my customers always care about predictable latency and smooth degradation and stuff like that...so yeah


    18:42 <twb> rlaager: in https://github.com/zfsonlinux/zfs/wiki/Debian-Buster-Root-on-ZFS §2  you use mdadm and sgdisk to wipe existing config on a disk.  You might want to also use wipefs(1), which removes everything that blkid(1) can see
    18:49 <twb> Ugh, I was wondering why gdisk instead of libparted --- turns out the ZFS support in libparted is Ubuntu-specific


2019-04-23 #zfsonlinux::

    12:31 /join #zfsonlinux
    12:34 <twb> rlaager: re zfs.wiki/Debian-Buster-Root-on-ZFS.md, you said last week that you wanted to give ZFS the whole disk(s), but had to give up in that HOWTO (because grub zfs is feature-limited, and because ESP).
    12:35 <twb> $coworker has a different idea and I want to know how much you (all) hate it
    12:35 <twb> 1. ESP is stock refind on a USB key inside the chassis, because refind Just Works, and then it's a completely static ESP that's easy to replace
    12:35 <twb> 2. except that refind has the same problem as grub, so instead of making /boot separate, leave it on the ZFS, but have an apt post-install hook that copies /boot onto the ESP
    12:36 <twb> If the ESP drive goes tits-up, you need to boot a live system to recover, instead of just making another stock refind USB key, but that's not TOO hard, and it means you can continue to give full disks (instead of partitions) to ZFS.  Which is/was a Super Nice Thing
    12:39 <PMT> twb: i mean, that assumes the existence of an internal USB key, which is a bit of a specialized way to solve this
    12:39 <twb> Granted
    12:39 <twb> That's our (work's) standard way to work around EFI having no concept of mirrored ESP
    12:40 <twb> You make the ESP the same across all systems, so it's more like a network card than a software configuration, and if it breaks you just install a new one
    12:41 <twb> grub zfs.mod limitations won't let me make it EXACTLY the same (at least, if the pool is going to use newer features), so #2 is my workaround for that
    12:42 <PMT> I mean, at that point, why not just netboot refind?
    12:43 <twb> I guess mainly because that makes it dependent on the PXE server already being up.  Also one of the targets for this design *is* the netboot server
    12:44 <PMT> So you have multiple netboot servers and they boot from each other. :V
    12:44 <twb> heh, fair
    12:46 <PMT> pts0: i mean, everything you've never done before is complex, so
    12:46 <twb> Looking at the issue from the other end: Solaris ZFS used to *really* want whole disks, not partitions (AFAIK).  Is that (disks not partitions) something ZOL still cares about?
    12:49 <PMT> twb: Solaris wanted whole disks to play with the write cache settings. ZoL wants whole disks to set the whole disk's IO scheduler to noop or equivalent. If you can't use the whole disk, you could still script doing that.
    12:49 <twb> Ah OK
    12:49 <PMT> (if it's not a whole disk you also need to go fiddle with the partition tables to expand a device, I believe, but vOv)
    12:49 <jasonwc> DeHackEd, Cisco already listed the Optane DIMMS for sale.  $2000 for 128GB and $21k for 512GB.  --> https://www.storagereview.com/cisco_details_intel_optane_dc_persistent_memory_pricing
    12:49 <zfs-bot> [ Cisco Details Intel Optane DC Persistent Memory Pricing | StorageReview.com - Storage Reviews ] - www.storagereview.com
    12:50 <PMT> lmfao
    12:50 <PMT> quite the goddamn dimm
    12:50 <twb> Does ZFS care if the partitions were created with parted --align=optimal (or whatever the gdisk equivalent is)?
    12:51 <PMT> ZFS doesn't, but the drives do. :D
    12:51 <twb> k
    12:51 <jasonwc> they have a review of a Supermicro server with those persistent DIMMs.  "Our first test is the 4K random read test here the persistent memory started at 1,371,386 IOPS at 4.6μs and went on to peak at 13,169,761 IOPS at a latency of only 12.1μs. "
    12:51 <pts0> PMT so it sounds like mdadm with mbr partitions booting to bios boot mode is the way to given all that twb is going through
    12:51 <PMT> That's mostly a joke - using non-physical block size aligned partitions is an invitation to pain and suffering.
    12:51 <PMT> pts0: what?
    12:52 <twb> PMT: yeah understood, I just half-expected you to say "ZFS is so smart it'll automatically fix that for you if you screw up"
    12:52 <pts0> so just ext4 a root and then a boot partition and do a bios as opposed to a uefi boot
    12:52 <PMT> pts0: ...how did you get there from any of this discussion.
    12:52 <pts0> zfs on boot with uefi is crazy complex
    12:52 <twb> PMT: I think they're thinking of a md raid1 ext /boot, and the UEFI->BIOS thing is just a conflation
    12:52 <pts0> what i said is much simpler
    12:53 <pts0> no forget uefi
    12:53 <PMT> twb: i know, i'm just unsure why they think this is helpful.
    12:53 <pts0> it won't even see the stuff right without a bunch of fanagling
    12:53 <jasonwc> pts0, What's so complex about it? It's documenteed by rlaager
    12:53 <twb> pts0: are you suggesting that the non-UEFI (BIOS) ZFS drivers in grub are *better* than the UEFI ZFS drivers in grub?
    12:53 <pts0> no i'm saying no zfs for the boot and root
    12:53 <pts0> and just do a bios boot
    12:54 <PMT> pts0: you appear to be advocating a solution to a problem nobody else thinks is a problem.
    12:54 <jasonwc> You can do BIOS boot with separate boot and root pools
    12:54 <jasonwc> You can also do a BIOS boot with a single root pool if you don't need features Grub doesn't support
    12:54 <pts0> seems to complex to me from everything i've looked at
    12:54 <pts0> 100 damn steps
    12:54 <jasonwc> It's just thoroughly documented.  It's not a hard process.
    12:54 <twb> I don't want pre-UEFI BIOS, and I want / on ZFS.  Having /boot on ZFS is "nice to have" but not critical..
    12:55 <rlaager> pts0: You're free to do whatever you want on your systems, of course, but this works fine and is well documented. It's also the way forward if this ever gets distro integration, which is being considered.
    12:55 <pts0> Can someone give me a link to the documentation
    12:55 <pts0> maybe i saw the wrong thing
    12:55 <twb> pts0: zfs.wiki/Debian-Buster-Root-on-ZFS.md
    12:55 <jasonwc> pts0, What distribution are you planning to use?
    12:55 <twb> pts0: so uh... https://github.com/zfsonlinux/zfs/wiki/Debian-Buster-Root-on-ZFS
    12:55 <pts0> ubuntu
    12:56 <twb> pts0: there is a similar page for Ubuntu LTS on that wiki
    12:56 <jasonwc> pts0, https://github.com/zfsonlinux/zfs/wiki/Ubuntu-18.04-Root-on-ZFS
    12:56 <pts0> yeah...that's the one i saw
    12:56 <pts0> it's above my brain capacity
    12:56 <pts0> ugh
    12:56 <jasonwc> pts0, For one thing, it documents both an encrypted and non-encrypted setup
    12:56 <jasonwc> For another, it includes every step, so you just need to follow the instructions once.
    12:56 <twb> pts0: most of it is standard, e.g. the ESP stuff is the same as if Ubuntu's debian-installer script was doing it.
    12:57 <rlaager> twb: If you put /boot into the ESP, then yes, your alternative proposal works fine, and ZFS can have whole disks for the rpool.
    12:57 <pts0> I just can't understand it
    12:57 <jasonwc> pts0, What don't you understand?
    12:57 <pts0> I wonder if they were to put it in sections--maybe taking out the encrypted part,etc.
    12:57 <twb> rlaager: OK, thanks.  I was mainly hoping for a response of "oh, interesting idea" or "don't do that, because <reason>!"  :-)
    12:57 <pts0> put the encrypted part in another link
    12:57 <pts0> i dunno
    12:58 <pts0> Then it's just not explained well
    12:58 <pts0> run sgdisk with crazy switches
    12:58 <pts0> well i've never run sgdisk before
    12:58 <pts0> i've used fdisk
    12:58 <rlaager> twb: This also works if you have separate disks generally. My tentative plan for my personal server is to replace everything with the following: 1) A mirrored pair of SSDs. They will be partitioned for BBP (because this is BIOS only hardware but could be ESP), bpool, and a "special" (allocation classes) partition for the rpool. 2) The spinning disks will be a raidz3 with wholedisk.
    12:58 <pts0> and another one so I dont' even know what it's doing
    12:58 <twb> pts0: fdisk -> gdisk is just MBR -> GPT
    12:58 <jasonwc> pts0, I mean, you can manually partition with fdisk
    12:59 <rlaager> twb: The main reason to NOT do it in a server environment is the lack of mirroring, which you've acknowledged.
    12:59 <pts0> all day long
    12:59 <pts0> but what's with all the switches
    12:59 <jasonwc> pts0, In fact, I"ve done it.  You can also use the commands provided and it will work.
    12:59 <pts0> then they don't talk about how to get the disk uuids or any explanation for the sizing and zfs switches
    12:59 <pts0> just do all this craziness
    12:59 <twb> rlaager: OK, I understood all that except for the special allocation classes thing
    12:59 <pts0> ok, my main disk dies
    12:59 <pts0> then what
    13:00 <pts0> i have to go into the bios and repoint it to another disk or something
    13:00 <rlaager> pts0: If you have concrete suggestions, I'm open to adding more explanations. It does make it longer, but people seem to like the explanations generally, as they can skip them if they don't care.
    13:00 <pts0> the format arch uses always helps me
    13:00 <pts0> i don't know why
    13:00 <jasonwc> I suppose it would be nice if there were expandable sections so you could just follow the one you wnated
    13:00 <pts0> yeah
    13:00 <pts0> yes
    13:00 <rlaager> twb: It's a new in 0.8.0 feature that puts the metadata on separate devices, in this case, SSDs. So the hope is I'd get a useful mix of capacity from raidz3 and metadata performance from SSDs.
    13:00 <twb> rlaager: ah, OK.  I have seen a similar thing in btrfs, I think
    13:01 <jasonwc> but not sure if the Wiki even supports that
    13:01 <rlaager> I'm not sure either. Maybe I could look at readthedocs.io or whatever the kids are using these days.
    13:01 <pts0> rlaager for starters make two separate links for encrypted vs non-encryped
    13:01 <pts0> to clear up the cluttered appearance...but other than that...truth is i dont' know what the hell im doing so
    13:01 <pts0> this is going to be a tough one
    13:02 <jasonwc> pts0, Try following the instructions in a VM.  I bet you'll find it is easier than you think.
    13:02 <jasonwc> pts0, and if you break the VM, no big deal
    13:02 <rlaager> Another option would be to have the .md output from some script. Then it could write out BIOS vs UEFI; encrypted vs. not, etc. And we wouldn't care how many different outputs there were, because it's automated. Then just have the users click some links to get the right version.
    13:02 <rlaager> But another way to do this is to write this into the OS installer, which is hopefully happening.
    13:02 <pts0> so what happens if my main disk dies
    13:02 <pts0> do i have to go into the bios
    13:03 <rlaager> pts0: "Hints: ... ls -la /dev/disk/by-id will list the aliases."
    13:03 <jasonwc> pts0, You can install Grub to all disks. Presumably, your BIOS allows you to sort boot order, so it should just boot from the second listed disk
    13:03 <twb> One of my hats is documentation manager / tech writer, and I can definite vouch for reST/sphinx/readthedocs stack over markdown.  The expanding/collapsing block thing would either be part of the CSS/js theme, or an extension.
    13:04 <pts0> ok, updates are ALWAYS changing grub crap
    13:04 <pts0> is an update going to break it
    13:04 <twb> (sphinx is what linux kernel and python communities use these days)
    13:04 <rlaager> pts0: As a general tip... I suggest following the instructions once, exactly as written. Once you see how things work and have a working system to poke at, it will make a lot more sense.
    13:04 <pts0> ****do you want to overwrite custom grub with package maintaners...that one
    13:04 <rlaager> It's fine. Obviously say "no" to that prompt.
    13:05 <pts0> then i miss something the package maintainer wanted
    13:05 <rlaager> But to be fair, you do need to have some idea what you're doing, and be willing to take some risk. This is not the common-case scenario, as it's not a distro default. It's not distro supported. etc. etc.
    13:05 <pts0> i always hate that one
    13:05 <pts0> sgdisk     -n2:1M:+512M   -t2:EF00 /dev/disk/by-id/scsi-SATA_disk1
    13:05 <pts0> so what are all the switches
    13:06 <twb> pts0: per the manpage, it creates a new partition, number 2, from 1M to 1+512MB, on the disk with serial number "SATA_disk1"
    13:06 <twb> pts0: oh and sets its GPT UUID (i.e. "partition type") to "EF00", which is presumably the magic number for the ESP
    13:06 <pts0> ok so thats not the uuids i see when i run blkid
    13:07 *** ChanServ MODE +v mahrens
    13:07 <twb> pts0: again per the gdisk manpage, the 4-digit UUID is a shorthand for the "real" UUID
    13:07 <pts0> yeah see i have no idea what the hell that means
    13:08 <jasonwc> pts0, I think you're overthinking this.  Try it in a VM to get a feel for how things work.  If you have a decent understanding of ZFS, you should be fine.
    13:08 <twb> pts0: that's OK.  The HOWTOs on the ZOL wiki assume you already have some experience with these tools.
    13:08 <pts0> I'll go throught it and make a bunch of notes when I have time and try to install it so you guys know what an idiot thinks about so if you want to idiot proof it you can
    13:09 <twb> pts0: you just need some practice, I think.  As jasonwc says, you can do this with a toy VM to avoid risking your "real" setups
    13:10 <pts0> Honestly the complexity just makes me want to use the hardware raid...then you have people saying it's perfectly fine
    13:11 <pts0> anyway, i appreciate all the help
    13:11 <PMT> I mean, if you dislike things you don't understand, you're free to do whatever you want.
    13:12 <pts0> why do you say that
    13:12 <pts0> i know that
    13:12 <pts0> i'm not trying to be a jerk or anything
    13:13 <pts0> I'm tempted to learn this but I haven't been feeling well lately
    13:13 <twb> hardware raid is a huge pain, because you have to buy a spare rescue card in case the main card dies (because different models aren't compatible); you can't SMART to the drives to find out when they're dying; you have to run proprietary software in a RHEL chroot to manage the array, &c &c
    13:13 <jasonwc> pts0, I found it overwhelming at first too, but I wouldn't go back to using anything but ZFS for my root fs.  It's saved me so many times when something broke and I was able to fix it with a quick rollback, live, without restoring from backups
    13:14 <pts0> twb but you'd have to buy a spare hba too right
    13:14 <pts0> the thing that scares me is zfs just dying due to hardware raid
    13:14 <pts0> some people really stress that
    13:14 <PMT> that...doesn't make sense.
    13:15 <twb> pts0: if you're doing RAIDZ, then you're using passthrough HBAs
    13:15 <twb> pts0: which are 1. cheap; and 2. fungible.
    13:15 <twb> pts0: I know that if I put the disks into *any* computer with enough SAS slots, I can recover the data.
    13:16 <PMT> If you hide the multiple disks from ZFS, then yes, it can't recover from corruption, because it doesn't have any sort of redundancy even if there's a HW raid under it, but that's not really a ZFS-ism, it's just a fact.
    13:16 <twb> (When I was talking about buying a "rescue card" for hardware raid, "card" = "HBA")
    13:16 <pts0> check out my posts on reddit to see what I was looking at:https://www.reddit.com/user/5tzr/posts/
    13:16 <zfs-bot> [REDDITOR] 5tzr | Link: 6 | Comment: 35
    13:16 <zfs-bot> [ 5tzr (u/5tzr) - Reddit ] - www.reddit.com
    13:16 <PMT> no
    13:17 <pts0> PMT you're free to do whatever you want.
    13:18 <jasonwc> pts0, I think the first post from fengshui is on point
    13:18 <jasonwc> https://www.reddit.com/r/zfs/comments/bg4t3w/any_thoughtsopinionscorrections_on_this_article/
    13:18 <zfs-bot> [REDDIT] Any thoughts/opinions/corrections on this article? (https://mangolassi.it/topic/12047/zfs-is-perfectly-safe-on-hardware-raid/9) to r/zfs | 0 points (33.0%) | 4 comments | Posted by 5tzr | Created at 2019-04-22 - 17:27:15UTC
    13:18 <zfs-bot> [ Any thoughts/opinions/corrections on this article? : zfs ] - www.reddit.com
    13:20 <pts0> I'm not sure if I should trust that Perc card even in passthrough mode
    13:21 <pts0> maybe I should get another card?
    13:21 <pts0> One guy said you could flash the full perc but not the mini
    13:21 <pts0> I have the mini
    13:21 <jasonwc> Ah
    13:21 <jasonwc> I mean, you can get a 9211-8i pre-flashed with IT mode for $50 on Ebay
    13:21 <jasonwc> I've got 6 of them and they work great
    13:22 <pts0> That's what you would recommend for a Dell R720?
    13:22 <pts0> Do you have a link?
    13:22 <twb> perc 5 HBA can't do passthrough IIRC
    13:22 <twb> It can only do single-disk RAID0s, and you lost all your SMART
    13:23 <pts0> perc h310 mini
    13:23 <pts0> that's the model I have
    13:23 <twb> Last time I was on a server that came with a RAID HBA, I just paid $20 to replace it with a passthrough HBA
    13:24 <pts0> What HBA would you recommend for a R720?
    13:24 <twb> pts0: whatever the first-party one is
    13:24 <pts0> I don't know what that is
    13:24 <jasonwc> pts0, I mean, there are a ton of them on Ebay.  Here's one from a seller I've actually purchased from in the past that I trust
    13:24 <jasonwc> pts0, https://www.ebay.com/itm/LSI-SAS-9211-8i-8-port-6Gb-s-Internal-IT-MODE-ZFS-JBOD-HBA-IR-MODE-RAID/352612690487?hash=item52195aaa37:m:mbEDowx1AV5jIlfu6fiaMHg
    13:24 <zfs-bot> [ LSI SAS 9211-8i 8-port 6Gb/s Internal (IT-MODE) ZFS JBOD HBA / (IR-MODE) RAID | eBay ] - www.ebay.com
    13:24 <twb> pts0: so call your dell salesdroid and ask
    13:25 <pts0> I don't know what you mean by first party
    13:25 <twb> jasonwc: what's "IT mode" mean, there?
    13:25 <twb> pts0: "first-party" means Dell sells it to you and guarantees it'll work in their server
    13:25 <jasonwc> pts0, Each SAS port will give you 4x 6 Gbit bandwidth, so it'll give you full bandwidth for 8 drives, but if you're using HDDs and have a backplane with a SAS expander, you can use a single HBA for 24 or more drives
    13:25 <jasonwc> twb, IT mode means it's just a pure HBA, no RAID features, just pass-through
    13:25 <jasonwc> twb, full SMART access etc.
    13:25 <twb> jasonwc: ah cool
    13:26 <twb> jasonwc: that's not a shibboleth I knew, but I'll remember it for next time
    13:26 <jasonwc> twb, These LSI cards are sold under a bunch of names, and you can flash different firmware.  IR is RAID IIRC, IT is what you want for ZFS
    13:26 <pts0> and this will work with the Dell disk case?
    13:26 <jasonwc> twb, The 9211-8i are pervasive but there are newer and faster variants with SAS 12 Gb support
    13:27 <jasonwc> pts0, Dell disk case?
    13:27 <jasonwc> You mean Dell Chasis?
    13:27 <jasonwc> I think a lot of people use 9211-8i's from various manufacturers with the IT firmware.  Unless Dell is doing something to block non-Dell hardware, I don't see why it wouldn't work.
    13:28 <pts0> Hell I don't know
    13:28 <jasonwc> You can ask on Reddit.  Lots of people use these cards.
    13:28 <pts0> I thought you had to cable something to it
    13:28 <jasonwc> They're just dumb HBAs
    13:28 <jasonwc> yeah, you cable it to the backplane
    13:28 <pts0> the backplane where all the disks plug into?
    13:28 <jasonwc> with a direct attach backplane, you'll have one SAS port for every 4 drive bays
    13:28 <jasonwc> With a SAS expander, you may have one or two ports for all drives
    13:29 <jasonwc> pts0, Yes, drives attach to the backplane, and then on the back of the backplane there will be SAS ports that go to the HBA
    13:29 <jasonwc> pts0, How many drives are you going to be using?
    13:30 <pts0> well i was thinking 8, 2 mirrored for the boot/root stuff
    13:30 <pts0> but is that not what the guide says to do?
    13:31 <jasonwc> pts0, This what you have? https://www.youtube.com/watch?v=a5toVeaLqRA
    13:31 <zfs-bot> [ PowerEdge R720: Hard Drive Backplane - YouTube ] - www.youtube.com
    13:31 <pts0> i think that's the xd with a bunch of drives
    13:31 <pts0> mine just has 8
    13:31 <pts0> R720xd vs R720
    13:32 <pts0> I just have the R720
    13:32 /dim pts0
    13:32 *** DIM pts0 Ashenzari Henzell Sizzell Sequell Lantell Rotatell Eksell Cheibriados Gretell Kramell BTS zfs-bot nalpre EMIYA ladybird ldybrd ladybug ladybot ladyblurb mrscribe travis-ci Brainstorm Not-ba84 lubotu2 kenaan Svadilfari bbot2 feepbot curlbot zfs firebot pg_docbot pg_docbot crosbot zwiebelbot sd-bot sd-bot0 sd-bot1 sd-bot2 sd-bot3 sd-bot4 sd-bot5 sd-bot6 sd-bot7 32NAAB16X CIA- CIA-0 CIA-1 CIA-2 CIA-3 ChanServ Energon lpaste FBI GumbyPAN Jesdisciple KGB KGB-0 KGB-1 KGB-2 KGB-3 MentorSeeker birny checkbot cyber darcscommitbot darcswikibot dpkg dselect evalbot evil_twin factorbot fajita friendlyToaster fsbot fsbot` fsbot`` infobob ghcbot gitbot gitinfo GitHub157 GitHub106 greybot hgbot hpaste iwakura judd knoba lambdabot lamebot lisppaste lopbot pasteban perlbot pursuivant rudybot rudybot_ samba-bot sarahbot shadowpaste shbot shebang specbot tpg trungl-bot twiggz ubottu ubotu ubot2 ubot93 upstartbot uvirtbot uvirtbot~ uvirtbot` vpnHelper wikibugs IZBot Selvvir Not-63f6 logerritbot loircbot Nugs Not-c51b wrt Anaphaxeton Apic Buglouse average theos edgar-rft ManateeLazyCat MrSassyPants Roxyhart0 TheEvilPhoenix ^law^ a7a ams bazhang coldhead consolers coolack djszapi djszapi_ eagles0513875 eagles051387_ eagles0513875|2 eagles0513875| fix jaimef akkad joeoshawa jordanb jordanb_ kaushal notbrien tanasinn wolferz yates BlueOcean blass rockxloose rockxloose_ p0a rob_debian jeromelanteri epony RaycatRakittra Aethaeryn kenzo kenzo215 njsg Muimi Muimi1 iqubic Juesto bazhang Megabyte Shinigami-Sama sam_yan biffhero reisio Branes Calinou Captain_Crow CcSsNET John[Lisbeth] dorothyw johnnymacs Cheiron nunag rotorua tokeroa Devastator Error404NotFound Grum str1ngs LimCore Mgamerz WorstHumanEver Pikkachu RoyK RoyK^ antono atomx aurelien buu clint clint- cloneMX cluelessperson cluelessphone dominicdinada mpourhadi mpourhadi_ drake1 aeth fxiny haylo haylo_ heviarti heviarti_ icepick caffe j4jackj dvorakbot lich linocisco maroloccio maroloccio2 omeddragon pizzasauce rgr robottinosino ruben32 seanjohn silentwhisper xk05 |xk05| xk051 xorred thunk
    13:32 <pts0> So you think that card will work?
    13:33 <pts0> I can buy a more expensive one if it's better
    13:33 <jasonwc> you're right about the mini card
    13:33 <jasonwc> "If you're using a mini card don't attempt flash it as you'll brick the card instead configure the disks as non raid to pass them through. The mini card looks like this."
    13:33 <jasonwc> https://www.serverhome.nl/media/catalog/product/cache/1/image/9df78eab33525d08d6e5fb8d27136e95/p/e/perc-h310-mini-mono.jpg
    13:33 <pts0> this midget porn collection is very important to me
    13:33 <pts0> bad joke
    13:34 <jasonwc> " If you're using a non mini card, I'd suggest you flash it to a pure HBA using 9211-8i firmware."
    13:34 <jasonwc> Apparently the non-mini card is just a 9211-8i
    13:34 <pts0> yes i have the mini
    13:34 <jasonwc> So I don't see why buying one online wouldn't work
    13:34 <pts0> someone told me exactly what you just posted
    13:34 <jasonwc> if it makes you feel better, you can get the dell one
    13:35 <jasonwc> https://www.ebay.com/itm/LSI-9211-8i-P20-IT-Mode-for-ZFS-FreeNAS-unRAID-Dell-H310-6Gbps-SAS-HBA/253955813684?epid=19006955695&hash=item3b20f23134:g:kKkAAOSwMjpb11RL
    13:35 <zfs-bot> [ LSI 9211-8i P20 IT Mode for ZFS FreeNAS unRAID Dell H310 6Gbps SAS HBA | eBay ] - www.ebay.com
    13:35 <jasonwc> So this is just a Dell H310 flashed with the IT firmware
    13:35 <jasonwc> given that the firmware is the same, it's silly to pay more for it
    13:35 <jasonwc> You're better off to asking on Reddit if anyone had problems using a 9211-8i that was not sourced from Dell.  I suspect it'll work just fine.
    13:36 <pts0> I wonder if I do Dell firmware updates if it will brick it
    13:36 <jasonwc> What firmware updates would you need to do?
    13:36 <pts0> Dell has that automated firmware updates through the lifecycle controller, etc
    13:36 <jasonwc> Other than update the BIOS on my motherboard
    13:36 <pts0> and with an iso
    13:37 <pts0> oh it updates disk firmware too
    13:37 <jasonwc> Wouldn't that assume the disks came from Dell?
    13:37 <pts0> yeah good point
    13:37 <pts0> hell im going to try it
    13:37 <pts0> screw it
    13:37 <jasonwc> I've had good experience with UnixSurplus
    13:38 <jasonwc> They are based in CA and I've bought a few servers from them
    13:40 <pts0> Does that guide create a separate pool/mirror for the boot/root stuff or is it all one big raid6 or something
    13:41 <jasonwc> pts0, The guide can be modified to do whatever you want.  I personally use a mirror of SSDs for my root pool.  Mirrors provide better IOPS and are more efficient than raidz2 for storing small blocks (8 and 16K).  With raidz/2/3, you get the random IOPS of a single disk, but sequential reads and writes scale with the # of disks - 1/2/3 disks based on your parity selection.  Mirrors will give you the write IOPS of a single
    13:41 <jasonwc> disk and the read IOPS of the # of disks in the mirror.
    13:42 <jasonwc> pts0, Are you using just SSDs or will this system also have HDDs?
    13:44 <pts0> It will just have HDDs
    13:44 <pts0> Man I just can't understand what that guide is doing
    13:44 <pts0> I want my boot/root disks to be mirrored
    13:44 <jasonwc> I don't think you want a single raidz2 in that case.  You'll get the IOPS of a single disk and that's not great.
    13:45 <pts0> It seems like that woudl be the default
    13:45 <jasonwc> pts0, The guide can be used to create a mirror, raidz/2/3, or a singleton
    13:45 <pts0> but how
    13:45 <pts0> you have to boot off one disk
    13:45 <pts0> and if that one dies, you have to boot of the other
    13:45 <pts0> unless you just don't want to boot
    13:46 <jasonwc> pts0, so, the guide shows how to create a pool using a single disk
    13:47 <pts0> ok so then what
    13:47 <jasonwc> pts0, To simplify, zpool create -o options rpool mirror Disk1 Disk2
    13:47 <pts0> that disk dies and I'm screwed
    13:48 <jasonwc> pts0, for raidz, zpool create -o options rpool raidz Disk1 Disk2 Disk3
    13:48 <jasonwc> pts0, The guide actually covers this: "If you are creating a mirror or raidz topology, create the pool using zpool create ... rpool mirror /dev/disk/by-id/scsi-SATA_disk1-part4 /dev/disk/by-id/scsi-SATA_disk2-part4 (or replace mirror with raidz, raidz2, or raidz3 and list the partitions from additional disks). "
    13:48 <pts0> I would bet good money it wouldn't jsut boot off that second drive then
    13:48 <jasonwc> pts0, The guide assumes some knowledge of ZFS
    13:49 <pts0> if i pulled the first out
    13:49 <jasonwc> pts0, It would if you install Grub to both drives, which the guide instructs you to do
    13:49 <jasonwc> pts0, I've pulled a drive and it boots
    13:50 <pts0> So it starts the partitions off at an offset or something so you can put grub on it
    13:56 !!! irc.freenode.net: connection broken by remote peer (closed)
    13:56 !!! irc.freenode.net: connection broken by remote peer (closed)
    > The other advantage of all first-party hardware is that 14:01 /join
      #zfsonlinux
    14:01 *** TOPIC ZFS on Linux - http://zfsonlinux.org/ - Latest release: 0.7.13 (beta 0.8.0-rc4) - FAQ: https://github.com/zfsonlinux/zfs/wiki/faq - LiveCD: http://list.zfsonlinux.org/zfs-iso/ - Offtopic (non-computer stuff) should use #zfsonlinux-social - Paste using nopaste - Native encryption is not production ready, see #6224 (rlaager!~rlaager@grape.coderich.net on Thu Apr 18 11:29:11 2019)
    14:01 *** NAMES @ChanServ +mahrens +ryao _KaszpiR_ aarwine adisbladis aegiap agris aither Albori alexxy AllanJude amir amospalla andjjj23 AndrewG AndyCap apekatten APTX_ apus asjackson b636bc7ca bart- Baughn bdha beardface beowuff betawaffle bglod_ blassin Blub\0 bobe bol breitenj brethil bs338 BtbN buu c3-Win c3r1c3-Lin captain42 CarlGel catalase cbmuser Ceryn cirdan clete2 clever cliluw CoJaBo Comnenus CompanionCube copec cpt-oblivious Crocodillian ctrlbreak_MAD cyberbootje cyphase cyteen Dagger dave_uy debfx deepy DeHackEd delx devster Dewin DHE djdunn djs_ donhw Dragnslcr dsockwell dustinm DzAirmaX eab echelog-3 echeveria edef eightyeight Ekho elvishjerricco endre` ephemer0l_ ericnoan erska esotericnonsense euank ezbp f_g feliks FinalX FireSnake fishfox__ fling fs2 Fubarovic Fusl fwaggle gardar gbkersey gchristensen geaaru ghfields ghormoon ghoti gila GrayShade green-- guardian Gugge gurg gurki gyakovlev haasn Harekiet heinrich5991 hexa- hmw hondza HotSwap hsp hyper_ch Hypfer ih8wndz iklio INeedAHandle infecticide infernix IonTau irdr_ iTeV ius izibi JanC jasonrm jasonwc javi404 jidar jkux jmcgnh JMoVS JPT jtara JulietDeltaGolf JWU42 k-man kahiru kalasmannen kegwin kerberizer kimmyk King_InuYasha kini kittysh Klox krupke L0j1k Lalufu lblume leah2 LeoTh3o leper` libsci likewhoa logan- Logos01 lord4163 lpsmith lundman lurkless lynchc m3thos madwizard maestronolte manfromafar marble_visions MarisaKi1 mavhq mf migy__ MilkmanDan mishiranu misuto mlatin mmlb MooingLemur morphin mquin mybalzitch mz` nahamu_ neffi nicoulaj noone nwf oblikoamorale optiz0r p_l pcd percY- perfinion phantomcircuit phibs Phil-Work Phil21 pink_mist piti Pjerky pmooney PMT poots pR0Ps prawn prologic prometheanfire ps-auxw pts0 puxavida radkos raoulb ReimuHakurei rich0 riddle rj1 rlaager rnydam robinsmidsrod Ryushin sarnold sauravg scar scastano Selavi ShaRose Shinigami-Sama siruf Sketch skomorokh_ snehring soltys sphalerite sphrak sponix Stern stiell stochastix stoiko storrgie str4nd STRML SuperSeriousCat sztanpet Tashtari tds teardown TheJollyRoger theracermaster tilpner timeless timewalker TimWolla tlacatlc6 tmiw trisk troyt tsp tuxmontero twb uebera|| ulope user1234567 user_51 varesa veegee_ veremitz vertigo voldial vrakfall__ wak-work Walex waz0wski whyz woffs wolfire wolke xedniv Xeha xlued xmw XpineX xqsl yangm97 yawkat yoink yomisei Yukikaze z1mme zapotah zfs-bot Zgrokl zoredache |woody|
    14:01 *** services. 328 #zfsonlinux http://zfsonlinux.org
    14:01 <pts0> sgdisk     -n2:1M:+512M    ok from 1M to 512m but then this just overwrites it sgdisk     -n3:0:+512M
    14:02 <pts0> the third one overwrites the second one
    14:02 <pts0> and it overwrites the first one because it starts at 0
    14:03 <pts0> there's just nothing in the gude about it...ah well
    14:04 <pts0> maybe i'll be smarter in the future...anyhow thanks everyone
    14:04 <pts0> take care
    14:04 <jasonwc> no, the third is +512M...
    14:04 <jasonwc> so it doesn't overwrite it
    14:04 <jasonwc> first is 24K-1M, second 1M-512M, third starts at 512M
    14:04 <jasonwc> pts0, I would recommend using ZFS for your data pool to get familiar with it, and you can revisit using ZFS for a boot pool when you feel more comfortable
    14:07 <jtara> i'd agree
    14:07 <pts0> unfortunately i don't think i can
    14:07 <pts0> i can either enable hardware raid or not
    14:07 <jtara> it's best used on a small scale first if you're still learning it
    14:07 <pts0> why are there three ts
    14:07 <pts0> t1, t2, t3
    14:08 <jasonwc> pts0, You can use two disks for a mdadm mirror for your boot, and give the remaining disks to ZFS for a raidz/2 pool
    14:08 <pts0> n1, n2 and n3 are partitions but the ts are types right
    14:08 <jasonwc> pts0, mdadm should be very simple
    14:08 <pts0> lol
    14:08 <pts0> yeah i looked at that guide too
    14:08 <pts0> there is a script that supposedly makes it simple
    14:08 <jasonwc> pts0, And if you're not booting off of ZFS, you don't have to worry about kernel compatibility.
    14:09 <pts0> i think, truth is, you just have to understand all this to do a non-standard install
    14:09 <jasonwc> You can setup a RAID1 mirror using mdadm with either the Debian or Ubuntu installer
    14:09 <pts0> ok, yeah, i'll try that
    14:09 <pts0> last question though why are there three ts
    14:09 <pts0> t1, t2, t3
    14:09 <jasonwc> so, use 2 disks for your / with mdadm and LVM
    14:09 <pts0> if it's just a partition type
    14:09 <jasonwc> it'll do that for you
    14:10 <jasonwc> then I would probably do a raidz2 with the 6 disks unless you need IOPS in which case you can do 3 mirrors striped
    14:10 <pts0> truth is im' just learning
    14:10 <pts0> jsut a cheap box to play with
    14:10 <pts0> seriously though what are the three ts
    14:11 <pts0> in the guide in that partitioning section
    14:11 <pts0> t1, t2 and t3...n1, n2 and n3 are the partition numbers
    14:11 <jtara> if you're just learning, leave hw raid off, maybe even plan on two separate pools
    14:11 <jtara> so you can back up everything to one if you need to blow away the other, or something
    14:11 <jtara> plus some things are best appreciated with a couple different pools
    14:12 <jtara> personally i would only do sw raid for a boot drive, but that's probably more of a solaris perspective than linux
    14:12 <jasonwc> jtara, Why? mdadm should be well tested.  Raid5/6 has the write hole but RAID10 should be fine.
    14:14 <jtara> i'd rather have a sw raid setup because i don't trust boot disks sitting behind proprietary controllers, for the most part
    14:14 <jtara> and i feel that sw raid gives better indications of predictive failure for root mirroring
    14:14 <jasonwc> Yeah, I meant why would you trust a hardware RAID controller over a hardware RAID solution for RADI10?
    14:14 <jtara> plus zfs makes it really easy on solaris to do
    14:14 <jasonwc> "personally I would only do sw raid for a boot drive"
    14:14 <jtara> i didn't say i would, i don't think
    14:15 <jasonwc> Or do you just mean you would use ZFS for anything else?
    14:15 <jtara> i would use sw raid either with a mirror on a zpool or with something non-zfs i guess
    14:15 <jtara> back in the bad old days of sunos we used something very mdadm-ish that worked reasonably
    14:15 <pts0> -n3:0:+512M
    14:16 <pts0> does that mean to start where the last partition ended
    14:16 <jtara> oh i think i understand the misunderstanding
    14:16 <jtara> i mean, "for a boot drive, i would only use sw raid, not hw raid"
    14:16 <jasonwc> ah, got it
    14:16 <jtara> english is such a stupid language lol
    14:16 <jasonwc> pts0, Yeah, it starts at +512M
    14:16 <jtara> doesn't even have proper lambdas
    14:17 <jasonwc> Yeah, I interpreted it as I would only use SW raid for this one situation, and hardware RAID in all others.
    14:17 <jtara> ahh
    14:17 <pts0> i thought it meant to start where the last one ended and go 512M more
    14:18 <pts0> i'll play and see what it does
    14:18 <pts0> thanks guys for putting up with me
    14:18 <jasonwc> pts0, Since you seem uncomfortable with sgdisk, you can also achieve the same results with fdisk, which is definitely easier to use
    14:18 <jasonwc> pts0, But for a howto, it's better to use instructions that minimize the opportunity for error
    14:19 <jasonwc> You don't need to become an expert on sgdisk to use ZFS.
    14:19 <jtara> just don't be in a hurry to put mission critical stuff that requires all kind of stuff on it
    14:19 <jtara> play with it, destroy some pools, create some more, get a feel for it
    14:19 <jasonwc> yeah, I created several VM root pools before I did one on my real system
    14:19 <pts0> i see what the ns and ts match up now
    14:20 <jasonwc> And I had used ZFS for a non-root pool for a few years before using ZFS on root
    14:20 <pts0> it's specifying the partition number in both
    14:20 <pts0> so t4 to change the type of partition 4
    14:20 <jtara> we still don't use zfs for root here, but i'm trying to get it widely adopted for containers
    14:20 <pts0> i have nothing important to screw up so
    14:20 <pts0> meh
    14:20 <jtara> considering how many pools i'm jamming into a single physical it works pretty well
    14:20 <twb> Strictly, sgdisk is GPT sfdisk.  gdisk is the GPT fdisk.  The "s" is the non-interactive version, IIRC.
    14:21 <twb> I personally prefer libparted over g/fdisk, but the feature sets aren't identical.
    14:21 <jtara> pts0: that's an awesome place to be in!
    14:21 <pts0> well i'm also sickly, ugly and old so
    14:22 <jasonwc> jtara, Why have so many pools on a single disk?
    14:22 <jtara> our original use case had one pool per local container, as well as one "backup" pool for each container on remote block storage
    14:23 <jtara> going forward we may have multiple containers per pool but this has been the cleanest way to give them isolation so far
    14:23 <pts0> twb so sgdisk is fdisk
    14:24 <jtara> it lets each container either manage its own pool or delegates that to a backup pod
    14:24 <twb> fdisk and sfdisk are MBR; gdisk and sgdisk are GPT.  GPT is required for UEFI.  GPT is required for ≥4TiB disks.
    14:25 <jtara> plus it's mostly ssd so we're not nearly so iop constrained as long as each pool io plays nice
    14:26 <pts0> sfdisk is not fdisk?
    14:26 <jasonwc> On an unrelated topic, what's the best way for a consumer to obtain enterprise SSDs?
    14:27 <fling> Does not zfs support badblocks?
    14:27 <jasonwc> I only see them offered by 3rd party sellers that are surely not authorized retailers.  Do you have to buy in bulk?
    14:28 <pts0> the ones with s in front of them are for non-interactive?
    14:30 /join #7z
    14:30 /join #7zip
    14:48 <PMT> fling: I don't believe there's any mechanism, no
    14:50 <twb> jasonwc: what do you mean by "enterprise" SSDs?
    14:50 <twb> Different FTL?
    14:50 <fling> enterprise bytes!
    14:51 <jtara> different duty cycles, more overprovisioning, often pre burned in
    14:51 <twb> Fair enough
    14:51 <fling> jtara: what is pre burned?
    14:52 <twb> I didn't know they were A Thing, presumably because I'm not enterprisey enough to even get them offered to me :-)
    14:52 <jtara> normally ssd performance degrades over time as blocks fill
    14:53 <jtara> burning them in basically gets you to steady state performance instead of facing a steeply falling curve
    14:53 <jtara> in a lot of places consistent performance is more important than brief maximum performance basically
    14:53 <jtara> and big overprovisioning really diminishes erase cost
    14:55 <jtara> the other thing is...if you can run with big blocks on zfs and not raidz them, you make really sequential writes
    14:55 <jtara> so you can greatly minimize ssd erase cost
    14:56 <PMT> I mean, I imagine you could probably implement a badblocks equivalent assuming the region isn't in use, by permanently leaking it. :V
    14:57 <twb> Is badblocks even A Thing still?  Doesn't the HDD firmware have the equivalent internally, and when that runs out, you should throw the disk away
    14:57 <PMT> twb: in theory.
    15:32 <twb> http://ix.io/1GWp/rst  are my notes about my plan for booting w/ ZFS
    15:34 <rlaager> twb: #1 is not applicable. The initramfs will set noop on the disks in the root pool.
    15:34 <rlaager> I think "crap feature set" is a bit harsh. That was _the_ feature set not that long ago.
    15:34 <twb> OK :-)
    15:35 <twb> I haven't looked at exactly what the featureset is
    15:35 <rlaager> If you're going to use a bpool, what's the point in using refind instead of GRUB? Because you can't mirror the ESP? What type of motherboards are you using?
    15:36 <rlaager> Oh, nevermind, I misread.
    15:41 <rlaager> twb: You might be able to save some steps with this approach: 1) Always keep your FAT32-formatted zvol mounted at /boot. Mount it with "sync" for extra safety, especially in light of... 2) rsync the zvol to /dev/disk/by-id/TOSHIBA-xxxx "Whenever flashrom would run"
    15:42 <twb> ah so copy it in the other direction, basically
    15:42 <rlaager> Exactly. Then it should be "safe" to keep it mounted all the time, which is what I assume you were trying to avoid with the flash drive, which causes the dual re-mount steps.
    15:43 <twb> That would do (slightly) more writes to the USB key, but probably fine.  It would also hose any writes by the bootloader or mainboard (or other OSs) to the USB key...
    15:43 <rlaager> Would it write more?
    15:44 <rlaager> But yes, it would overwrite any changes from elsewhere. That is a downside.
    15:44 <twb> depends if the rsync blocks line up with the FAT and/or erase blocks, I guess
    15:46 <rlaager> I'm still not sure why you can't have multiple ESPs.
    15:46 <twb> You can have multiple disconnected ESPs, but you can't have them all be mirrored together without fighting the UEFI standard (AFAICT)
    15:47 <twb> If you have multiple ESPs, then things that aren't Linux that write to them will write to *one* of them, maybe the wrong one
    15:47 <rlaager> Correct. Do your systems actually write to them?
    15:47 <twb> I don't know :-)
    15:47 <twb> UEFI conformant devices are allowed to
    15:48 <twb> and IIUC refind supports it as an alternative to writing to the efi boot vars, because they're on a PROM with a very limited number of writes, and it's soldered onto the mainboard
    15:48 <rlaager> Also, to answer your question about NOP writes... if you set the checksum to one of the cryptographically secure choices, then ZFS will avoid writing blocks to disk that are the same as the existing blocks. That's the nop-write thing. That's pretty small potatoes here, though.
    15:49 <twb> Looking at your HOWTO, I must be reading it wrong, because it looks like you're turning on most features on the bpool, not the rpool?  e.g. feature@async_destroy=enabled
    15:49 <rlaager> I'm reasonably confident that my Supermicro boards do not have any features that write to the EFI partition and do not care about EFI variables. They're basically doing the "scan the disks" approach from BIOS booting, but with EFI. So for a setup like that, dd'ing them is pretty safe. Your systems may vary.
    15:50 <rlaager> You have different goals than me, so you have reached a different result. I think your approach is sound and well-thought-out.
    15:50 <rlaager> twb: The bpool has features disabled with -d, then individual features turned back on. The rpool takes the default, which is all features enabled.
    15:50 <twb> Ah, so *that's* what the -d does
    15:51 <twb> So let's see what features are actually different between bpool and rpool on a buster system...
    15:51 <rlaager> The bpool enables those features which GRUB supports, _plus_ all "read-only compatible" features, because GRUB is only reading the pool. That may change if we can get GRUB to write to the pool, even in a limited way, for the grubenv file.
    15:51 <lundman> Didn't I see a commit recently that finally lets us feature=disabled too
    15:52 <rlaager> There's also a reasonable debate to be had here about whether we should enable features "just because we can" or whether we should only able features that "matter" on the bpool.
    15:52 <rlaager> lundman: I think that exists, but I'm not 100% sure. I could use that, but this approach is safer if someone backports a newer ZFS or something.
    15:53 <rlaager> twb: multi_vdev_crash_dump (which is never used anyway), large_dnode, sha512, skein, and edonr.
    15:53 <lundman> yeps - just fresh in my mind as I cherry-picked it over last week
    15:53 <twb> I actually use ext2 for /boot quite often because YAGNI even for the journal and extents :-)
    15:53 <rlaager> twb: Oops, sorry, that was bionic, not buster.
    15:56 <twb> SHA-2 sounds important given that SHA-1 is orange since 2004 http://valaurora.org/hash.html  (damn, that link is dead today...)
    15:56 <rlaager> twb: Buster is the same as Bionic. 0.8.0 adds encryption, device_removal, obsolete_counts (part of device_removal, essentially), and bookmark_v2 (part of encryption, essentially)
    15:57 <rlaager> Neither is sha1. The sha512 feature is to allow you to use checksum=sha512 (which is SHA512/256) rather than checksum=sha256. But the default is checksum=fletcher4 anyway.
    15:57 <twb> Ah cool
    15:58 <twb> Is 0.8 likely to be "ready" in the next six months?
    15:58 <twb> at-rest FDE (without the LUKS hassle) is a "nice to have" for me
    15:58 <rlaager> From a practical standpoint, assuming you don't need device_removal, the main feature difference is large_dnode and/or encryption, with the latter being the killer difference.
    15:59 <rlaager> Yes, 0.8.0 will probably be released soon.
    15:59 <twb> Will it be possible to in-place upgrade from no encryption to encryption?
    15:59 <twb> Like turn it on and then resilver the pool?
    15:59 <FireSnake> turn it on for new data
    16:00 <FireSnake> for new datasets
    16:00 <FireSnake> not for existing
    16:00 <rlaager> Yes, in the sense that you can enable it on your pool on new _datasets_. But if you have sensitive data in the pool already, you may not want to do that, as that would leave sensitive data on the disk. You might want to wipe the disks.
    16:00 <twb> That makes sense
    16:01 <rlaager> Were you looking at Debian then?
    16:01 <twb> So even if I had e.g. /var/mail and made a new encrypted zfs and copied all the files from old-var-mail zfs to new-var-mail zfs, it wouldn't do any kinda secure erase of the old-var-mail disk blocks
    16:02 <twb> So remanence would still screw me unless I manually nwipe'd the drives or similar
    16:02 <rlaager> twb: You might want to compile the 0.8.0rc4 package from git. Here's the not-secret-but-not-linked-from-anywhere experimental HOWTO for that: https://github.com/zfsonlinux/zfs/wiki/Debian-Buster-Encrypted-Root-on-ZFS
    16:02 <twb> rlaager: oh!  I saw that page but my brain assumed it was the LUKS method still.
    16:04 <twb> Another dumb question: if I zfs send from a system without ZFS encryption, to a system *with* ZFS encryption, will the receiving host have FDE on the received snapshots?
    16:05 <rlaager> That depends on the properties inherited and such. It should be straightforward to achieve that result. Unless you need to receive the root dataset; that might be an issue. I haven't tested this, though.
    16:05 <twb> Right now my historical infrastructure looks like:   prod server → rsync → ZFS archive server → rsync → offsite DR server with FDE
    16:06 <twb> And my plan is to have end up with prod server w/ ZFS snaps → zfs send → offsite DR server with FDE
    16:06 <jtara> that can work well, zfs send/receive can work much better for some workloads
    16:06 <jtara> but for some its not that important
    16:07 <rlaager> twb: Right, so if I were you, I'd probably start by replacing the offsite DR server with ZFS encryption. That would result in: prod server → rsync → ZFS archive server → zfs send → offsite DR server with FDE
    16:07 <rlaager> That's a minimal change. Assuming that works, then convert to the end game.
    16:08 <twb> that's fair, although right now the budget prefers me to upgrade the prod server first
    16:09 <rlaager> I proposed a software change. Unless you're talking about labor budgets, there's no spending involved. ;)
    16:09 <twb> as long as the offsite host will have FDE, I don't *really* care
    16:09 <twb> rlaager: well, I gotta have enough capacity to have the old and new setups running side-by-side briefly, or be confident enough to do it in-place
    16:10 <rlaager> Sure, there's nothing wrong with doing it the other direction.
    16:10 <jtara> do you want to apply the incrementals on the remote end
    16:10 <jtara> or just store periodic full and incremental sends
    16:10 <jtara> both work, just different approaches
    16:11 <twb> The end goal is to have all the archive snapshots on the offsite host.  Right now, the offsite host only has the latest state
    16:11 <rlaager> If bandwidth is limited, it may actually be better to start with the prod server. This could avoid the need to send two full copies during the migration process.
    16:11 <twb> Right now the bandwidth is about 1Mbit/s, but hopefully that'll go to about 10Mbit/s in a couple of months
    16:12 <twb> But for initial syncs I can just give someone a USB HDD
    16:12 <rlaager> Yeah, do the prod server first. Get your zfs send | recv worked out on site, between prod and ZFS archive server. Once that's golden, then it's just a matter of converting the DR server to ZFS and doing one big send | recv.
    16:12 <twb> The offsite host is only DR ("the building burns down") anyway, so it's not a major concern
    16:13 <rlaager> For your initial sync, you can do send | recv to a pool on a USB drive, then have someone drive it to the DR server, then send | recv from USB to the DR pool. Thereafter, just do incrementals.
    16:14 <twb> yeah or even just zfs send >/mnt/usb-hdd/zfs-send.img
    16:14 <rlaager> Sure, that works too.
    16:21 <twb> What does device_removal get me?  If a disk is dying, I want to replace it while the system is running.
    16:29 <twb> FTR, https://valerieaurora.org/hash.html  was the URL I was trying for earlier.
    16:29 <zfs-bot> [ Lifetimes of cryptographic hash functions ] - valerieaurora.org
    16:34 <rlaager> device_removal allows you to remove a top-level vdev, subject to some limitations. For example, if you accidentally `zpool add` instead of `zpool attach`. Or if you want to reduce something from N striped mirrors to N-1.
    16:36 <twb> But not relevant for "disk WDC-xxx is dead and I need to replace it with new blank disk WDX-yyy" ?
    16:37 <rlaager> Correct. That's just a regular replacement that's worked since "forever".
    16:37 <twb> Yay


    18:13 <twb> https://github.com/zfsonlinux/zfs/wiki/Debian-Buster-Encrypted-Root-on-ZFS says- oh never mind.  It's written for the future, after ftpmasters move 0.8~rc from NEW to experimental.
    18:15 <sphalerite> jtara: not so much a use case as "what happens if I accidentally move the disks around badly, and how afraid should I be of it?"
    18:15 [twb tries to remember why CCM or GCM is better than plain old CBC or CTR]
    18:16 <lundman> position based ivset generation?
    18:16 <twb> Ah, they appear to do the authentication AND encryption as mode, rather than chaining CBC/OTR and a separate MAC
    18:17 <twb> Originally they were separate, which meant that everyone screwed up combining them
    18:17 <twb> And most of my textbooks are from that era :-)
    18:19 <twb> GCM was first published in 2005; CCM in 2003.
    18:21 <twb> Ohhh does 0.8 add zfs-mount-generator for systemd?
    18:21 <twb> Because that's something I've been worrying about
    18:21 <twb> (the lack of it)
    18:23 <jtara> sphalerite: basically if you totally remove a disk and then import a pool it will come up as degraded
    18:23 <jtara> and you'll need to take steps to really break the mirror so to speak
    18:25 <jtara> my security guys are really happy about tightly coupled authenticated encryption, i think that's going to be a deciding factor
    18:26 <twb> Yeah it makes sense
    18:27 <twb> I'm just 15 years behind best *current* practice :-)
    18:27 <jtara> yes well :)
    18:28 <jtara> the thing a lot of places have issues with is that pool metadata itself is not encrypted
    18:28 <jtara> volume names, properties, on-disk zfs structure metadata
    18:28 <twb> Not at all, not ever?
    18:29 <jtara> the whole point is you want a pool to be able to manage data it doesn't have the key for
    18:29 <jtara> so all the payload data itself is secure
    18:29 <twb> IIRC ecryptfs had that problem
    18:29 <jtara> well none of the structural metadata contains anything revealing
    18:29 <jtara> it's just "this chunk is linked to this node"
    18:30 <twb> Oh hang on.  When you say pool metadata, you don't mean dirents
    18:30 <jtara> right
    18:30 <twb> OK you just mean like the equivalent of LVM and md metadata -- "I'm a RAIDn array with three disks named X, Y and Z" sort of metadata
    18:31 <jtara> well, kind of
    18:31 <jtara> basically the metadata block for each leaf data block has things like its checksum
    18:31 <jtara> and where the data block itself is stored
    18:31 <twb> Are the names and snapshot times of the zvols/zfses encrypted?
    18:31 <jtara> so you might know that there is a file or volume X, which has at position Y a chunk of data Z
    18:31 <jtara> but you don't know what X is or what the data inside Z says
    18:32 <twb> Or can the attacker see e.g. there is a snapshot called illegal-porn@2007-01-01
    18:32 <jtara> snapshot and volume names are in the clear
    18:32 <jtara> so that as you apply deltas you can manipulate them
    18:32 <twb> Oh, another dumb question then
    18:32 <jtara> like, you can apply an incremental zfs send of encrypted data, then delete an old snap to do an incremental roll
    18:33 <jtara> and you never need the encryption keys to do so
    18:33 <twb> If I zfs send encrypted data to an offsite backup, does and offsite host just holds it, never "uses" it, does the offsite host *ever* have to decrypt it?
    18:33 <jtara> honestly if you have people who work with you who manage to hide useful information in pool or dataset names, more power to you, i can't get mine to :p
    18:33 <jtara> it never has to decrypt it
    18:34 <twb> that's awesome
    18:34 <jtara> you can zfs receive into that pool without the key, and zfs send out of it without the key
    18:34 <jtara> yeah
    18:35 <jtara> and you can rekey things without reencryption
    18:35 <jtara> hopefully i can take advantage of that to give my backups a specific key per time period, while keeping all the source pools with totally random keys

    18:52 <twb> rlaager: BTW, you can do "apt-get build-dep" on a directory, rather than having to copy things from debian/control Build-Depends by hand
    18:52 <twb> e.g. git clone .../zfs.git && sudo apt-get build-dep ./zfs
    19:14 <twb> 18:21 <twb> Ohhh does 0.8 add zfs-mount-generator for systemd?
    19:14 <twb> ...I can see in git that it does.
    19:32 <twb> Boy the ZOL codebase does a lot of $(MAKE) -C
    19:33 [twb waves the RMCH paper around grumpily]

    19:41 <twb> X: zfsutils-linux: missing-systemd-timer-for-cron-script etc/cron.d/zfsutils-linux
    19:41 <twb> 24 0 8-14 * * root [ $(date +\%w) -eq 0 ] && [ -x /usr/lib/zfs-linux/scrub ] && /usr/lib/zfs-linux/scrub  # Scrub the second Sunday of every month.  
    19:41 <twb> That looks pretty straightforward...
    19:42 <twb> zfs-scrub.timer: [Timer] OnCalendar=Sun/2 *-*-*
    19:43 <twb> zfs-scrub.service: [Service] Type=oneshot ExecStart=/usr/lib/zfs-linux/scrub
    19:43 <fling> Lalufu: not even in master yet?
    19:43 <Lalufu> I couldn't say
    19:43 <Lalufu> it might be
    19:43 [fling checking]
    19:44 <fling> Is not trim eating data anymore?
    19:44 <twb> Ah my math might be a little wrong, I think mine would run "once per fortnight, on sundays" rather than "once per month, on the second sunday of that month"
    19:45 <twb> OnCalendar=Sun *-*-8..14 00:24:00
    19:48 <twb> looks good to me
    19:48 <twb> "NEXT: Sun 2019-05-12 00:24:00 AEST  LEFT: 2 weeks 4 days left "

    19:54 <twb> Wow.  I discovered arc_summary, and looking at a prod ZFS pool, I can see L1ARC "Actual Hit Ratio" is 82%, where L2ARC "Hit Ratio" is only 5%
    19:54 <twb> Meaning (I think) that the L2ARC really isn't helping much

2019-04-23 ##eros-chat::

    18:24 <twb> MTecknology: so are you moving things from NEW queue still
    18:24 <twb> MTecknology: because as it happens zfs 0.8 is in there right now, wanting to move from NEW to experimental
    18:24 <twb> MTecknology: and that version fixes systemd integration AND adds full-disk encryption
    18:25 <MTecknology> that sounds like an unpleasant review
    18:25 <twb> unpleasant for you maybe but convenient for me when it's done ;P
    18:25 <twb> AIUI it's only going into experimental ANYWAY so it's not like it should be a big deal, except maybe licensing?
    18:26 <twb> I don't know where the ftpmasters keep their notes about NEW queue, though
    18:30 <MTecknology> It's in a postgresql db that's kind of unpleasant to attach to.
    18:30 <MTecknology> I don't see any notes on it
    18:47 <twb> righto
    18:48 <twb> Do you know the URL to dget stuff from NEW queue?  It's kinda hidden because they don't want people playing with it
    18:53 <MTecknology> I don't see anything available. That should probably be a new addition
    18:54 <twb> don't worry the upstream instructures are building it from a git on salsa anyway, which is near enough
    18:56 <MTecknology> Have you ever used pbuilder or sbuild?
    19:11 <twb> MTecknology: not for a decade

2019-04-24 #flashrom::

    18:43 <twb> I have a dumb question.
                I have a boring Debian x86_64 system, with a boring FAT32 ESP, and
                I want to copy /boot into the ESP anytime /boot changes.
                Somebody told me "flashrom does basically that, for more complicated hardware,
                  where the number of erase cycles is really small.
                  So it should already have OS integration to know WHEN to reflash the kernel and ramdisk."
    18:43 <twb> But I'm looking at the flashrom package and all I see is the binary itself,
                no hooks into apt or update-initramfs or anything.
    18:44 <twb> Am I looking wrong, or was my friend too optimistic?
    18:54 <twb> Ah sorry to bother you.  It seems I was confusing flashrom with flash-kernel
    19:14 <Hellsenberg> twb: I think you did :P

2019-04-24 #debian-arm::

    18:48 <twb> Hey, suppose I have a dumb system, where just dropping a new kernel and ramdisk into /boot isn't enough, but some extra flashing process has to happen afterward.  What part of Debian makes that actually happen?  I thought flashrom had a update-initramfs hook or dpkg trigger, but I don't see it.
    18:50 <hrw> twb: flash-kernel?
    18:51 [twb starts poking around the installation-guide-armhf and-]
    18:51 <twb> ah, thanks
    18:51 <hrw> flashrom is a tool to update bios chips and other flashable firmware chips
    18:54 <twb> My use case is pretty silly.  I'm actually on x86-64 UEFI, and for stupid reasons I want to just copy /boot into the ESP anytime /boot changes.  Rather than guessing /etc/kernel/postinst.d/ or something, I wanted to see see how other people were already solving this, and steal their answers
    18:55 <hrw> twb: you know that you do not need to have kernels in ESP?
    18:56 <twb> I know, but then I have to move /boot out of my main ZFS pool
    18:56 [pabs3 would just run rsync from an apt hook]
    18:59 <twb> pabs3: yeah that would be the 90% right solution.  That wouldn't cover e.g. tweaking /etc/initramfs-tools/conf.d/ and then doing an "update-initramfs -u"
    19:00 <pabs3> ack, I would only ever tweak that via apt
    19:00 <hrw> twb: maybe you need to tweak grub to read zfs?
    19:00 <twb> hrw: grub2.02/efifs1.3 can't read ZFS pools with some newer features enabled - notably ZFS-native encryption
    19:01 <twb> http://ix.io/1H3C if you want the gory details
    19:10 <hrw> twb: make non-encrypted pool for /boot maybe?
    19:10 <twb> hrw: at which point, it needs to live somewhere
    19:13 <hrw> true
    19:23 <twb> Looks like flash-kernel adds /etc/initramfs/post-update.d/ *and*
                                             /etc/kernel/{postinst,postrm}.d/, and also
          a dpkg trigger for ITSELF, and also
          an initramfs-tools hook to (I think) resolve root=UUID=X in advance,
          in case udev/libblkid isn't available at boot time.

2019-04-24 #zfsonlinux::

    19:50 <twb> OK help me out here.  I have a pool of disks and I want to take down the pool and wipe the metadata, prior to creating a new pool
    19:51 <twb> umount -a -t zfs and zpool export -a are refusing to get rid of some stuff, even though lsof says nothing is actually open
    19:51 <twb> so I went "sod it" and tried to just do "wipefs -a /dev/sda", and even THAT fails: wipefs: error: /dev/sda: probing initialization failed: Device or resource busy
    19:56 <twb> Unrelated: for a small SATA/AHCI server, is it better to use /dev/disk/by-id/wwn-XXX or /dev/disk/by-id/ata-XXX ?  The FAQ isn't very clear.

2019-04-24 #parted::

    21:32 <twb> gparted can resize the *filesystem* in a FAT32 ESP GPT partition.
    21:32 <twb> AFAICT, parted can't actually do that.
    21:33 <twb> But gparted appears to just be asking libparted to do it, so... WTF?  Is filesystem resizing limited to people without visual impairment?
    21:33 <twb> (parted used to have "resize", but current versions only seem to have "resizepart", which explicitly doesn't affect the filesystem, only the partition)
    21:35 <twb> libparted definitely has libparted/fs/r/fat/resize.c:fat_resize()
    21:35 <twb> So how do I access that functionality, without a GUI?
    21:49 <twb> A-ha!  This isn't supported by any of the normal fat packages (e.g. dosfstools), but there is a package that does *just* this, using libparted for the actual work.  http://sf.net/projects/fatresize
    21:49 <twb> Sorry I lost my temper

2019-04-24 #emacs::

    <RANTING ABOUT #PARTED>
    21:46 <npostavs> twb: https://manned.org/fatresize.1 maybe?
    21:47 <twb> npostavs: ah thanks
    21:47 <twb> That exists in Debian
    21:47 <twb> I was looking under dosfstools and similar
    21:50 <mkletzan> And it looks like fatresize is in portage as well.  Stupid me for trying to eat more healthy and get some exercise...
    21:51 <twb> It looks at a glance like fatresize was basically someone who was in the same position as me, but a couple of years ago
    21:51 <twb> And unlike me, they actually cared enough to solve it for other people
    21:52 <mkletzan> otoh the efi partition is usually small enough to copy the data elsewhere, reformat and copy back
    21:53 <twb> mkletzan: that requires me to get things like the fat size correct, and it's easy to fuck up such things
    21:53 <twb> e.g. mkfs.vfat on a 4MB disk will be FAT12 or 16 by default, and then SOME firmwares will ignore it
    21:53 <mkletzan> `-F 32` ?
    21:54 <twb> yeah, but you have to notice that and remember to use it
    21:54 <twb> "grow the thing that's already there and works" is (hopefully) harder to fuck up
    21:54 <mkletzan> agreed
    21:54 <twb> hahaha
    21:54 <twb> Note ==== You can't resize FAT32 partition lesser than 512Mb because Windows(R) doesn't work properly with small FAT32 file system. Use FAT16.
    21:55 <twb> I think they mean MiB not Mbit

2019-04-26 #systemd::

    15:18 <twb> Hey, you know how having systemd in the ramdisk is generally a Good Thing?
    15:19 <twb> And debian and arch don't do that by default, and for arch it's just a simple config option.
    15:19 <twb> On debian, is "just use dracut (not initramfs-tools)" the best way to get a systemdized ramdisk?
    15:21 <Xogium> twb: yeah that's what I do here. Except I generate the initramfs as a standard cpio archive that I embed into the kernel, via buildroot. My initramfs contains systemd as init, busybox for all the shells and utilities, and whatever deps from util-linux is required. 4 mb in size compressed in xz
    15:21 <Xogium> I'm just not sure why systemd doesn't seem to recognize that it is inside an initramfs
    15:22 <twb> Xogium: are you doing that via dracut, or are you doing some kind of hand-rolled business?
    15:22 <Xogium> I do have /etc/initrd-released inside, or forcing initrd.target on kernel cmdline would have failed spectacularly
    15:23 <Xogium> twb: no dracut, no mkinitcpio. But buildroot is able to create a cpio archive suitable for use as an initramfs with whatever packages you selected in your rootfs
    15:23 <twb> My immediate use case is I'm rolling a modern Debian Buster system with multi-disk ZFS for root (with a zfs-mount-generator), and I'm hoping dracut will be more future-proof-y.
    15:24 <twb> #systemd used to get people with confused systems because init was plain sh and / was multi-disk btrfs, which made me afraid
    15:24 <twb> I mean /bin/init in the initrd was a sh script
    15:24 <Xogium> heh I can understand
    15:25 <grawity> Arch's sh-based initramfs just runs `btrfs device scan`
    15:25 <grawity> not sure what archzfs stuff does, but it also *seems* to work fine with my pool, so I haven't really tried systemd initramfs on the fileserver yet
    15:26 <grawity> (it doesn't help that the initial 242 release completely broke systemd-initramfs for everyone)
    15:26 <Xogium> here /bin/init points to systemd, but it is very minimal, contains only one or two other things, not even networkd, since I don't actually need it, having a serial console whenever I want
    15:26 <Xogium> grawity: did it ?
    15:26 <grawity> so say the people affected by it
    15:26 <twb> Xogium: yeah I accept that it works for you, but you're being a bit more "unique snowflake" than I have in mind for my current system
    15:27 <grawity> Xogium: https://git.archlinux.org/svntogit/packages.git/commit/trunk?h=packages/systemd&id=aa6c53bf7c18ef733da2bcf3e33324dda151b39f
    15:27 <Xogium> I'm on systemd 242.0-1 and I had no problem, though I've seen people complain a LOT
    15:28 <grawity> my dev system went from 241-git to 242-git and I haven't actually booted it to Linux for two weeks anyway, so I haven't experienced this personally
    15:28 <Xogium> twb: hehe well I'm simply trying to use this new systemd.volatile=overlay for my tiny system here
    15:29 <twb> I'm not familiar with that
    15:29 <twb> I haven't really kept track since v219
    15:29 <Xogium> and I do need an initramfs for this, so I had to figure how to do it in buildroot
    15:30 <Xogium> new stuff in 242, mounts the root device read-only with a writable tmpfs on top using overlayfs, that way anything you modify there is lost after a reboot, perfet for what I plan here, making the system go back to sane default
    15:30 <Xogium> *perfect even
    15:31 <twb> Looks like I'll be on v241, at least initially.
    15:32 <Xogium> grawity: haha I made sure to reboot right after the update, just to see what it'd do, and I didn't have such problem lucky for me I imagine
    15:33 <Xogium> that being said I think someone had an even weirder issue, this is the second time I see a machine getting stuck on 'loading initial ramdisk...'
    15:37 <Xogium> anyway, for what I'm doing here, clearly using rd.systemd.unit= on cmdline is a workaround, but I'm still not understanding why it treats the initramfs as a normal rootfs without this, I also have /etc/initrd-release into it
    15:38 <Xogium> might be coming from the fact I have a fully fledged systemd in the filesystem, and not just a couple binaries and services with targets such as multi-user.target for example, I have no idea
    15:39 <Xogium> I've been trying to do the best of both worlds with buildroot in this case
    15:43 <twb> Xogium: without that, how does systemd (running in the initrd) know whether the initrd is the "final system", or if it's destined to mount /root and switch_root?
    15:44 <twb> Because you might just be packing the final rootfs into the initrd and then stopping, e.g. for a smartbulb firmware
    15:46 *** [diablo] JOIN
    15:46 <Xogium> I'm not sure that's what I've been trying to find out, how does it determine to be in an initramfs at all… boucman_work found that it apparently just check for /etc/initrd-release, so I'm guessing that I have a bunch of stuff that I'd need to take out of my initramfs or something, because it doesn't react at all how I expect it to
    15:48 *** oiaohm QUIT Read error: Connection reset by peer
    15:48 *** oiaohm JOIN
    15:52 <Xogium> hmm
    15:53 <Xogium> okay so by examining the archlinux initramfs they just pack a lot less systemd stuff than me
    15:54 <Xogium> but they've made the default target be initrd.target in the first place, so I could also just do the same and it would work, my default target is multi-user.target because that's how buildroot makes it by default
    15:55 <twb> bootup(7) has some info about that
    15:58 <Xogium> yeah, the charts are a bit difficut to parse with my screen reader, but I think I got the basic idea of it
    15:58 <Xogium> *difficult
    15:59 <twb> Xogium: ah ouch.  I can manually convert it to graphviz-format if it helps
    15:59 [twb is sighted]
    16:00 <Xogium> hehe I've never used graph, so I'm not at all sure how helpful that would actually be
    16:00 <twb> Ruh Roh, in Debian, intel-microcode might not work with dracut
    16:01 <Xogium> well that's annoying
    16:01 <twb> amd-microcode, by comparison, explicitly supports either initramfs-tools or dracut or tiny-initramfs
    16:01 <twb> (tiny-initramfs is similar design to RH ash - bare minimum all in one C binary as /init)

2019-04-26 #debian-kernel::

    15:31 <twb> Hey, dumb question.  Does/will buster have something like ksplice, so I can get kernel bugfixes without a full reboot?  ISTR there was mainline replacement for ksplice in the news recently...
    15:34 <pabs> there isn't a Debian service producing patches yet, Ubuntu has one though
    15:34 <pabs> I haven't heard of anyone working on one either
    15:35 <twb> Oh.  I assumed that happened automagically by just having the finished kernels in /boot :P
    15:36 <twb> It looks like the low-level functionality is in mainline 4.0, shared between kpatch (RH) and kgraft (SUSE)
    15:37 <pabs> I got the impression it needs a service, not sure tho https://linux-audit.com/livepatch-linux-kernel-updates-without-rebooting/
    15:38 <twb> FTR the high-level goal is to bypass the "file a Change Request and wait 6 weeks for approval" step.  The actual reboot itself wouldn't be a big deal.
    15:38 <pabs> shame ksplice didn't get merged, this new feature looks pretty useful: https://blogs.oracle.com/linux/using-ksplice-to-detect-exploit-attempts
    15:39 <twb> pabs: AFAICT oracle bought ksplice and shut it down
    15:39 <twb> pabs: which was when RH et al spun up their own replacements
    15:40 <pabs> seems like it is still available, gratis for Ubuntu/Fedora https://ksplice.oracle.com/
    15:42 <twb> "Support for RHEL changed to a free 30-day trial for RHEL customers as an incentive to migrate to Oracle^[5]^[6] and use of the Oracle Unbreakable Enterprise Kernel (UEK) became mandatory for Ksplice on production systems.^[7]"
    15:42 <twb> So yeah, it's available on desktops but they clearly want to lock in all the serevrs
    15:42 <pabs> apparently one can build ones own patches, the tools aren't in Debian tho http://chrisarges.net/2015/09/21/livepatch-on-ubuntu.html
    15:43 <pabs> (linked from https://blog.dustinkirkland.com/2016/10/canonical-livepatch.html)
    15:44 <pabs> woops, kpatch is in Debian
    15:45 <twb> Can I do that from pre-built debian kernels, or would I have to roll my own kernels from the .DSCs?
    15:53 <pabs> kpatch-build apparently relies on building the source twice with extra options. kgraft seems to use debug info from the old/new kernels
    15:54 <twb> cool
    15:54 <twb> but kgraft isn't in debian yet
    15:55 <twb> So probably this won't be "Just Work" until debian 11
    15:55 <pabs> (also not sure if that is outdated info)
    15:55 <pabs> I think this is the first time I've seen anyone in/around Debian show interest in the livepatching stuff
    15:57 <twb> I'm just sick of $customer's admin people dicking me around
    15:57 <twb> I don't care enough to do a lot of work, but if you'd said "oh yeah just apt install XXX" then I'd be super happy :-)
    15:58 <twb> kexec used to be that simple before systemd
    15:58 <twb> so all my servers with 5 minute POSTs got nicer overnight
    16:05 <pabs> hmm, can't find the code to kgraft userspace
    16:12 <pabs> aha https://git.kernel.org/pub/scm/linux/kernel/git/jirislaby/kgraft.git/
    16:13 <pabs> oh that is the old kernel stuff
    16:17 <pabs> Gentoo's solution: https://wiki.gentoo.org/wiki/Elivepatch https://linuxplumbersconf.org/event/2/contributions/258/attachments/55/62/Elivepatch_Kernel_Summit_20182.pdf

2019-04-26 #zfsonlinux::

    12:40 <twb> Why does rlaager's Debian HOWTO use normalization=formD?  Didn't Apple used to force NFKD in HFS+, and realized it was a bad idea, and stopped doing it in APFS?
    12:41 <twb> (Specifically, the badness happens when you have paths inside a file, e.g. in a .diff or .tar, created on a system with a different normal form.)
    12:41 [twb checks the zfs manpages]
    12:49 <CompanionCube> iirc the way apple normalizes things does indeed suck
    12:49 <twb> CompanionCube: so basically the way ZFS does it, doesn't have the same problem?
    12:51 <WrongDevice> hello
    12:51 <WrongDevice> This pool uses the following feature(s) not supported by this system: 	org.zfsonlinux:userobj_accounting 	org.zfsonlinux:project_quota
    12:51 <WrongDevice> i can disable those ?
    12:51 <WrongDevice> without recreate the zpool ?
    12:52 <WrongDevice> im trying to mount my zpool created with ZOL 0.8.0-rc4
    12:52 <CompanionCube> on?
    12:52 <WrongDevice> i want to mount it on mojave
    12:53 <WrongDevice> using openzfs osx 1.9.0
    12:53 <WrongDevice> last version
    12:53 <WrongDevice> cannot be disabled those features ?
    12:53 <WrongDevice> removed
    12:53 <WrongDevice> i cannot write
    12:54 <WrongDevice> and i moved from btrfs just to write my files from mac to linux
    12:54 *** ChanServ MODE +v behlendorf
    12:54 <twb> WrongDevice: I think you mean "can I disable those features, so I can use the pool from MacOS?"
    12:54 <WrongDevice> thats why i moved to zfs inthe first place
    12:54 <WrongDevice> yes
    12:54 <WrongDevice> yes
    12:55 <WrongDevice> no
    12:55 <WrongDevice> ok yes
    12:55 <CompanionCube> you can disable those features...at pool creation time
    12:55 <WrongDevice> im confused
    12:55 <twb> I don't know if you can disable them from an existing pool.  You can definitely make a new pool with those features never turned on.
    12:55 <WrongDevice> omg
    12:55 <Selavi> I don't think so. enabling features changes the on-disk format. a ZFS without those features won't know what to do with the extra info, and you don't want to write to a pool with unknown features as it could corrupt things
    12:55 <WrongDevice> no
    12:56 <WrongDevice> ok
    12:57 <Selavi> those are both read-only compatible, so you can read from it at least
    12:57 <WrongDevice> and whats the way to create a pool
    12:57 <twb> Could WrongDevice do something like "zpool set -o featureX=disabled pool", then resilver?
    12:57 <WrongDevice> no
    12:57 <CompanionCube> twb: nope.
    12:57 <CompanionCube> WrongDevice: if you don't know that...how did you make the pool in the first place?
    12:57 <WrongDevice> zpool create -f zroot /dev/disk/by-id/id-to-partition-partx
    12:57 <WrongDevice> that
    12:57 <WrongDevice> ^
    12:58 <WrongDevice> command
    12:58 <CompanionCube> OK then
    12:58 <WrongDevice> what i need to add to that command disabling the two features i dont want
    12:59 <WrongDevice> ?
    12:59 <twb> WrongDevice: -o xxx=disabled, I think, for each of the "bad" features
    13:00 <WrongDevice> ok
    13:01 <Selavi> do you have to use the full flag name with the project prefix, or just simple name? I assume simple?
    13:01 <twb> I *think* there is also an option called something like "grub" or "portable" which tries to turn off bad things
    13:01 <CompanionCube> nope
    13:02 <WrongDevice> what i need to do for future reference to make a pool compatible with freebsd too
    13:02 <WrongDevice> and the truebsd is using zol now , all the zol features are compatible with trueos zol ?
    13:03 <twb> There's a table in the wiki somewhere of all the features and what systems support which ones
    13:04 <WrongDevice> where ?
    13:04 <twb> http://www.open-zfs.org/wiki/Feature_Flags
    13:04 <zfs-bot> [ Feature Flags - OpenZFS ] - www.open-zfs.org
    13:04 <twb> Sorry I'm a bit slow today, I'm not in my real office
    13:04 <jasonwc> twb, There is discussion about making such a flag.  It does not exist at this time.
    13:05 <twb> jasonwc: ah thanks.  I misremembered that it existed but didn't work very well :-)
    13:05 <jasonwc> WrongDevice, First off, do you need write access to this pool? If not, you can simply mount it read-only on OS X since those flags are read-only compatible.
    13:06 <twb> Is userobj_accounting similar to ext4's usrjquota?  i.e. per-UID EDQUOT block and inode caps?
    13:06 <jasonwc> WrongDevice, Relatively few features are not read-only compatible.
    13:06 <WrongDevice> yes i want to write
    13:07 <jasonwc> twb, man zpool-features says " This feature allows administrators to account the object usage information by user and group."
    13:08 <twb> I want to limit $HOME for human users to 1GB each, but I planned to do that by just giving each one a separate dataset with a cap (i.e. zfs create -o quota=1G pool/home/alice)
    13:09 <jasonwc> WrongDevice, Based on the feature flag chart, for OS X, you'll want to disable encryption, resilver_defer, allocation_class, large_dnode, project_quota, and userobj_accounting
    13:09 <lundman> depends what you run? 1.9.0 only project_quota and userobj_accounting
    13:10 <twb> lundman: 12:53 <WrongDevice> using openzfs osx 1.9.0
    13:10 <lundman> i only read 5 lines above, more is too heavy!
    13:10 <CompanionCube> but question was 'what flags for freebsd' though?
    13:10 <jasonwc> WrongDevice, The other ones are likely enabled but not active so they aren't causing issues.  Encryption won't be active unless you create an encrypted dataset.  resilver_defer is only active during a deferred resilver. Allocation classes would require adding a special vdev.  Large_dnode would require setting dnode size to something other than the default of legacy
    13:10 <CompanionCube> though the answer to that will be changing in the medium-term :p
    13:11 <jasonwc> WrongDevice, but you're getting hit by userobj_accounting and project_quota because they become active as soon as they are enabled and can't be disabled
    13:11 <jasonwc> the others can be disabled if not active
    13:11 <WrongDevice> lundman: hello
    13:11 <lundman> o/
    13:11 <WrongDevice> yeah 1.9.0
    13:11 <lundman> I only cut that this morning
    13:12 <jasonwc> lundman, The feature chart should be updated then.  It shows resilver_defer as unsupported on OS X master
    13:13 <WrongDevice> ok recreating now
    13:13 <lundman> it should
    13:13 <WrongDevice> i have to reboot to a liveusb
    13:13 <twb> WrongDevice: good luck! :-)
    13:13 <WrongDevice> yeah
    13:14 <WrongDevice>  thanks
    13:14 <lundman> good thing I added feature=disabled too
    13:15 <WrongDevice> i will no need to do disable anything else for use freebsd , i mean freebsd is too broken for my laptop atm , and too damm old graphic stack , this hackintosh works 100 times better than freebsd
    13:16 <WrongDevice> lundman: nice so this means i can create the pool here now and use it for linux
    13:16 <WrongDevice> maybe not good idea
    13:16 <WrongDevice> ...
    13:18 <jasonwc> WrongDevice, From looking at the feature chart, assuming you don't change defaults, if you disable project_quota and userobj_accounting, you should be fine on FreeBSD 12 as well
    13:18 <WrongDevice> lundman: one thing how i create a hostid spl hostid on mac ? or show i set a spl hostid like i set it on the linux kernel command line ?
    13:19 <WrongDevice> jasonwc: good to know thank u
    13:19 <WrongDevice> the table is not easy to read for me
    13:19 <WrongDevice> but i understand what i was looking for to solve my little issue with osx
    13:19 <lundman> it generates a hostid if not set. So either set it with sysctl, or just let it set for you
    13:20 <WrongDevice> lundman: i need to set the . same spl hostid for all my oses
    13:20 <lundman> because?
    13:20 <WrongDevice> i know how to do this before but i forgot
    13:20 <WrongDevice> to not have to import -f and export all the time
    13:21 <WrongDevice> im using  clover
    13:21 <lundman> just import -f on boot, no need to export
    13:21 <WrongDevice> any way to set it in clover ?
    13:21 <twb> WrongDevice: probably /etc/hostid doesn't match in your live system and real system
    13:21 <twb> WrongDevice: just copy /etc/hostid from one system to the other, so they have the same value
    13:21 <WrongDevice> i have two linux using the same hostid
    13:22 <WrongDevice> no is for osx
    13:22 <WrongDevice> that only maybe work on linux
    13:22 <WrongDevice> i dont know
    13:22 <twb> WrongDevice: ah, I misunderstood the question, sorry
    13:23 <CompanionCube> heh, you can see the results of https://github.com/zfsonlinux/zfs/pull/8641 on one of the feature tables :p
    13:23 <zfs-bot> [GitHub] [zfsonlinux/zfs #8641] rlaager: zpool-features(5) and other man page fixes | ### Motivation and Context This fixes some issues with zpool-features(5) and zfs-module-parameters(5)...
    13:24 <WrongDevice> lundman: ok how is set with sysctl ?
    13:25 <lundman> sysctl kern.hostid=
    13:25 <WrongDevice> where i can see my current spl.hostid
    13:25 <lundman> afaik
    13:25 <WrongDevice> ok
    13:25 <twb> and sysctl -a should print all the things it knows about, so "sysctl -a | grep hostid" should tell you something useful.
    13:26 <WrongDevice> hmm this number is bigger than the ones on linux
    13:26 <WrongDevice> on linux are jsut 8 chars
    13:26 <lundman> sysctl -x kern.hostid
    13:26 <twb> WrongDevice: AFAIK hostid is always a 32-bit integer
    13:26 <twb> WrongDevice: so the biggest value should be 4294967296
    13:27 <lundman> sysctl kern.hostid=0xde816bec
    13:27 <WrongDevice> nice
    13:28 <WrongDevice> yeah now i just need to use this one from osx on all the others linuxes
    13:28 <WrongDevice> thank u
    13:28 <twb> https://bugs.debian.org/595790  has a discussion of the "backstory" of hostid
    13:28 <zfs-bot> [ #595790 - The value from gethostid() should be more unique and not change when the host IP changes - Debian Bug report logs ] - bugs.debian.org
    13:29 <lundman> Fowler/Noll/Vo FNV-1a hash of your ioplatformuuidstr
    13:29 <lundman> if you want the details :)
    13:29 <lundman> (on osx)
    13:30 <twb> Is ZFS ever likely to get Zstd or LZMA2?  I'm thinking mainly of /var/log/ (write-once read-never).
    13:32 <jasonwc> twb, There's already a working PR for zstd on ZoL
    13:32 <twb> Cool
    13:33 <jasonwc> twb, https://github.com/zfsonlinux/zfs/pull/8044
    13:33 <zfs-bot> [GitHub] [zfsonlinux/zfs #8044] BrainSlayer: Support zstd compression (port of Allan Judes patch from FreeBSD) | This Patch adds zstd compression support zo ZFS ...
    13:33 <jasonwc> I was told it works fine aside from requiring more memory than other compressors
    13:33 <jasonwc> should be faster than gzip with better compression but much slower than lz4
    13:33 <twb> What abuot blake2? :-)
    13:33 <CompanionCube> originated from a freebsd diff, no? Still not merged in either, though.
    13:34 <jasonwc> yeah
    13:34 <jasonwc> Allan Jude wrote the FreeBSD one
    13:35 <jasonwc> I believe there is an issue if compressed ARC is disabled, but the option to disable compression for ARC may be removed
    13:41 <CompanionCube> twb: btw about the normalization thing, google finds good stuff like https://zfs-discuss.opensolaris.narkive.com/3NqQVG0H/utf8only-and-normalization-properties
    13:41 <zfs-bot> [ utf8only and normalization properties ] - zfs-discuss.opensolaris.narkive.com
    13:43 <twb> CompanionCube: thanks
    13:44 <CompanionCube> the difference is basically *when* the normalization happens
    13:46 <twb> Hrm, OK.  I assumed utf8only (no normalization=) meant that it'd allow non-normalized (i.e. mixed-normalization) codepoint sequences in file namse
    13:47 <CompanionCube> no normalization is nornalization=none though
    13:48 <twb> So surely that means when = never
    13:49 <twb> (unless the userland application chooses to normalize)
    13:49 <CompanionCube> (btw i meant 'diference' vs apple's method)
    13:50 <twb> ah
    13:50 <twb> That makse more sense :-)
    14:15 <rlaager> twb: normalization in ZFS preserves the bytes of the filename, in whatever form they are in. It only affects the indexing. So it won't break tarballs with a different normal form. However, it implies utf8only, which will be a problem if you unpack a tarball with non-UTF8 filenames.
    14:15 <twb> What does "indexing" mean, there?
    14:17 <rlaager> Sorry, the index of directory entries. So if you try to open("file_in_NFC") or open("file_in_NFD"), they open the same file.
    14:18 <twb> Whereas if there was no normalization, those would be different byte sequences and therefore different files?
    14:19 <rlaager> Correct.
    14:22 <twb> Righto
    14:24 <CompanionCube> and also isn't it nfd in particular because the other choices are either identical or slightly slower?
    14:29 <twb> Is the SMB and SID stuff in zfs manpages at all relevant to a linux samba AD/SMB server?  AIUI the answer is "no, that all happens inside samba and user xattrs, and is invisible to ZFS"
    14:58 <twb> rlaager: in your HOWTO, you try to keep the "OS" separate from the user/OS data.  But e.g. rpool/home and rpool/var/mail are still in separate trees.  Wouldn't it make more sense to have only 2 or 3 top levels, like rpool/OS/debian and rpool/USER/home rpool/USER/mail rpool/USER/www ?
    14:59 <rlaager> Yes, that's a perfectly reasonable approach.
    15:00 <twb> Any reason you did't already do that in the howto?
    15:00 <javashin> lundman, hi
    15:01 <rlaager> twb: I'm not 100% sure yet, and I'd like to avoid making changes that later get undone.
    15:01 <javashin> on osx there is a zfs-import service that try to import everything at boot ? if yes how i disable that ?
    15:02 <PMT> IMO rpool/.../USER/{www,mail,...} makes sense for a multi-user env, but at that point a lot of people might have .../USER on a different pool.
    15:02 <lundman> o/
    15:02 <twb> rlaager: righto
    15:02 <lundman> javashin: there is a launchctl script that run on boot
    15:02 <lundman> which executes the zpool-import-all.sh scipt
    15:02 <lundman> just comment out "zpool import" line in it
    15:02 <twb> Another thing I didn't understand: why are you making zfs's for intermediary paths, e.g. you have a "real" / and a "real" /var/mail, but you also make a /var that doesn't seem to do anything
    15:03 <javashin> what is the location of that script ?
    15:03 <javashin> and thanks by the way
    15:03 <lundman> scripts/zpool-autoimport.sh
    15:03 <lundman> but you probably meant once it is installed
    15:04 <javashin> yeah
    15:04 <lundman> one sec
    15:04 <javashin> no prob
    15:04 <lundman> the plist is in /Library/LaunchDaemons/org.openzfsonosx.zpool-import-all.plist   ... and
    15:05 <lundman> it executes /usr/local/libexec/zfs/launchd.d/zpool-import-all.sh
    15:05 <javashin> nice
    15:08 <rlaager> twb: The /var "container" is just to keep a 1:1 relationship between the dataset hierarchy and the filesystem hierarchy, such that inheritance works, avoiding the need to set mountpoints on rpool/var/*.
    15:10 <twb> But it's not 1:1 already bcause you have /root under /home in the pool, but not in the mountpoints
    15:11 <rlaager> Sure. That feels like it should be in /home. ;)
    15:14 <twb> So if I skip those empty mounts, all I have to do is set -o mountpoints on the descendants, and I'm A-OK?
    16:24 <twb> Can a zfs dataset have separate "soft" and "hard" byte limits?
    16:25 <twb> Right now on ext4, my users can go 20% over their soft quota for up to 1 week, which gives them "wiggle room", e.g. to do a "git repack" when they're over quota
    16:33 <twb> Is moving a directory tree between two datasets on the same pool and expensive operation?
    16:37 <CompanionCube> well, cross dataset 'mv' is actually 'cp+rm' if that's what you mean
    16:53 <twb> yeah I was just thinking that
    16:53 <twb> there's no way for the userland to do a move that just changes a couple of top-level dirents
    16:54 <twb> The main use case I'm thinking of is "oh I really want to move /var/foo out of /var and into its own dataset"
